"""
SeismicFlow
Copyright (C) 2025 Matin Mahzad

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""
import colorsys
import math
import os.path
import win32api
from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QLineEdit, QMessageBox, QScrollArea, QSlider, QButtonGroup, QTabWidget, QSizePolicy, \
    QTabBar, QProgressBar, QSplitter, QMainWindow, QMenu, QColorDialog, QTimeEdit, QDialog, QDialogButtonBox, \
    QListWidget, QSpinBox, QComboBox, QTreeWidget, QTreeWidgetItem, QApplication, QWidget, QHBoxLayout, QVBoxLayout, \
    QPushButton, QLabel, QFileDialog, QTableView, QAbstractItemView, QGridLayout, QInputDialog, QSpacerItem, QToolTip, \
    QDoubleSpinBox, QGraphicsRectItem, QTableWidget, QTableWidgetItem, QToolButton, QDockWidget, QSplashScreen, \
    QGroupBox, QTextEdit, QFormLayout
from PyQt6.QtWebEngineWidgets import QWebEngineView
from PyQt6.QtWebEngineCore import QWebEngineDownloadRequest, QWebEngineProfile, QWebEnginePage
import subprocess
import win32gui
import win32process
import win32con
import shutil
import signal
from PyQt6.QtGui import QMouseEvent, QIcon, QFont, QCursor, QDoubleValidator, QAction, QPixmap, QColor, \
    QDesktopServices, QMovie, QIntValidator, QValidator
from PyQt6.QtCore import QAbstractTableModel, QTimer, QTime, QSize, Qt, QUrl, QFileInfo, \
    QRectF, pyqtSignal, QEventLoop, QObject
from matplotlib.colors import LinearSegmentedColormap
import re
import ast
from OpenGL.GLUT import *
from pyproj import Transformer, Geod
from scipy.interpolate import interp1d, griddata
import numpy as np
import json
import psutil
import pyqtgraph as pg
from pyqtgraph.GraphicsScene import exportDialog
from pyqtgraph import ColorBarItem
from scipy.signal import hilbert
from scipy.signal.windows import gaussian
from skimage.filters import threshold_otsu
from skimage.transform import resize
import matplotlib.pyplot as plt
import segyio
import pandas as pd
import random
import os
import torch
import multiprocessing
from multiprocessing import shared_memory
from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.optim as optim
from torchvision.models import resnet50
from torch.utils.data import DataLoader, TensorDataset
from torch.cuda.amp import autocast, GradScaler
from torch.nn.utils import weight_norm
from pytorch_tabnet.tab_model import TabNetRegressor
from ngboost import NGBRegressor
from catboost import CatBoostRegressor
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE
from sklearn.random_projection import GaussianRandomProjection
from tensorflow.keras.models import Model, Sequential, load_model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, Flatten, Reshape, \
    MultiHeadAttention, LayerNormalization, Add
from sklearn.cluster import AgglomerativeClustering, DBSCAN, SpectralClustering, Birch, MeanShift, \
    AffinityPropagation, OPTICS, KMeans
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import hdbscan
from minisom import MiniSom
import joblib
from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, EarlyStopping, ModelCheckpoint, Callback
from sklearn.multioutput import MultiOutputRegressor
from skimage.restoration import denoise_nl_means, estimate_sigma
from bm3d import bm3d, BM3DProfile
from keras_tuner import BayesianOptimization
import tensorflow as tf
from tensorflow.keras import regularizers
import tempfile
from scipy.spatial import cKDTree, Delaunay
from scipy.stats import pearsonr, spearmanr
import traceback
import requests
from datetime import datetime
import time
import cv2
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, \
    BaggingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.naive_bayes import GaussianNB
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from pytorch_tabnet.tab_model import TabNetClassifier
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
)
from dataclasses import dataclass
from typing import Optional, Any
from scipy import ndimage
from skimage.morphology import ball
from vtkmodules.util import numpy_support
import vtk
from vtkmodules.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor
import sys
from numpy.fft import fft, ifft, fftfreq
import seismic_processing
vtk.vtkObject.GlobalWarningDisplayOff()

try:
    import cupy as cp
    from cupyx.scipy.ndimage import zoom
    from cupyx.scipy import ndimage as gpu_ndimage
    # Attempt a simple GPU operation to check CUDA availability
    cp.array([1])  # Will raise an exception if no GPU is available
    gpu_available = True
except (ImportError, ModuleNotFoundError, cp.cuda.runtime.CUDARuntimeError):
    cp = None
    gpu_available = False

os.environ["TF_GPU_ALLOCATOR"] = "cuda_malloc_async"


class ConstrainedInteractorStyle(vtk.vtkInteractorStyleTrackballCamera):
    def __init__(self, renderer, parent_widget):
        super().__init__()  # Ensure the parent class is properly initialized
        self.current_renderer = renderer
        self.parent_widget = parent_widget  # Store reference to parent widget
        self.AddObserver("MouseMoveEvent", self.mouse_move_callback)
        # When adding observers, specify the priority
        self.AddObserver("LeftButtonPressEvent", self.left_button_press_callback)
        self.AddObserver("LeftButtonReleaseEvent", self.left_button_release_callback)
        self.AddObserver("MiddleButtonPressEvent", self.middle_button_press_callback)
        self.AddObserver("MiddleButtonReleaseEvent", self.middle_button_release_callback)
        self.AddObserver("RightButtonPressEvent", self.right_button_press_callback)
        self.last_x = 0
        self.last_y = 0
        self.is_left_button_pressed = False

    def left_button_press_callback(self, obj, event):
        # Check if interaction involves a vtkImagePlaneWidget
        if self.parent_widget and self.parent_widget.is_interacting_with_widget():
            return  # Skip default behavior for widgets
        if self.parent_widget:
            self.parent_widget.setCursor(Qt.CursorShape.ClosedHandCursor)

        self.is_left_button_pressed = True
        self.last_x, self.last_y = self.GetInteractor().GetEventPosition()

    def left_button_release_callback(self, obj, event):
        if self.parent_widget:
            self.parent_widget.setCursor(Qt.CursorShape.OpenHandCursor)

        self.is_left_button_pressed = False

    def middle_button_press_callback(self, obj, event):
        if self.parent_widget:
            self.parent_widget.setCursor(Qt.CursorShape.ClosedHandCursor)

        super().OnMiddleButtonDown()

    def middle_button_release_callback(self, obj, event):
        if self.parent_widget:
            self.parent_widget.setCursor(Qt.CursorShape.OpenHandCursor)

        super().OnMiddleButtonUp()

    def right_button_press_callback(self, obj, event):
        pass

    def mouse_move_callback(self, obj, event):
        # Prevent default behavior during widget interaction
        if self.parent_widget and self.parent_widget.is_interacting_with_widget():
            return

        if not self.is_left_button_pressed:
            super().OnMouseMove()
            return

        # Get current mouse position
        x, y = self.GetInteractor().GetEventPosition()
        dx = x - self.last_x
        dy = y - self.last_y

        renderer = self.GetCurrentRenderer()
        if renderer is None:
            super().OnMouseMove()  # Fallback if no renderer
            return  # Safeguard against a missing renderer

        camera = renderer.GetActiveCamera()
        if camera is None:
            super().OnMouseMove()  # Fallback if no camera
            return  # Safeguard against a missing camera

        direction = list(camera.GetDirectionOfProjection())
        # Get the actual angle (keeping the sign)
        current_angle = math.degrees(math.asin(direction[1]))

        # Allow movement if:
        # 1. We're within safe limits (-85 to +85) OR
        # 2. We're at the top (>= 85) and trying to move down (dy > 0) OR
        # 3. We're at the bottom (<= -85) and trying to move up (dy < 0)
        if (abs(current_angle) < 85 or  # Within safe limits
                (current_angle >= 85 and dy > 0) or  # At top, moving down
                (current_angle <= -85 and dy < 0)):  # At bottom, moving up
            camera.Elevation(-dy * 1.0)

        # Horizontal rotation always allowed
        camera.Azimuth(-dx * 0.5)

        # Ensure up vector stays vertical
        camera.SetViewUp(0, -1, 0)

        self.GetInteractor().Render()
        self.last_x, self.last_y = x, y


class CustomVTKWidget(QVTKRenderWindowInteractor):
    def __init__(self, parent):
        super().__init__(parent)

        # Storing the methods and attributes passed from TensorVisualizer
        self.parent = parent
        # Initialize VTK components
        self.renderer = vtk.vtkRenderer()
        self.GetRenderWindow().AddRenderer(self.renderer)
        # Set the initial background color
        self.setBackgroundColor(self.parent.last_selected_color)

        # Set up the interactor
        self.interactor = self.GetRenderWindow().GetInteractor()

        # Set custom interaction style
        self.style = ConstrainedInteractorStyle(self.renderer, self)
        self.interactor.SetInteractorStyle(self.style)
        self.current_widget = None  # Track the widget being interacted with

    def resizeEvent(self, event):
        """Handle resize events - reset camera and render"""
        super().resizeEvent(event)
        if self.renderer.GetViewProps().GetNumberOfItems() > 0:
            self.renderer.ResetCamera()
            camera = self.renderer.GetActiveCamera()
            camera.Dolly(0.8)  # Simple zoom out - adjust this value to taste
            self.renderer.ResetCameraClippingRange()
            self.GetRenderWindow().Render()

    def is_interacting_with_widget(self):
        # Return True if a vtkImagePlaneWidget is active
        return self.current_widget is not None

    def attach_widget(self, widget):
        # Attach observers to the vtkImagePlaneWidget
        widget.AddObserver("StartInteractionEvent", self.start_widget_interaction)
        widget.AddObserver("EndInteractionEvent", self.end_widget_interaction)

    def start_widget_interaction(self, obj, event):
        self.current_widget = obj

    def end_widget_interaction(self, obj, event):
        self.current_widget = None

    def setBackgroundColor(self, color):
        """Override to handle both hex string and QColor inputs"""
        if isinstance(color, str):
            # Convert hex string to RGB
            color = QColor(color)

        self.renderer.SetBackground(
            color.red() / 255.0,
            color.green() / 255.0,
            color.blue() / 255.0
        )
        self.GetRenderWindow().Render()

    def contextMenuEvent(self, event):
        context_menu = QMenu(self)

        add_tab_action = context_menu.addAction("Add Tab")
        add_tab_action.triggered.connect(self.parent.add_tab)

        switch_to_2d_action = context_menu.addAction("Switch to 2D")

        change_color_action = context_menu.addAction("Change Background Color")

        # Add toggle actions for grid and axes
        toggle_color_bar_action = QAction("Toggle Color Bar", self)
        context_menu.addAction(toggle_color_bar_action)
        toggle_color_bar_action.triggered.connect(self.toggle_color_bar)

        # Add toggle actions for grid and axes
        toggle_grid_action = QAction("Toggle Axis", self)
        context_menu.addAction(toggle_grid_action)
        toggle_grid_action.triggered.connect(self.toggleGrid)

        # Add toggle actions for Compass
        toggle_compass_action = QAction("Toggle Compass", self)
        context_menu.addAction(toggle_compass_action)
        toggle_compass_action.triggered.connect(self.togglecompass)

        # Add toggle actions for grid and axes
        reset_view_action = QAction("Reset View", self)
        context_menu.addAction(reset_view_action)
        reset_view_action.triggered.connect(self.reset_view)

        if any(
                hasattr(item, "SetPriority")
                for item in self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), [])
        ):
            # Add submenu for selecting items
            self.select_item_submenu = QMenu("Select Item", self)
            context_menu.addMenu(self.select_item_submenu)

            # Populate the submenu for selection functionality
            self.populate_select_item_submenu(self.select_item_submenu)

        # Get the current tab's volume items
        current_tab_items = self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), [])

        # Add "Hide Items" submenu if there are items
        if current_tab_items:

            # Add a new submenu for hiding items
            self.hide_item_submenu = QMenu("Hide Item", self)
            context_menu.addMenu(self.hide_item_submenu)

            # Call the function to populate the submenus
            self.populate_hide_item_submenu(self.hide_item_submenu)

            # Add a new submenu for removing items
            self.remove_item_submenu = QMenu("Remove Item", self)
            context_menu.addMenu(self.remove_item_submenu)

            self.populate_remove_item_submenu(self.remove_item_submenu)

        # Add toggle actions for grid and axes
        grid_color = QAction("Change Axis Color", self)
        context_menu.addAction(grid_color)

        # Add a new submenu for camera views
        camera_view_submenu = QMenu("Camera View", self)
        context_menu.addMenu(camera_view_submenu)

        # Add Save actions
        Save_Image = QAction("Save Image", self)
        Save_Image.triggered.connect(self.save_image)
        context_menu.addAction(Save_Image)

        # Define camera view actions
        top_view_action = QAction("Top", self)
        bottom_view_action = QAction("Bottom", self)
        left_view_action = QAction("Left", self)
        right_view_action = QAction("Right", self)
        front_view_action = QAction("Front", self)
        back_view_action = QAction("Back", self)

        # Add actions to the submenu
        camera_view_submenu.addAction(top_view_action)
        camera_view_submenu.addAction(bottom_view_action)
        camera_view_submenu.addAction(left_view_action)
        camera_view_submenu.addAction(right_view_action)
        camera_view_submenu.addAction(front_view_action)
        camera_view_submenu.addAction(back_view_action)

        # Connect actions to methods
        top_view_action.triggered.connect(lambda: self.set_camera_view('top'))
        bottom_view_action.triggered.connect(lambda: self.set_camera_view('bottom'))
        left_view_action.triggered.connect(lambda: self.set_camera_view('left'))
        right_view_action.triggered.connect(lambda: self.set_camera_view('right'))
        front_view_action.triggered.connect(lambda: self.set_camera_view('front'))
        back_view_action.triggered.connect(lambda: self.set_camera_view('back'))

        action = context_menu.exec(self.mapToGlobal(event.pos()))

        # Handle actions
        if action == change_color_action:
            color = QColorDialog.getColor(parent=self)
            if color.isValid():
                self.setBackgroundColor(color)
                grid_clr = self.parent.adjust_grid_color(color)

                current_index = self.parent.tab_widget.currentIndex()

                self.parent.grid_color[current_index] = grid_clr

                self.parent.set_foreground_color(grid_clr, current_index)
        elif action == switch_to_2d_action:
            self.parent.toggle_canvas('2D')
        elif action == grid_color:
            color = QColorDialog.getColor(options=QColorDialog.ColorDialogOption.ShowAlphaChannel, parent=self)
            if color.isValid():
                current_index = self.parent.tab_widget.currentIndex()

                self.parent.grid_color[current_index] = color

                self.parent.set_foreground_color(color, current_index)

    def save_image(self):
        # Get the index of the currently active tab
        active_tab_index = self.parent.tab_widget.currentIndex()

        # Retrieve the widget of the currently active tab
        active_tab_widget = self.parent.tab_widget.widget(active_tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()
        # Get the primary screen of the application
        screen = QApplication.primaryScreen()

        # Grab the window associated with the canvas
        screenshot = screen.grabWindow(canvas.winId())

        # Get the file path from the user using a file dialog window
        file_path, _ = QFileDialog.getSaveFileName(self, "Save File", "", "PNG Files (*.png)")

        # Check if the user selected a file path
        if file_path:
            # Save the high-quality image
            screenshot.save(file_path, "PNG")

    def set_camera_view(self, view):
        """Sets the camera to a predefined view."""
        if not self.renderer:
            return  # Safeguard if renderer is not available

        camera = self.renderer.GetActiveCamera()
        if not camera:
            return  # Safeguard if camera is not available

        # Calculate the offset needed for 85 degrees
        # tan(85°) ≈ 11.43, so using this ratio will give us exactly 85 degrees
        view_angle = math.radians(85)  # Convert to radians
        vertical_position = math.sin(view_angle)  # How far up/down
        horizontal_offset = math.cos(view_angle)  # How far from center

        # Define camera positions and focal points for each view
        if view == 'top':
            # Position slightly offset from directly above
            camera.SetPosition(0, -vertical_position, horizontal_offset)
            camera.SetViewUp(0, 0, -1)
            camera.SetFocalPoint(0, 0, 0)
        elif view == 'bottom':
            # Position slightly offset from directly below
            camera.SetPosition(0, vertical_position, -horizontal_offset)
            camera.SetViewUp(0, 0, -1)
            camera.SetFocalPoint(0, 0, 0)
        elif view == 'left':
            camera.SetPosition(-1, 0, 0)
            camera.SetViewUp(0, -1, 0)
            camera.SetFocalPoint(0, 0, 0)
        elif view == 'right':
            camera.SetPosition(1, 0, 0)
            camera.SetViewUp(0, -1, 0)
            camera.SetFocalPoint(0, 0, 0)
        elif view == 'front':
            camera.SetPosition(0, 0, 1)
            camera.SetViewUp(0, -1, 0)
            camera.SetFocalPoint(0, 0, 0)
        elif view == 'back':
            camera.SetPosition(0, 0, -1)
            camera.SetViewUp(0, -1, 0)
            camera.SetFocalPoint(0, 0, 0)

        self.renderer.ResetCamera()
        camera.Dolly(0.8)  # Simple zoom out - adjust this value to taste
        self.renderer.ResetCameraClippingRange()
        # Reset camera to update the scene
        self.GetRenderWindow().Render()

    def reset_view(self):
        """Reset camera to default position"""
        self.renderer.ResetCamera()
        camera = self.renderer.GetActiveCamera()
        camera.SetViewUp(0, -1, 0)
        camera.Dolly(0.8)  # Simple zoom out - adjust this value to taste
        self.renderer.ResetCameraClippingRange()
        self.GetRenderWindow().Render()

    def toggle_item_selection(self, item):

        current_index = self.parent.tab_widget.currentIndex()
        # Retrieve the widget of the currently active tab
        active_tab_widget = self.parent.tab_widget.widget(current_index)
        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        # Toggle item selection
        if (item.GetVisibility() if hasattr(item, "GetVisibility") else item.GetEnabled()):

            # Show the item by enabling visibility
            if hasattr(item, "SetVisibility"):  # For volumes
                item.SetVisibility(0)

            elif hasattr(item, "SetEnabled"):  # For plain widgets (e.g., vtkImagePlaneWidget)
                item.SetEnabled(0)

            # Check if this is the last visible item
            if all(
                    (not i.GetVisibility() if hasattr(i, "GetVisibility") else not i.GetEnabled())
                    for i in self.parent.tab_volume_items.get(current_index, [])
            ):
                grid = self.parent.grids[current_index]
                grid.SetVisibility(0)

                if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
                    # Set assembly visibility
                    canvas.arrow_assembly.SetVisibility(0)

                    # Toggle orientation marker widget
                    canvas.orientation_marker.SetEnabled(0)

            # Loop through all actors in the renderer
            for i in range(self.renderer.GetActors2D().GetNumberOfItems()):
                actor = self.renderer.GetActors2D().GetItemAsObject(i)
                if isinstance(actor, vtk.vtkScalarBarActor) and actor.GetVisibility():
                    actor.SetVisibility(0)

            if self.parent.three_d_color_bar_active:
                current_items = self.parent.tab_volume_items.get(current_index, [])

                # Find the index of the hidden item
                current_idx = current_items.index(item)
                # If it's the first item (index 0), look forwards
                if current_idx == 0:
                    for i in range(len(current_items)):
                        check_idx = (current_idx + 1 + i) % len(current_items)
                        check_item = current_items[check_idx]

                        # Check visibility
                        if hasattr(check_item, 'GetEnabled') and check_item.GetEnabled():
                            if hasattr(check_item, 'color_bar'):
                                check_item.color_bar.SetVisibility(1)
                                break
                # For all other items, look backwards
                else:
                    for i in range(len(current_items)):
                        check_idx = (current_idx - 1 - i) if (current_idx - 1 - i) >= 0 else (
                                    len(current_items) + (current_idx - 1 - i))
                        check_item = current_items[check_idx]

                        # Check visibility
                        if hasattr(check_item, 'GetEnabled') and check_item.GetEnabled():
                            if hasattr(check_item, 'color_bar'):
                                check_item.color_bar.SetVisibility(1)
                                break

        else:
            # Loop through all actors in the renderer
            for i in range(self.renderer.GetActors2D().GetNumberOfItems()):
                actor = self.renderer.GetActors2D().GetItemAsObject(i)
                if isinstance(actor, vtk.vtkScalarBarActor) and actor.GetVisibility():
                    actor.SetVisibility(0)

            if hasattr(item, "SetVisibility"):  # For volumes
                item.SetVisibility(1)
                if self.parent.three_d_color_bar_active:
                    if hasattr(item, 'color_bar'):
                        item.color_bar.SetVisibility(1)
            elif hasattr(item, "SetEnabled"):  # For plain widgets (e.g., vtkImagePlaneWidget)
                item.SetEnabled(1)
                if self.parent.three_d_color_bar_active:
                    if hasattr(item, 'color_bar'):
                        item.color_bar.SetVisibility(1)

            if self.parent.grid_active:
                grid = self.parent.grids[current_index]
                grid.SetVisibility(1)
            else:
                grid = self.parent.grids[current_index]
                grid.SetVisibility(0)

            if self.parent.compass_active:
                if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
                    # Set assembly visibility
                    canvas.arrow_assembly.SetVisibility(1)

                    # Toggle orientation marker widget
                    canvas.orientation_marker.SetEnabled(1)
            else:
                if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
                    # Set assembly visibility
                    canvas.arrow_assembly.SetVisibility(0)

                    # Toggle orientation marker widget
                    canvas.orientation_marker.SetEnabled(0)

        # Trigger the rendering update
        self.GetRenderWindow().Render()

    def populate_hide_item_submenu(self, submenu):
        # Populate the submenu with actions to hide items
        counter = 1
        for item in self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), []):
            if hasattr(item, "GetVisibility"):  # For volumes
                item_action = QAction(f"Item {counter}: {'Shown' if item.GetVisibility() else 'Hidden'}", self)
            elif hasattr(item, "GetEnabled"):  # For widgets (like vtkImagePlaneWidget)
                item_action = QAction(f"Item {counter}: {'Shown' if item.GetEnabled() else 'Hidden'}", self)
            item_action.triggered.connect(lambda _, i=item: self.toggle_item_selection(i))
            submenu.addAction(item_action)
            counter += 1

    def populate_remove_item_submenu(self, submenu):
        # Populate the submenu with actions to remove items
        counter = 1
        for item in self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), []):
            item_action = QAction(f"Remove Item {counter}", self)
            item_action.triggered.connect(lambda _, i=item: self.remove_item(i))
            submenu.addAction(item_action)
            counter += 1

    def remove_item(self, item):
        # Remove the item from the dictionary
        current_index = self.parent.tab_widget.currentIndex()
        if item in self.parent.tab_volume_items[current_index]:

            # Loop through all actors in the renderer
            for i in range(self.renderer.GetActors2D().GetNumberOfItems()):
                actor = self.renderer.GetActors2D().GetItemAsObject(i)
                if isinstance(actor, vtk.vtkScalarBarActor) and actor.GetVisibility():
                    actor.SetVisibility(0)

            if self.parent.three_d_color_bar_active:
                current_items = self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), [])

                # Find the index of the hidden item
                current_idx = current_items.index(item)
                # If it's the first item (index 0), look forwards
                if current_idx == 0:
                    for i in range(len(current_items)):
                        check_idx = (current_idx + 1 + i) % len(current_items)
                        check_item = current_items[check_idx]

                        # Check visibility
                        if hasattr(check_item, 'GetEnabled') and check_item.GetEnabled():
                            if hasattr(check_item, 'color_bar'):
                                check_item.color_bar.SetVisibility(1)
                                break
                # For all other items, look backwards
                else:
                    for i in range(len(current_items)):
                        check_idx = (current_idx - 1 - i) if (current_idx - 1 - i) >= 0 else (
                                    len(current_items) + (current_idx - 1 - i))
                        check_item = current_items[check_idx]

                        # Check visibility
                        if hasattr(check_item, 'GetEnabled') and check_item.GetEnabled():
                            if hasattr(check_item, 'color_bar'):
                                check_item.color_bar.SetVisibility(1)
                                break

            self.parent.tab_volume_items[current_index].remove(item)
            # Remove the item from the renderer
            if hasattr(item, "SetVisibility"):  # For volumes
                self.renderer.RemoveActor(item)
                if hasattr(item, 'color_bar'):
                    self.renderer.RemoveActor2D(item.color_bar)
            elif hasattr(item, "SetEnabled"):  # For plain widgets (e.g., vtkImagePlaneWidget)
                item.SetEnabled(0)
                if hasattr(item, 'color_bar'):
                    self.renderer.RemoveActor2D(item.color_bar)
            del item
            # Check if this was the last item in the list
            if not self.parent.tab_volume_items[current_index]:
                # Retrieve the widget of the currently active tab
                active_tab_widget = self.parent.tab_widget.widget(current_index)
                # Assuming the canvas is the first widget in the layout of the active tab
                canvas_layout = active_tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()
                if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
                    # Proper removal sequence
                    canvas.orientation_marker.SetEnabled(0)
                    canvas.orientation_marker.SetInteractor(None)
                    canvas.orientation_marker.SetOrientationMarker(None)  # Break internal reference

                    # Correct removal method for assemblies
                    canvas.renderer.RemoveViewProp(canvas.arrow_assembly)

                    # Force immediate release
                    canvas.arrow_assembly.ReleaseGraphicsResources(canvas.GetRenderWindow())
                    canvas.arrow_assembly = None

                    # Clean up marker
                    del canvas.orientation_marker
                    canvas.__dict__.pop('orientation_marker', None)  # Ensure deletion

                if current_index in self.parent.grids:
                    grid = self.parent.grids.pop(current_index)  # Remove grid from the dictionary
                    self.renderer.RemoveActor(grid)  # Remove grid from the renderer

            # Trigger the rendering update
            self.GetRenderWindow().Render()
            # Update the submenus to reflect the removal
            if hasattr(self, "hide_item_submenu") or hasattr(self, "remove_item_submenu"):
                self.populate_hide_item_submenu(self.hide_item_submenu)
                self.populate_remove_item_submenu(self.remove_item_submenu)

    def populate_select_item_submenu(self, submenu):
        # Populate the submenu with actions to select items
        counter = 1
        for item in self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), []):
            # Check if the item is a vtkImagePlaneWidget (i.e., it has SetPriority)
            if hasattr(item, "SetPriority"):
                priority = item.GetPriority()
                item_action = QAction(
                    f"Item {counter}: {'Unselected' if priority == 0.0 else 'Selected'}", self
                )
                item_action.triggered.connect(lambda _, i=item: self.toggle_item_priority(i))
                submenu.addAction(item_action)
                counter += 1

    def toggle_item_priority(self, item):
        # Toggle priority for vtkImagePlaneWidget items
        if hasattr(item, "SetPriority"):
            if item.GetPriority() == 0.0:
                item.SetPriority(1000.0)
                item.SetLeftButtonAction(1)
                self.attach_widget(item)  # Attach observers
            else:
                item.SetPriority(0.0)
                item.SetLeftButtonAction(3)
                self.current_widget = None  # Clear current widget reference

        # Trigger the rendering update
        self.GetRenderWindow().Render()

    def toggleGrid(self):
        # Get the index of the currently active tab
        active_tab_index = self.parent.tab_widget.currentIndex()

        # Check if the grid for the current tab exists in the TensorVisualizer's grids dictionary
        if active_tab_index in self.parent.grids:
            grid = self.parent.grids[active_tab_index]
            visible = grid.GetVisibility()
            grid.SetVisibility(not visible)
            self.GetRenderWindow().Render()

    def togglecompass(self):
        # Get the index of the currently active tab
        active_tab_index = self.parent.tab_widget.currentIndex()
        # Retrieve the widget of the currently active tab
        active_tab_widget = self.parent.tab_widget.widget(active_tab_index)
        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
            visible = canvas.arrow_assembly.GetVisibility()
            # Set assembly visibility
            canvas.arrow_assembly.SetVisibility(not visible)
            # Toggle orientation marker widget
            canvas.orientation_marker.SetEnabled(not visible)
            self.GetRenderWindow().Render()

    def toggle_color_bar(self):
        """
        Toggle the visibility of color bars in the current VTK window.
        If any color bar is currently visible, the function hides all color bars.
        Otherwise, it shows the color bar of the last visible item in the list.
        """
        current_items = self.parent.tab_volume_items.get(self.parent.tab_widget.currentIndex(), [])

        # Step 1: Check if any color bar is currently visible
        any_visible = False
        for item in current_items:
            if hasattr(item, 'color_bar'):
                if (
                    (hasattr(item, 'GetEnabled') and item.GetEnabled()) or
                    (hasattr(item, 'GetVisibility') and item.GetVisibility())
                ):
                    if item.color_bar.GetVisibility():  # Color bar is visible
                        any_visible = True
                        break

        # Step 2: Hide all color bars
        for item in current_items:
            if hasattr(item, 'color_bar'):
                if item.color_bar.GetVisibility():
                    item.color_bar.SetVisibility(0)

        # Step 3: If no color bar was initially visible, show the color bar of the last visible item
        if not any_visible:
            for item in reversed(current_items):  # Start from the last item
                if (
                    (hasattr(item, 'GetEnabled') and item.GetEnabled()) or
                    (hasattr(item, 'GetVisibility') and item.GetVisibility())
                ):
                    if hasattr(item, 'color_bar'):
                        item.color_bar.SetVisibility(1)
                        break

        # Step 4: Redraw the render window to reflect changes
        self.GetRenderWindow().Render()

    def enterEvent(self, event):
        # Set cursor to open hand when entering the widget
        self.setCursor(QCursor(Qt.CursorShape.OpenHandCursor))
        super().enterEvent(event)

    def leaveEvent(self, event):
        # Revert cursor to arrow when leaving the widget
        self.setCursor(QCursor(Qt.CursorShape.ArrowCursor))
        super().leaveEvent(event)


class ResNet1D(nn.Module):
    def __init__(self, input_dim):
        super(ResNet1D, self).__init__()
        self.model = resnet50(weights=None)  # Initialize without pre-trained weights

        # Modify the first convolutional layer to accept 1D input
        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)

        # Replace the fully connected layer to match the number of input features and output a single value
        self.model.fc = nn.Linear(self.model.fc.in_features, 1)

        # Additional linear layer to convert flattened 1D input to a suitable shape for conv layer
        self.fc1 = nn.Linear(input_dim, 64 * 7 * 7)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)  # Apply the linear layer to get the right shape
        x = self.relu(x)
        x = x.view(-1, 1, 56, 56)  # Reshape to (batch_size, channels, height, width)
        return self.model(x)


class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()


class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        self.conv1 = weight_norm(
            nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = weight_norm(
            nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                 self.conv2, self.chomp2, self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TCN(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
        super(TCN, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i - 1]
            out_channels = num_channels[i]
            layers += [
                TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                              padding=(kernel_size - 1) * dilation_size, dropout=dropout)]

        self.network = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], 1)

    def forward(self, x):
        x = self.network(x)
        x = x.mean(dim=2)  # Global average pooling
        return self.fc(x)


class RoundedCanvas(pg.PlotWidget):
    def __init__(self, tensor_visualizer):
        super().__init__()
        self.hideAxis('bottom')
        self.hideAxis('left')
        self.setMenuEnabled(False)  # This will disable the context menu
        self.tensor_visualizer = tensor_visualizer
        self.setBackground(self.tensor_visualizer.last_selected_color)
        # Initialize editing mode and annotation value
        self.edit_mode = False
        self.annotation_value = None
        self.annotation_radius = 0

    def mousePressEvent(self, event: QMouseEvent):
        if event.button() == Qt.MouseButton.LeftButton:
            self.setCursor(Qt.CursorShape.ClosedHandCursor)  # Set the cursor to a fist
            super().mousePressEvent(event)
        elif event.button() == Qt.MouseButton.MiddleButton:
            if self.edit_mode:
                # Step 1: Check if the click is on the image item
                self.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))
                active_tab_index = self.tensor_visualizer.tab_widget.currentIndex()

                # Get the image item from the dictionary
                img_item = self.tensor_visualizer.tensor_image_items.get(active_tab_index, {}).get(self.tensor_visualizer.file_name)
                if img_item:

                    click_pos = event.position()
                    img_pos = img_item.mapFromScene(click_pos)

                    if 0 <= img_pos.x() <= img_item.width() and 0 <= img_pos.y() <= img_item.height():
                        self.edit_annotation(event)
                        event.accept()  # Accept the event so it doesn't propagate further
                else:
                    QMessageBox.warning(
                        self,
                        "Data Not Rendered",
                        "The selected data is not currently rendered on the canvas."
                        "\nPlease render it before attempting to interact with it."
                    )
            else:
                # Step 1: Check if the click is on the image item
                active_tab_index = self.tensor_visualizer.tab_widget.currentIndex()

                # Get the image item from the dictionary
                img_item = self.tensor_visualizer.tensor_image_items.get(active_tab_index, {}).get(self.tensor_visualizer.file_name)
                if img_item:

                    click_pos = event.position()
                    img_pos = img_item.mapFromScene(click_pos)

                    if 0 <= img_pos.x() <= img_item.width() and 0 <= img_pos.y() <= img_item.height():
                        self.map_click_to_tensor(event)
                        event.accept()  # Accept the event so it doesn't propagate further
                else:
                    QMessageBox.warning(
                        self,
                        "Data Not Rendered",
                        "The selected data is not currently rendered on the canvas."
                        "\nPlease render it before attempting to interact with it."
                    )

        else:
            super().mousePressEvent(event)

    def edit_annotation(self, event):
        try:
            # Step 1: Get the active tab index from the tensor visualizer
            active_tab_index = self.tensor_visualizer.tab_widget.currentIndex()

            # Find the key in the tensor_dict that corresponds to the tensor_data
            tensor_key = self.tensor_visualizer.file_name

            # Step 3: Retrieve the metadata for the active tab using the tensor key
            metadata_list = self.tensor_visualizer.tensor_metadata[active_tab_index]

            # Retrieve the metadata
            metadata = next(item for item in metadata_list if item['tensor_key'] == tensor_key)
            slice_params = metadata['slice_params']

            # Step 3: Get the coordinates of the click on the interpolated image
            click_pos = event.position()
            # Get the image item from the dictionary
            img_item = self.tensor_visualizer.tensor_image_items.get(active_tab_index, {}).get(tensor_key)
            img_pos = img_item.mapFromScene(click_pos)
            x_click, y_click = int(img_pos.x()), int(img_pos.y())

            # Step 4: Map the click position from the interpolated image to the original 2D slice it has to be backwards
            # it's not a mistake
            x_orig = y_click // self.tensor_visualizer.two_d_interpolation
            y_orig = x_click // self.tensor_visualizer.two_d_interpolation

            # Step 5: Retrieve the original 3D tensor data using the tensor key
            original_tensor = self.tensor_visualizer.tensor_data

            # Step 6: Retrieve the slice parameters
            index_dim = slice_params['index_dim']
            selected_index = slice_params['selected_index']
            channel_index = slice_params['channel_index']
            index_entry = slice_params['index_entry']

            # Step 7: Determine the coordinates in the original 3D tensor
            if index_dim == 0:  # Time slice: depth as index, plotting cross-line vs. inline
                coord = (selected_index, y_orig, x_orig, channel_index)
            elif index_dim == 1:  # Inline: cross-line as index, plotting depth vs. inline
                coord = (y_orig, selected_index, x_orig, channel_index)
            elif index_dim == 2:  # Cross-line: inline as index, plotting depth vs. cross-line
                coord = (y_orig, x_orig, selected_index, channel_index)

            # Step 8: Print the value at the determined coordinates
            value = original_tensor[coord]

            radius = self.annotation_radius  # Assume radius is defined as a separate attribute

            if radius == 0:
                # Directly update only the clicked cell
                original_tensor[coord] = self.annotation_value

            else:
                # Implement the radius functionality for circles
                # Determine the bounds of the region to be edited
                min_y = max(0, y_orig - radius)
                max_y = min(original_tensor.shape[0 if index_dim != 0 else 1] - 1, y_orig + radius)
                min_x = max(0, x_orig - radius)
                max_x = min(original_tensor.shape[2 if index_dim != 2 else 1] - 1, x_orig + radius)

                # Edit all points within the radius
                for y in range(min_y, max_y + 1):
                    for x in range(min_x, max_x + 1):
                        # Check if the point is inside the circle
                        if index_dim == 0:
                            original_tensor[selected_index, y, x, channel_index] = self.annotation_value
                        elif index_dim == 1:
                            original_tensor[y, selected_index, x, channel_index] = self.annotation_value
                        elif index_dim == 2:
                            original_tensor[y, x, selected_index, channel_index] = self.annotation_value

            self.tensor_visualizer.tensor_data = original_tensor

            # Optionally update the display or save the tensor
            self.tensor_visualizer.add_tensor(f"{self.tensor_visualizer.file_name}", self.tensor_visualizer.tensor_data)

            dim1 = slice_params['dim1']
            dim2 = slice_params['dim2']

            # Set the text for the self-references
            self.tensor_visualizer.dim1_entry.setText(str(dim1))
            self.tensor_visualizer.dim2_entry.setText(str(dim2))
            self.tensor_visualizer.index_dim_entry.setText(str(index_dim))
            self.tensor_visualizer.index_entry.setValue(index_entry)
            self.tensor_visualizer.channel_index_entry.setValue(channel_index)

            self.tensor_visualizer.plot_tensor()
            # Extract the metadata from the filename
            metadata = self.tensor_visualizer.metadata[tensor_key]

            # Extract data
            inline, xline = coord[1:3]

            # get the cordinates for the input data
            origin = metadata.get('origin', (0, 0))  # (x, y) for origin (inline=0, crossline=0)
            xline_end = metadata.get('xline_end', (original_tensor.shape[2] - 1, 0))
            inline_end = metadata.get('inline_end', (0, original_tensor.shape[1] - 1))

            # Calculate the new coordinates for the sliced tensor
            H = original_tensor.shape[1]  # Original number of inlines
            W = original_tensor.shape[2]  # Original number of crosslines

            # Convert coordinates to NumPy arrays for vector operations
            origin = np.array(origin, dtype=float)
            xline_end = np.array(xline_end, dtype=float)
            inline_end = np.array(inline_end, dtype=float)

            # Compute step vectors, handling cases where H or W is 1
            V_inline = (inline_end - origin) / (H - 1) if H > 1 else np.array([0, 0], dtype=float)
            V_crossline = (xline_end - origin) / (W - 1) if W > 1 else np.array([0, 0], dtype=float)

            # Calculate earth coordinates by applying the linear transformation
            earth_coordinates = origin + inline * V_inline + xline * V_crossline

            sampling_interval = metadata.get('sampling_interval_ms')

            time_min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
            time_step = sampling_interval

            inline_range = metadata.get('inline_range', (0, original_tensor.shape[1] - 1))

            xline_range = metadata.get('xline_range', (0, original_tensor.shape[2] - 1))

            inline_min_slider_value = inline_range[0]
            inline_step = (
                (inline_range[1] - inline_range[0]) / (original_tensor.shape[1] - 1) if original_tensor.shape[
                                                                                            1] > 1 else 1)

            xline_min_slider_value = xline_range[0]
            xline_step = (
                (xline_range[1] - xline_range[0]) / (original_tensor.shape[2] - 1) if original_tensor.shape[
                                                                                          2] > 1 else 1)

            # Step 11: Show the tooltip with the coordinates and value
            global_pos = event.globalPosition().toPoint()

            if '_' in tensor_key:
                # Split at the first underscore
                parts = tensor_key.split('_', 1)
                data_name = parts[0]
            else:
                data_name = tensor_key

            template = metadata.get('template')

            QToolTip.showText(global_pos,
                              f"Data: {data_name}\n"
                              f"Template: {template}\n"
                              f"Time: {int((coord[0] * time_step) + time_min_slider_value)}\nInline: {int((coord[1] * inline_step) + inline_min_slider_value)}\nCrossline: {int((coord[2] * xline_step) + xline_min_slider_value)}\n"
                              f"X: {earth_coordinates[0]:.2f}\nY: {earth_coordinates[1]:.2f}\n"
                              f"Value Before Editing: {value:.2f}", self)

        except Exception as e:
            QMessageBox.critical(self.tensor_visualizer, "Error", f"An error occurred: {e}")

    def map_click_to_tensor(self, event):
        try:
            # Step 1: Get the active tab index from the tensor visualizer
            active_tab_index = self.tensor_visualizer.tab_widget.currentIndex()

            # Find the key in the tensor_dict that corresponds to the tensor_data
            tensor_key = self.tensor_visualizer.file_name

            # Step 3: Retrieve the metadata for the active tab using the tensor key
            metadata_list = self.tensor_visualizer.tensor_metadata[active_tab_index]

            # Retrieve the metadata
            metadata = next(item for item in metadata_list if item['tensor_key'] == tensor_key)
            slice_params = metadata['slice_params']

            # Step 3: Get the coordinates of the click on the interpolated image
            click_pos = event.position()
            # Get the image item from the dictionary
            img_item = self.tensor_visualizer.tensor_image_items.get(active_tab_index, {}).get(tensor_key)
            img_pos = img_item.mapFromScene(click_pos)
            x_click, y_click = int(img_pos.x()), int(img_pos.y())

            # Step 4: Map the click position from the interpolated image to the original 2D slice it has to be backwards
            # it's not a mistake
            x_orig = y_click // self.tensor_visualizer.two_d_interpolation
            y_orig = x_click // self.tensor_visualizer.two_d_interpolation

            # Step 5: Retrieve the original 3D tensor data using the tensor key
            original_tensor = self.tensor_visualizer.tensor_data

            # Step 6: Retrieve the slice parameters
            index_dim = slice_params['index_dim']
            selected_index = slice_params['selected_index']
            channel_index = slice_params['channel_index']

            # Step 7: Determine the coordinates in the original 3D tensor
            if index_dim == 0:  # Time slice: depth as index, plotting cross-line vs. inline
                coord = (selected_index, y_orig, x_orig, channel_index)
            elif index_dim == 1:  # Inline: cross-line as index, plotting depth vs. inline
                coord = (y_orig, selected_index, x_orig, channel_index)
            elif index_dim == 2:  # Cross-line: inline as index, plotting depth vs. cross-line
                coord = (y_orig, x_orig, selected_index, channel_index)

            # Step 8: Extract the value at the determined coordinates
            value = original_tensor[coord]

            # Extract the metadata from the filename
            metadata = self.tensor_visualizer.metadata[tensor_key]

            # Extract data
            inline, xline = coord[1:3]

            # get the cordinates for the input data
            origin = metadata.get('origin', (0, 0))  # (x, y) for origin (inline=0, crossline=0)
            xline_end = metadata.get('xline_end', (original_tensor.shape[2] - 1, 0))
            inline_end = metadata.get('inline_end', (0, original_tensor.shape[1] - 1))

            # Calculate the new coordinates for the sliced tensor
            H = original_tensor.shape[1]  # Original number of inlines
            W = original_tensor.shape[2]  # Original number of crosslines

            # Convert coordinates to NumPy arrays for vector operations
            origin = np.array(origin, dtype=float)
            xline_end = np.array(xline_end, dtype=float)
            inline_end = np.array(inline_end, dtype=float)

            # Compute step vectors, handling cases where H or W is 1
            V_inline = (inline_end - origin) / (H - 1) if H > 1 else np.array([0, 0], dtype=float)
            V_crossline = (xline_end - origin) / (W - 1) if W > 1 else np.array([0, 0], dtype=float)

            # Calculate earth coordinates by applying the linear transformation
            earth_coordinates = origin + inline * V_inline + xline * V_crossline

            sampling_interval = metadata.get('sampling_interval_ms')

            time_min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
            time_step = sampling_interval

            inline_range = metadata.get('inline_range', (0, original_tensor.shape[1] - 1))

            xline_range = metadata.get('xline_range', (0, original_tensor.shape[2] - 1))

            inline_min_slider_value = inline_range[0]
            inline_step = (
                (inline_range[1] - inline_range[0]) / (original_tensor.shape[1] - 1) if original_tensor.shape[
                                                                                            1] > 1 else 1)

            xline_min_slider_value = xline_range[0]
            xline_step = (
                (xline_range[1] - xline_range[0]) / (original_tensor.shape[2] - 1) if original_tensor.shape[
                                                                                          2] > 1 else 1)

            # Step 11: Show the tooltip with the coordinates and value
            global_pos = event.globalPosition().toPoint()

            if '_' in tensor_key:
                # Split at the first underscore
                parts = tensor_key.split('_', 1)
                data_name = parts[0]
            else:
                data_name = tensor_key

            template = metadata.get('template')

            QToolTip.showText(global_pos,
                              f"Data: {data_name}\n"
                              f"Template: {template}\n"
                              f"Time: {int((coord[0] * time_step) + time_min_slider_value)}\nInline: {int((coord[1] * inline_step) + inline_min_slider_value)}\nCrossline: {int((coord[2] * xline_step) + xline_min_slider_value)}\n"
                              f"X: {earth_coordinates[0]:.2f}\nY: {earth_coordinates[1]:.2f}\n"
                              f"Value: {value:.2f}", self)

        except Exception as e:
            QMessageBox.critical(self.tensor_visualizer, "Error", f"An error occurred: {e}")

    def mouseReleaseEvent(self, event):
        if event.button() == Qt.MouseButton.LeftButton:
            if self.edit_mode:
                self.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))
            else:
                self.setCursor(Qt.CursorShape.ArrowCursor)
        super().mouseReleaseEvent(event)

    def enterEvent(self, event):
        # Set cursor to open hand when entering the widget
        if self.edit_mode:
            self.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))
        super().enterEvent(event)

    def leaveEvent(self, event):
        # Revert cursor to arrow when leaving the widget
        self.setCursor(QCursor(Qt.CursorShape.ArrowCursor))
        super().leaveEvent(event)


class CustomColorBarItem(ColorBarItem):
    def __init__(self, *args, **kwargs):
        super(CustomColorBarItem, self).__init__(*args, **kwargs)
        # Disable the default context menu
        self.setMenuEnabled(False)


class ResNet1D_Model(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(ResNet1D_Model, self).__init__()
        self.model = resnet50(weights=None)  # Initialize without pre-trained weights

        # Modify the first convolutional layer to accept 1D input
        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)

        # Replace the fully connected layer to match the number of input features and output the correct number of values
        self.model.fc = nn.Linear(self.model.fc.in_features, output_dim)

        # Additional linear layer to convert flattened 1D input to a suitable shape for conv layer
        self.fc1 = nn.Linear(input_dim, 64 * 7 * 7)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)  # Apply the linear layer to get the right shape
        x = self.relu(x)
        x = x.view(-1, 1, 56, 56)  # Reshape to (batch_size, channels, height, width)
        return self.model(x)


class Chomp1dLayer(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1dLayer, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()


class TemporalBlockLayer(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlockLayer, self).__init__()
        self.conv1 = weight_norm(
            nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = weight_norm(
            nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1, self.conv2, self.chomp2,
                                 self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TCN_Model(nn.Module):
    def __init__(self, num_inputs, num_channels, num_outputs, kernel_size=2, dropout=0.2):
        super(TCN_Model, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i - 1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                     padding=(kernel_size - 1) * dilation_size, dropout=dropout)]

        self.network = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], num_outputs)

    def forward(self, x):
        x = self.network(x)
        x = x.mean(dim=2)  # Global average pooling
        return self.fc(x)


class AddressBar(QLineEdit):
    def __init__(self, parent=None):
        super().__init__(parent)
        self._select_all_on_focus = False

    # Override the mousePressEvent method
    def mousePressEvent(self, event):
        super().mousePressEvent(event)
        if not self._select_all_on_focus:
            self.selectAll()
            self._select_all_on_focus = True

    # Override focusInEvent to reset selection state
    def focusInEvent(self, event):
        super().focusInEvent(event)
        self._select_all_on_focus = False

    # Override focusOutEvent to reset selection state
    def focusOutEvent(self, event):
        super().focusOutEvent(event)
        self._select_all_on_focus = False
        self.setCursorPosition(0)  # Moves the cursor to the beginning
        self.home(False)  # Scrolls the view to the start of the text

    def setText(self, text):
        super().setText(text)
        # After setting the text, ensure the cursor is at the start (showing the beginning of the text)
        self.setCursorPosition(0)  # Moves the cursor to the beginning
        self.home(False)  # Scrolls the view to the start of the text

    def keyPressEvent(self, event):
        if event.key() in (Qt.Key.Key_Return, Qt.Key.Key_Enter):
            # Check if all the text or any part of the text is selected
            if self.hasSelectedText():
                # Call the base class keyPressEvent to handle default behavior
                super().keyPressEvent(event)
                self.clearFocus()
            else:
                super().keyPressEvent(event)
                self.clearFocus()
        else:
            # For all other key events, pass them to the parent method
            super().keyPressEvent(event)


class PowerShellWindow(QWidget):
    # Signal to notify when terminal is ready to be displayed
    terminal_ready = pyqtSignal()

    def __init__(self, terminal_type):
        super().__init__()
        self.powershell_hwnd = None
        self.layout = QVBoxLayout(self)
        self.layout.setContentsMargins(0, 0, 0, 0)
        self.terminal_type = terminal_type
        self.is_terminal_ready = False

        # Hide the widget initially
        self.hide()

        # Start terminal process in the background
        QTimer.singleShot(0, self.start_terminal)

        # Timer for window adjustments
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.adjust_terminal_window)
        self.timer.start(100)

    def get_bundled_python_path(self):
        """Get the path to the bundled Python interpreter dynamically"""
        if getattr(sys, 'frozen', False):  # If running in a PyInstaller bundle
            # New location for python.exe when not using --onefile
            # Assumes the "internal" directory is next to the executable
            exe_dir = os.path.dirname(sys.executable)  # Directory of the main executable
            internal_path = os.path.join(exe_dir, '_internal')  # Path to the internal folder
            seismic_path = os.path.join(internal_path, 'seismic')  # Path to the seismic folder within internal
            bundled_python_path = os.path.join(seismic_path, 'python.exe')  # Path to the python executable

            if not os.path.exists(bundled_python_path):
                bundled_python_path = sys.executable
        else:
            # Running in a normal Python environment
            bundled_python_path = sys.executable

        return bundled_python_path

    def prepare_environment(self):
        """Prepare environment variables for the terminal process"""
        bundled_python_path = self.get_bundled_python_path()
        python_dir = os.path.dirname(bundled_python_path)
        new_env = os.environ.copy()
        new_env["VIRTUAL_ENV"] = python_dir  # Point to the current Python virtual environment
        new_env["PATH"] = f"{python_dir};{new_env['PATH']}"  # Prepend the Python directory to PATH
        return new_env

    def create_hidden_terminal_process(self, command, env):
        """Create a terminal process that starts hidden"""
        startup_info = win32process.STARTUPINFO()
        startup_info.dwFlags = win32con.STARTF_USESHOWWINDOW
        startup_info.wShowWindow = win32con.SW_HIDE  # Start the process hidden

        # Create the process
        process_handle, thread_handle, pid, tid = win32process.CreateProcess(
            None, command, None, None, False,
            win32con.CREATE_NEW_CONSOLE, env, None, startup_info
        )

        return pid

    def find_terminal_window(self, pid, max_attempts=50, delay=0.1):
        """Find the terminal window by process ID with timeout"""

        def callback(hwnd, hwnds):
            # Check even hidden windows to ensure we find it
            if win32gui.IsWindowVisible(hwnd) or not win32gui.IsWindowVisible(hwnd):
                _, found_pid = win32process.GetWindowThreadProcessId(hwnd)
                if found_pid == pid:
                    hwnds.append(hwnd)
            return True

        for _ in range(max_attempts):
            hwnds = []
            win32gui.EnumWindows(callback, hwnds)
            if hwnds:
                return hwnds[0]
            time.sleep(delay)

        return None

    def embed_terminal(self, hwnd):
        """Embed the terminal window into our widget"""
        if hwnd:
            # Keep the window hidden during embedding
            win32gui.ShowWindow(hwnd, win32con.SW_HIDE)

            # Remove window decorations
            style = win32gui.GetWindowLong(hwnd, win32con.GWL_STYLE)
            style = style & ~(win32con.WS_CAPTION | win32con.WS_THICKFRAME)
            win32gui.SetWindowLong(hwnd, win32con.GWL_STYLE, style)

            # Set as child of our widget
            win32gui.SetParent(hwnd, int(self.winId()))

            # Adjust size and position
            self.adjust_terminal_window()

            # Now show the embedded window
            win32gui.ShowWindow(hwnd, win32con.SW_SHOW)
            win32gui.SetForegroundWindow(hwnd)

            return True
        return False

    def start_terminal(self):
        """Start the terminal process and embed it"""
        new_env = self.prepare_environment()

        # Set the appropriate command with execution policy for PowerShell
        if self.terminal_type == "CMD":
            command = "cmd.exe"
        else:  # PowerShell
            # Include the Execution Policy Bypass to ensure scripts can run
            command = "powershell.exe -ExecutionPolicy Bypass"

        # Create the terminal process (hidden initially)
        pid = self.create_hidden_terminal_process(command, new_env)

        # Find the window (it might be hidden)
        self.powershell_hwnd = self.find_terminal_window(pid)

        if self.powershell_hwnd:
            # Embed the terminal
            if self.embed_terminal(self.powershell_hwnd):
                self.is_terminal_ready = True
                self.terminal_ready.emit()  # Signal that terminal is ready
                self.show()  # Show our widget now that terminal is ready

    def adjust_terminal_window(self):
        """Adjust the terminal window size to match our widget"""
        if self.powershell_hwnd and self.isVisible():
            # Get the client area of our widget
            client_rect = win32gui.GetClientRect(int(self.winId()))
            width = client_rect[2] - client_rect[0]
            height = client_rect[3] - client_rect[1]

            # Move and resize the terminal window
            win32gui.MoveWindow(self.powershell_hwnd, 0, 0, width, height, True)

            # Force a redraw
            win32gui.RedrawWindow(
                self.powershell_hwnd, None, None,
                win32con.RDW_INVALIDATE | win32con.RDW_ALLCHILDREN
            )

    def resizeEvent(self, event):
        """Handle resize events"""
        super().resizeEvent(event)
        self.adjust_terminal_window()

    def showEvent(self, event):
        """Handle show events"""
        super().showEvent(event)
        self.adjust_terminal_window()

    def closeEvent(self, event):
        """Clean up when closing"""
        if self.powershell_hwnd:
            # Terminate the terminal process
            # Get the process ID associated with the terminal window
            _, pid = win32process.GetWindowThreadProcessId(self.powershell_hwnd)

            # Terminate the process
            process_handle = win32process.OpenProcess(
                win32con.PROCESS_TERMINATE, False, pid
            )
            win32process.TerminateProcess(process_handle, 0)
            win32api.CloseHandle(process_handle)

        # Stop the timer
        self.timer.stop()

        super().closeEvent(event)


a2e = [
    0, 1, 2, 3, 55, 45, 46, 47, 22, 5, 37, 11, 12, 13, 14, 15,
    16, 17, 18, 19, 60, 61, 50, 38, 24, 25, 63, 39, 28, 29, 30, 31,
    64, 79, 127, 123, 91, 108, 80, 125, 77, 93, 92, 78, 107, 96, 75, 97,
    240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 122, 94, 76, 126, 110, 111,
    124, 193, 194, 195, 196, 197, 198, 199, 200, 201, 209, 210, 211, 212, 213, 214,
    215, 216, 217, 226, 227, 228, 229, 230, 231, 232, 233, 74, 224, 90, 95, 109,
    121, 129, 130, 131, 132, 133, 134, 135, 136, 137, 145, 146, 147, 148, 149, 150,
    151, 152, 153, 162, 163, 164, 165, 166, 167, 168, 169, 192, 106, 208, 161, 7,
    32, 33, 34, 35, 36, 21, 6, 23, 40, 41, 42, 43, 44, 9, 10, 27,
    48, 49, 26, 51, 52, 53, 54, 8, 56, 57, 58, 59, 4, 20, 62, 225,
    65, 66, 67, 68, 69, 70, 71, 72, 73, 81, 82, 83, 84, 85, 86, 87,
    88, 89, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 116, 117,
    118, 119, 120, 128, 138, 139, 140, 141, 142, 143, 144, 154, 155, 156, 157, 158,
    159, 160, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,
    184, 185, 186, 187, 188, 189, 190, 191, 202, 203, 204, 205, 206, 207, 218, 219,
    220, 221, 222, 223, 234, 235, 236, 237, 238, 239, 250, 251, 252, 253, 254, 255
]

e2a = [
    0, 1, 2, 3, 156, 9, 134, 127, 151, 141, 142, 11, 12, 13, 14, 15,
    16, 17, 18, 19, 157, 133, 8, 135, 24, 25, 146, 143, 28, 29, 30, 31,
    128, 129, 130, 131, 132, 10, 23, 27, 136, 137, 138, 139, 140, 5, 6, 7,
    144, 145, 22, 147, 148, 149, 150, 4, 152, 153, 154, 155, 20, 21, 158, 26,
    32, 160, 161, 162, 163, 164, 165, 166, 167, 168, 91, 46, 60, 40, 43, 33,
    38, 169, 170, 171, 172, 173, 174, 175, 176, 177, 93, 36, 42, 41, 59, 94,
    45, 47, 178, 179, 180, 181, 182, 183, 184, 185, 124, 44, 37, 95, 62, 63,
    186, 187, 188, 189, 190, 191, 192, 193, 194, 96, 58, 35, 64, 39, 61, 34,
    195, 97, 98, 99, 100, 101, 102, 103, 104, 105, 196, 197, 198, 199, 200, 201,
    202, 106, 107, 108, 109, 110, 111, 112, 113, 114, 203, 204, 205, 206, 207, 208,
    209, 126, 115, 116, 117, 118, 119, 120, 121, 122, 210, 211, 212, 213, 214, 215,
    216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,
    123, 65, 66, 67, 68, 69, 70, 71, 72, 73, 232, 233, 234, 235, 236, 237,
    125, 74, 75, 76, 77, 78, 79, 80, 81, 82, 238, 239, 240, 241, 242, 243,
    92, 159, 83, 84, 85, 86, 87, 88, 89, 90, 244, 245, 246, 247, 248, 249,
    48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 250, 251, 252, 253, 254, 255
]


@dataclass
class SeismicSurveyMetadata:
    """Class to store parsed seismic survey metadata"""
    source: str
    name: str
    survey_type: str
    inline_range: tuple[int, int]
    xline_range: tuple[int, int]
    crs: str
    x_range: tuple[float, float, float]
    y_range: tuple[float, float, float]
    time_range: tuple[float, float, float]
    lat_range: tuple[Optional[float], Optional[float], Optional[float]]
    long_range: tuple[Optional[float], Optional[float], Optional[float]]
    template: str
    trace_format: str
    coord_scale_factor: float


def parse_range_line(line: str) -> tuple[float, float, float]:
    """Parse a line containing min, max, and delta values"""

    matches = re.findall(r'min:\s*([-\d.]+)\s*max:\s*([-\d.]+)\s*delta:\s*([-\d.]+)', line)

    if matches:
        min_val, max_val, delta = matches[0]

        return (
            float(min_val) if min_val != '-' else None,
            float(max_val) if max_val != '-' else None,
            float(delta) if delta != '-' else None
        )

    return None, None, None


def second_parse_range_line(line: str) -> tuple[float, float, float]:
    """Parse a line containing min, max, and delta values, handling (template) and (data) keywords."""

    cleaned_line = re.sub(r'^.*?\((?:template|data)\)', '', line)

    matches = re.search(r'min:\s*([-~\d.]+)\s*max:\s*([-~\d.]+)\s*delta:\s*([-~\d.]+)', cleaned_line)

    if matches:
        min_val, max_val, delta = matches.groups()
        min_val = min_val.replace('~', '')
        max_val = max_val.replace('~', '')
        delta = delta.replace('~', '')

        return (
            float(min_val) if min_val != '-' else None,
            float(max_val) if max_val != '-' else None,
            float(delta) if delta != '-' else None
        )

    return None, None, None


def parse_ebcdic_header(header: str) -> tuple[SeismicSurveyMetadata, list[Any]] | SeismicSurveyMetadata:
    """Parse EBCDIC header and return structured metadata"""

    EBCDIC_lines = re.findall(r'(C\s*\d+.*?)(?=C\s*\d+|$)', header)

    lines = [re.sub(r'^C\s*\d+\s*', '', line.strip()) for line in EBCDIC_lines if line.strip()]

    try:
        source = lines[0].strip()

        name_line = lines[1].strip()

        name_match = re.search(r'Name:\s*(.*?)\s*Type:', name_line)
        type_match = re.search(r'Type:\s*(.*?)$', name_line)
        name = name_match.group(1).strip() if name_match else ''
        survey_type = type_match.group(1).strip() if type_match else ''
        survey_type = survey_type.rstrip('C').strip()

        inline_match = re.findall(r'First inline:\s*(\d+)\s*Last inline:\s*(\d+)',
                                  next((line for line in lines if 'First inline:' in line), ''))
        xline_match = re.findall(r'First xline:\s*(\d+)\s*Last xline:\s*(\d+)',
                                 next((line for line in lines if 'First xline:' in line), ''))
        inline_range = tuple(map(int, inline_match[0])) if inline_match else (None, None)
        xline_range = tuple(map(int, xline_match[0])) if xline_match else (None, None)

        crs_line = next((line for line in lines if 'CRS:' in line), '')
        crs = crs_line.split('CRS:')[1].strip() if crs_line else 'Unknown'
        crs = crs.rstrip('C').strip()

        x_range = parse_range_line(next((line for line in lines if 'X min:' in line), ''))
        y_range = parse_range_line(next((line for line in lines if 'Y min:' in line), ''))
        time_range = parse_range_line(next((line for line in lines if 'Time min:' in line), ''))
        lat_range = parse_range_line(next((line for line in lines if 'Lat min:' in line), ''))
        long_range = parse_range_line(next((line for line in lines if 'Long min:' in line), ''))

        template = None

        unrecognized_lines = []

        for line in lines:
            if '(template)' in line:
                template_match = re.search(r'(\w+)\s*\(template\)', line)
                template = template_match.group(1).strip() if template_match else 'Unknown'

            elif re.search(r'min:\s*[-\d.]+\s*max:\s*[-\d.]+\s*delta:\s*[-\d.]+', line):
                # Collect remaining unrecognized range lines
                unrecognized_lines.append(line)

        if len(unrecognized_lines) >= 2:
            # Directly access the last two lines for template and data
            if not template:
                template = unrecognized_lines[-2].split()[0]  # Second-to-last line for the template

        else:
            # If there are fewer than 2 lines, assign accordingly if possible
            if len(unrecognized_lines) == 1:
                if not template:
                    template = unrecognized_lines[0].split()[0]

        trace_format_line = next((line for line in lines if 'Trace sample format:' in line), '')
        trace_format = trace_format_line.split(':')[1].strip() if trace_format_line else 'Unknown'
        trace_format = re.sub(r'\s*C\d*$', '', trace_format).strip()

        coord_scale_line = next((line for line in lines if 'Coordinate scale factor:' in line), '')
        if coord_scale_line:
            coord_scale_str = coord_scale_line.split(':')[1].strip()
            coord_scale_factor = float(coord_scale_str)
        else:
            coord_scale_factor = None

        return (
            SeismicSurveyMetadata(
                source=source,
                name=name,
                survey_type=survey_type,
                inline_range=inline_range,
                xline_range=xline_range,
                crs=crs,
                x_range=x_range,
                y_range=y_range,
                time_range=time_range,
                lat_range=lat_range,
                long_range=long_range,
                template=template,
                trace_format=trace_format,
                coord_scale_factor=coord_scale_factor,
            ),
            EBCDIC_lines
        )
    except Exception as e:

        return (
            SeismicSurveyMetadata(
                source="Error", name="Error", survey_type="Unknown",
                inline_range=(None, None), xline_range=(None, None),
                crs="Unknown", x_range=(None, None, None),
                y_range=(None, None, None), time_range=(None, None, None),
                lat_range=(None, None, None), long_range=(None, None, None),
                template="Unknown",
                trace_format="Unknown",
                coord_scale_factor=None
            ),
            EBCDIC_lines
        )


class RangeDialog(QDialog):
    def __init__(self, parent, max_value):
        super().__init__(parent)
        self.setWindowTitle("Component Range Selection")
        layout = QVBoxLayout()

        # Start component selection
        start_layout = QHBoxLayout()
        start_layout.addWidget(QLabel("Start component:"))
        self.start_spin = QSpinBox()
        self.start_spin.setRange(1, max_value)
        start_layout.addWidget(self.start_spin)
        layout.addLayout(start_layout)

        # End component selection
        end_layout = QHBoxLayout()
        end_layout.addWidget(QLabel("End component:"))
        self.end_spin = QSpinBox()
        self.end_spin.setRange(1, max_value)
        end_layout.addWidget(self.end_spin)
        layout.addLayout(end_layout)

        # Connect spin box signals
        self.start_spin.valueChanged.connect(self._update_end_minimum)
        self.end_spin.setValue(max_value)  # Set initial end value

        # Buttons
        buttons = QDialogButtonBox(
            QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel
        )
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addWidget(buttons)

        self.setLayout(layout)

    def _update_end_minimum(self, value):
        self.end_spin.setMinimum(value)

    def get_values(self):
        return self.start_spin.value(), self.end_spin.value()


# Function to display the CSV viewer and capture user selection
def show_csv_in_dialog_segy(trace_headers, parent):
    selected_columns = []

    class TableView(QTableView):
        def __init__(self, parent=None):
            super(TableView, self).__init__(parent)
            self.setSelectionMode(QTableView.SelectionMode.MultiSelection)

            # Set model and connect the selection model after the model is set
            self.setModel(create_pandas_model_segy(trace_headers))
            self.selectionModel().selectionChanged.connect(self.selection_changed)

        def selection_changed(self, selected, deselected):
            for index in selected.indexes():
                if index.column() not in selected_columns and len(selected_columns) < 4:
                    selected_columns.append(index.column())
                if len(selected_columns) > 4:
                    QMessageBox.warning(self, "Selection Error", "Please select only 4 columns.")
                    selected_columns.pop()

            for index in deselected.indexes():
                if index.column() in selected_columns:
                    selected_columns.remove(index.column())

    dialog = QDialog(parent)
    dialog.setWindowTitle("Select Headers")
    dialog.resize(800, 600)
    layout = QVBoxLayout(dialog)

    table_view = TableView(dialog)
    layout.addWidget(table_view)

    dialog.setLayout(layout)
    dialog.exec()

    return selected_columns  # Returns the order of selected columns


def create_pandas_model_segy(trace_headers):
    class PandasModel(QAbstractTableModel):
        def __init__(self, data):
            super(PandasModel, self).__init__()
            self._data = data

        def rowCount(self, parent=None):
            return self._data.shape[0]

        def columnCount(self, parent=None):
            return self._data.shape[1]

        def data(self, index, role=Qt.ItemDataRole.DisplayRole):
            if role == Qt.ItemDataRole.DisplayRole:
                return str(self._data.iloc[index.row(), index.column()])
            return None

        def headerData(self, section, orientation, role=Qt.ItemDataRole.DisplayRole):
            if role == Qt.ItemDataRole.DisplayRole:
                if orientation == Qt.Orientation.Horizontal:
                    return str(self._data.columns[section])
                if orientation == Qt.Orientation.Vertical:
                    return str(self._data.index[section])
            return None

    return PandasModel(trace_headers)


def show_confirmation_dialog(mapping, parent):
    dialog = QDialog(parent)
    dialog.setWindowTitle("Confirm Column Mapping")
    dialog.resize(235, 194)

    layout = QVBoxLayout(dialog)

    # Create a table widget to show the mapping
    table = QTableWidget(len(mapping),
                         2, parent)  # Set the number of rows to the length of mapping and columns to 2
    table.setHorizontalHeaderLabels(["Chosen Column", "Target Column"])
    table.setEditTriggers(QTableWidget.EditTrigger.NoEditTriggers)  # Make the table non-editable

    # Fill the table with the mapping data
    for row, (key, value) in enumerate(mapping.items()):
        table.setItem(row, 0, QTableWidgetItem(key))
        table.setItem(row, 1, QTableWidgetItem(value))

    layout.addWidget(table)

    # Buttons for OK and Cancel
    button_layout = QHBoxLayout()

    ok_button = QPushButton("OK")
    cancel_button = QPushButton("Cancel")

    button_layout.addWidget(ok_button)
    button_layout.addWidget(cancel_button)

    layout.addLayout(button_layout)

    # Connect buttons to actions
    def on_ok():
        dialog.accept()  # Confirm and close the dialog

    def on_cancel():
        dialog.reject()  # Cancel and close the dialog

    ok_button.clicked.connect(on_ok)
    cancel_button.clicked.connect(on_cancel)

    dialog.setLayout(layout)

    return dialog.exec()  # Returns QDialog.DialogCode.Accepted or QDialog.DialogCode.Rejected


class ConfirmableTextEdit(QTextEdit):
    def __init__(self, original_text, metadata, parent=None):
        super().__init__(parent)
        self.parent = parent
        self.metadata = metadata
        self.setPlainText(original_text)
        self.setReadOnly(False)
        self.setMinimumHeight(650)

    def focusOutEvent(self, event):
        new_text = self.toPlainText()
        if new_text != "\n".join(line.strip() for line in self.metadata.get('ebcdic_header_metadata', [])):
            confirm = QMessageBox.question(
                self.parent,
                "Confirm Change",
                "Do you want to update the EBCDIC header metadata?",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
            )
            if confirm == QMessageBox.StandardButton.Yes:
                self.metadata['ebcdic_header_metadata'] = new_text.splitlines()
            else:
                self.setPlainText("\n".join(line.strip() for line in self.metadata.get('ebcdic_header_metadata', [])))  # Revert if declined)  # Revert if declined
        super().focusOutEvent(event)  # Call the default behavior


class WorkerSignals(QObject):
    """Defines the signals available from a running worker thread."""
    finished = pyqtSignal(object)
    error = pyqtSignal(str)
    progress = pyqtSignal(int)
    terminated = pyqtSignal()
    output_memory_needed = pyqtSignal(dict)  # Signal for when shared memory is needed for output


# Function to be used by the process wrapper
def _shared_memory_wrapper(func, shared_args, kwargs, return_dict, output_ready_event, continue_event):
    """Process wrapper function that handles shared memory arrays."""
    # Track shared memory blocks that need to be closed
    input_shared_mem_blocks = []
    output_shm_blocks = {}  # Track multiple output shared memory blocks

    try:
        # Process all args
        processed_args = []
        for arg_info in shared_args:
            if arg_info['type'] == 'numpy':
                # Get the shared memory block
                shm = shared_memory.SharedMemory(name=arg_info['shm_name'])
                input_shared_mem_blocks.append(shm)
                # Reconstruct numpy array from shared memory
                shape = arg_info['shape']
                dtype = np.dtype(arg_info['dtype'])
                # Create numpy array using the shared memory buffer
                array = np.ndarray(shape, dtype=dtype, buffer=shm.buf)
                processed_args.append(array)
            else:
                # Regular argument, pass as is
                processed_args.append(arg_info['value'])

        # Call the actual function with processed args
        result = func(*processed_args, **kwargs)

        # Process the result
        if isinstance(result, tuple):
            # Handle multiple return values
            return_dict['is_tuple'] = True
            return_dict['tuple_length'] = len(result)

            # Store array indices and their metadata
            numpy_indices = []

            # First pass: identify which elements are NumPy arrays
            for idx, item in enumerate(result):
                if isinstance(item, np.ndarray):
                    numpy_indices.append(idx)
                    return_dict[f'item_{idx}_type'] = 'numpy'
                    return_dict[f'item_{idx}_shape'] = item.shape
                    return_dict[f'item_{idx}_dtype'] = str(item.dtype)
                    return_dict[f'item_{idx}_nbytes'] = item.nbytes
                else:
                    return_dict[f'item_{idx}_type'] = 'other'
                    return_dict[f'item_{idx}_value'] = item

            # If we have numpy arrays, request shared memory
            if numpy_indices:
                return_dict['numpy_indices'] = numpy_indices

                # Set the output ready event to notify the main thread
                output_ready_event.set()

                # Wait for the main thread to create shared memory and signal continue
                continue_event.wait()

                # Now process each numpy array with its allocated shared memory
                for idx in numpy_indices:
                    output_shm_name = return_dict.get(f'output_shm_name_{idx}')

                    if output_shm_name:
                        # Access the shared memory block created by the main thread
                        output_shm = shared_memory.SharedMemory(name=output_shm_name)
                        output_shm_blocks[idx] = output_shm

                        # Create a numpy array backed by the shared memory
                        output_array = np.ndarray(
                            return_dict[f'item_{idx}_shape'],
                            dtype=np.dtype(return_dict[f'item_{idx}_dtype']),
                            buffer=output_shm.buf
                        )

                        # Copy the result data to the shared memory
                        np.copyto(output_array, result[idx])
                    else:
                        raise Exception(f"No shared memory name provided for numpy array at index {idx}")

        elif isinstance(result, np.ndarray):
            # Single numpy array result
            return_dict['is_tuple'] = False
            return_dict['type'] = 'numpy'
            return_dict['shape'] = result.shape
            return_dict['dtype'] = str(result.dtype)
            return_dict['nbytes'] = result.nbytes

            # Set the output ready event to notify the main thread
            output_ready_event.set()

            # Wait for the main thread to create shared memory and signal continue
            continue_event.wait()

            # Get the shared memory name from return_dict
            output_shm_name = return_dict.get('output_shm_name')

            if output_shm_name:
                # Access the shared memory block created by the main thread
                output_shm = shared_memory.SharedMemory(name=output_shm_name)
                output_shm_blocks[0] = output_shm

                # Create a numpy array backed by the shared memory
                output_array = np.ndarray(
                    result.shape,
                    dtype=result.dtype,
                    buffer=output_shm.buf
                )

                # Copy the result data to the shared memory
                np.copyto(output_array, result)
            else:
                raise Exception("No shared memory name provided by main thread")
        else:
            # For other types, just store directly in return_dict
            return_dict['is_tuple'] = False
            return_dict['type'] = 'other'
            return_dict['value'] = result

        return_dict['status'] = 'success'

    except Exception as e:
        try:
            return_dict['status'] = 'error'
            return_dict['error_msg'] = str(e) + "\n" + traceback.format_exc()
        except:
            return_dict['status'] = 'error'  # At least set status if error_msg fails
            return_dict['error_msg'] = 'Unknown error in child process'
    finally:
        # Clean up shared memory blocks
        for shm in input_shared_mem_blocks:
            shm.close()
        for shm in output_shm_blocks.values():
            shm.close()


class TaskRunner:
    """Helper class to run tasks safely in a way that keeps the GUI responsive.
    Handles numpy arrays using SharedMemory with dynamic output sizes."""

    def __init__(self, parent):
        self.parent = parent
        self._process = None
        self._event_loop = None
        self._task_result = None
        self._signals = WorkerSignals()
        self._check_timer = None
        self._original_timer_event = None
        self._input_shared_mems = []  # Track input shared memory blocks
        self._output_shared_mem = None  # Track output shared memory block

        # Events for communication between processes
        self._output_ready_event = None
        self._continue_event = None

        # Manager for return dictionary
        self._manager = None
        self._return_dict = None

        # Connect signals
        self._signals.output_memory_needed.connect(self._handle_output_memory_request)

    def is_running(self):
        """Check if a task is currently running."""
        return self._process is not None and self._process.is_alive()

    def run_task(self, task_function, *args, **kwargs):
        """Run a task safely using multiprocessing with shared memory, allowing GUI interactions.

        Args:
            task_function: The function to execute in a separate process
            *args: Arguments to pass to the function
            **kwargs: Keyword arguments to pass to the function

        Returns:
            The result of the task, or None if it failed or was terminated
        """
        if self.is_running():
            QMessageBox.warning(self.parent, "Warning", "A task is already running. Stop it first.")
            return None

        self._task_result = None  # Reset result
        self._cleanup_shared_memory()  # Clean up any existing shared memory

        # Signal task started to update UI
        self.parent.on_task_started()

        # Create event loop for this task execution
        self._event_loop = QEventLoop()

        # Create a manager for the return dictionary
        self._manager = multiprocessing.Manager()
        self._return_dict = self._manager.dict()

        # Create events for inter-process communication
        self._output_ready_event = multiprocessing.Event()
        self._continue_event = multiprocessing.Event()

        # Process all arguments for sharing
        shared_args = []

        # Now process all arguments for sharing
        for arg in args:
            if isinstance(arg, np.ndarray):
                # Process numpy array for shared memory
                # Create a shared memory block for input
                shm = shared_memory.SharedMemory(create=True, size=arg.nbytes)

                # Create a NumPy array that uses the shared memory
                shared_array = np.ndarray(arg.shape, dtype=arg.dtype, buffer=shm.buf)

                # Copy the data to shared memory
                np.copyto(shared_array, arg)

                # Store reference to shared memory block
                self._input_shared_mems.append(shm)

                # Add to shared args list
                shared_args.append({
                    'type': 'numpy',
                    'shm_name': shm.name,
                    'shape': arg.shape,
                    'dtype': str(arg.dtype),
                    'nbytes': arg.nbytes
                })
            else:
                # Regular argument, just pass the value
                shared_args.append({
                    'type': 'other',
                    'value': arg
                })

        # Connect signals
        self._signals.finished.connect(self._on_task_finished)
        self._signals.error.connect(self._on_task_error)
        self._signals.terminated.connect(self._on_task_terminated)

        # Save the original timerEvent
        if hasattr(self.parent, 'timerEvent'):
            self._original_timer_event = self.parent.timerEvent

        # Start a timer to periodically check if the process is still running
        # and also check for output ready events
        self._check_timer = self.parent.startTimer(100)  # Check every 100ms

        # Create a new timerEvent method
        def custom_timer_event(event):
            if event.timerId() == self._check_timer:
                # Check if process is finished
                if not self._process or not self._process.is_alive():
                    self._check_process_status()
                # Check if output ready event is set
                elif self._output_ready_event and self._output_ready_event.is_set():
                    # Emit signal to handle output memory request in the main thread
                    output_info = {
                        'shape': self._return_dict.get('shape'),
                        'dtype': self._return_dict.get('dtype'),
                        'nbytes': self._return_dict.get('nbytes')
                    }
                    self._signals.output_memory_needed.emit(output_info)
            elif self._original_timer_event:
                self._original_timer_event(event)

        # Replace the timerEvent method
        self.parent.timerEvent = custom_timer_event

        # Start the process
        try:
            self._process = multiprocessing.Process(
                target=_shared_memory_wrapper,
                args=(task_function, shared_args, kwargs, self._return_dict,
                      self._output_ready_event, self._continue_event)
            )
            self._process.daemon = True  # Daemonize process
            self._process.start()
        except Exception as e:
            self._signals.error.emit(f"Failed to start process: {str(e)}\n{traceback.format_exc()}")
            self._cleanup()
            return None

        # Process events while waiting, making the UI responsive
        self._event_loop.exec()

        return self._task_result

    def _handle_output_memory_request(self, output_info):
        """Handle a request from the child process for output shared memory.
        This runs in the main thread."""
        try:
            # Reset the output ready event
            self._output_ready_event.clear()

            # Check if this is a tuple result with multiple NumPy arrays
            if 'numpy_indices' in self._return_dict:
                # Create a shared memory block for each NumPy array
                self._output_shared_mems = {}  # Replace single shared memory with dictionary

                for idx in self._return_dict['numpy_indices']:
                    nbytes = self._return_dict[f'item_{idx}_nbytes']

                    # Create a shared memory block of the requested size
                    output_shm = shared_memory.SharedMemory(create=True, size=nbytes)
                    self._output_shared_mems[idx] = output_shm

                    # Store the shared memory name in the return dict
                    self._return_dict[f'output_shm_name_{idx}'] = output_shm.name
            else:
                # Extract the size information for single NumPy array
                nbytes = output_info['nbytes']

                # Create a shared memory block of the requested size
                output_shm = shared_memory.SharedMemory(create=True, size=nbytes)
                self._output_shared_mem = output_shm

                # Store the shared memory name in the return dict
                self._return_dict['output_shm_name'] = output_shm.name

            # Signal the child process to continue
            self._continue_event.set()
        except Exception as e:
            # If there's an error, terminate the process
            self._signals.error.emit(f"Error creating output shared memory: {str(e)}\n{traceback.format_exc()}")
            self.stop_task()

    def _check_process_status(self):
        """Check if the process has completed."""
        # Kill the timer
        self.parent.killTimer(self._check_timer)
        self._check_timer = None

        # Check the status from return_dict
        if 'status' in self._return_dict:
            if self._return_dict['status'] == 'success':
                # Check if the result is a tuple
                if self._return_dict.get('is_tuple', False):
                    # Handle tuple result
                    tuple_length = self._return_dict['tuple_length']
                    result_list = []

                    # Process each item in the tuple
                    for idx in range(tuple_length):
                        item_type = self._return_dict[f'item_{idx}_type']

                        if item_type == 'numpy':
                            try:
                                shape = self._return_dict[f'item_{idx}_shape']
                                dtype = np.dtype(self._return_dict[f'item_{idx}_dtype'])

                                # Get the shared memory for this index
                                output_shm = self._output_shared_mems.get(idx)

                                # Create a view into the shared memory
                                array_view = np.ndarray(
                                    shape, dtype=dtype, buffer=output_shm.buf
                                )

                                # Create a copy of the data to return
                                result_list.append(np.copy(array_view))
                            except Exception as e:
                                self._signals.error.emit(
                                    f"Error accessing numpy result at index {idx}: {str(e)}\n{traceback.format_exc()}")
                                result_list.append(None)
                        else:
                            # Other types, just use the value
                            result_list.append(self._return_dict[f'item_{idx}_value'])

                    # Convert list back to tuple
                    result = tuple(result_list)
                else:
                    # Process the single result based on its type
                    if self._return_dict['type'] == 'numpy':
                        try:
                            shape = self._return_dict['shape']
                            dtype = np.dtype(self._return_dict['dtype'])

                            # Create a view into the shared memory
                            array_view = np.ndarray(
                                shape, dtype=dtype, buffer=self._output_shared_mem.buf
                            )

                            # Create a copy of the data to return
                            result = np.copy(array_view)
                        except Exception as e:
                            self._signals.error.emit(f"Error accessing result data: {str(e)}\n{traceback.format_exc()}")
                            result = None
                    else:
                        # Other types, just use the value
                        result = self._return_dict['value']

                self._signals.finished.emit(result)
            elif self._return_dict['status'] == 'error':
                self._signals.error.emit(self._return_dict.get('error_msg', 'Unknown error in child process'))
        else:
            # Process terminated without setting status
            self._signals.terminated.emit()

    def stop_task(self):
        """Properly terminate the current task if one is running."""
        if not self.is_running():
            return False

        # Terminate the process
        self._process.terminate()
        self._process.join(0.5)  # Give it some time to terminate

        # If it's still alive, try to kill it more forcefully
        if self._process.is_alive():
            try:
                os.kill(self._process.pid, signal.SIGKILL)
            except:
                pass

        # Clean up
        self._process = None
        self._signals.terminated.emit()

        # Force quit the event loop
        if self._event_loop and self._event_loop.isRunning():
            self._event_loop.quit()

        return True

    def _on_task_finished(self, result):
        """Handle task completion."""
        self._task_result = result  # Store result before cleanup

        if self._event_loop and self._event_loop.isRunning():
            self._event_loop.quit()

        # Clean up resources
        self._cleanup()

        # Signal task ended to update UI
        self.parent.on_task_ended()

    def _on_task_error(self, error_msg):
        """Handle task errors."""
        self.parent.busy_label.hide()
        QMessageBox.critical(self.parent, "Error", error_msg)

        if self._event_loop and self._event_loop.isRunning():
            self._event_loop.quit()

        # Clean up resources
        self._cleanup()

        # Signal task ended to update UI
        self.parent.on_task_ended()

    def _on_task_terminated(self):
        """Handle process termination."""
        self._task_result = None

        if self._event_loop and self._event_loop.isRunning():
            self._event_loop.quit()

        # Clean up resources
        self._cleanup()

        # Signal task ended to update UI
        self.parent.on_task_ended()

    def _cleanup_shared_memory(self):
        """Clean up any shared memory resources."""
        # Clean up input shared memory
        for shm in self._input_shared_mems:
            try:
                shm.close()
                shm.unlink()
            except Exception:
                pass
        self._input_shared_mems = []

        # Clean up single output shared memory
        if self._output_shared_mem:
            try:
                self._output_shared_mem.close()
                self._output_shared_mem.unlink()
            except Exception:
                pass
            self._output_shared_mem = None

        # Clean up multiple output shared memories if they exist
        if hasattr(self, '_output_shared_mems') and self._output_shared_mems:
            for shm in self._output_shared_mems.values():
                try:
                    shm.close()
                    shm.unlink()
                except Exception:
                    pass
            self._output_shared_mems = {}

        # Clean up events
        self._output_ready_event = None
        self._continue_event = None

    def _cleanup(self):
        """Clean up all resources."""
        if self._check_timer:
            self.parent.killTimer(self._check_timer)
            self._check_timer = None

        # Restore original timerEvent
        if hasattr(self, '_original_timer_event') and self._original_timer_event:
            self.parent.timerEvent = self._original_timer_event
            self._original_timer_event = None

        if self._process:
            # Ensure process is truly terminated
            if self._process.is_alive():
                self._process.terminate()
                self._process.join(0.5)
            self._process = None

        # Clean up shared memory
        self._cleanup_shared_memory()

        # Clean up manager resources
        if self._manager:
            self._return_dict = None
            self._manager.shutdown()
            self._manager = None


class PandaswellinfoModel(QAbstractTableModel):
    def __init__(self, data):
        super().__init__()
        self._data = data
        self._original_data = data.copy()
        self._modified = False

    def rowCount(self, parent=None):
        return self._data.shape[0]

    def columnCount(self, parent=None):
        return self._data.shape[1]

    def data(self, index, role=Qt.ItemDataRole.DisplayRole):
        if index.isValid():
            if role == Qt.ItemDataRole.DisplayRole or role == Qt.ItemDataRole.EditRole:
                value = self._data.iloc[index.row(), index.column()]
                return str(value)
        return None

    def headerData(self, section, orientation, role):
        if role == Qt.ItemDataRole.DisplayRole:
            if orientation == Qt.Orientation.Horizontal:
                return str(self._data.columns[section])
            if orientation == Qt.Orientation.Vertical:
                return str(self._data.index[section])
        return None

    def setData(self, index, value, role):
        if role == Qt.ItemDataRole.EditRole:
            try:
                self._data.iloc[index.row(), index.column()] = value
                self._modified = True
                # Emit the dataChanged signal
                self.dataChanged.emit(index, index, [role])
                return True
            except Exception:
                return False
        return False

    def flags(self, index):
        return Qt.ItemFlag.ItemIsSelectable | Qt.ItemFlag.ItemIsEnabled | Qt.ItemFlag.ItemIsEditable

    def is_modified(self):
        return self._modified

    def get_data(self):
        return self._data

    def reset_to_original(self):
        self._data = self._original_data.copy()
        self._modified = False


def extract_epsg(crs_str):
    """Extract EPSG code from CRS string."""
    # Handle the original format with [EPSG,...]
    if '[EPSG,' in crs_str:
        start = crs_str.find('[EPSG,') + 6
        end = crs_str.find(']', start)
        return 'EPSG:' + crs_str[start:end]

    # Handle the new format that ends with "EPSG,code"
    if ' EPSG,' in crs_str:
        code = crs_str.split(' EPSG,')[1].strip()
        return 'EPSG:' + code

    # Check if it might be just "EPSG,code" without spaces
    if crs_str.startswith('EPSG,'):
        code = crs_str.split('EPSG,')[1].strip()
        return 'EPSG:' + code

    # Check if the string is already in EPSG:code format
    if crs_str.startswith('EPSG:'):
        # Verify it's a valid EPSG code format (EPSG: followed by numbers)
        pattern = r'^EPSG:\d+$'
        if re.match(pattern, crs_str):
            return crs_str

    # If none of the conditions are met or the result is not in proper format
    return None


class StepSpinBox(QSpinBox):
    def __init__(self, parent=None):
        super().__init__(parent)

    def setValue(self, value):
        """Override to enforce step alignment on ANY value change"""
        if self.singleStep() <= 0:
            return super().setValue(self.minimum())

        min_val = self.minimum()
        max_val = self.maximum()
        step = self.singleStep()

        # Calculate nearest valid step within range
        k_max = (max_val - min_val) // step
        k = round((value - min_val) / step)
        k_clamped = max(0, min(k, k_max))
        corrected = min_val + k_clamped * step

        super().setValue(corrected)

    def validate(self, input_str, pos):
        """Prevent invalid intermediate states during typing"""
        state, _, _ = super().validate(input_str, pos)

        if state != QValidator.State.Acceptable:
            return (state, input_str, pos)

        try:
            value = int(input_str)
        except ValueError:
            return (QValidator.State.Intermediate, input_str, pos)

        step = self.singleStep()
        min_val = self.minimum()

        if (value - min_val) % step != 0:
            return (QValidator.State.Intermediate, input_str, pos)

        return (QValidator.State.Acceptable, input_str, pos)

    def fixup(self, input_str):
        """Final correction when invalid input is submitted"""
        try:
            value = int(input_str)
        except ValueError:
            value = self.minimum()

        self.setValue(value)  # Use our overridden setValue


class StepSlider(QSlider):
    # Optional: Signal to emit when value changes with enforcement
    enforcedValueChanged = pyqtSignal(int)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sliderReleased.connect(self._snap_to_nearest_step)

    def setValue(self, value):
        """Override to enforce step alignment when setting values"""
        rounded_value = self._round_to_step(value)
        super().setValue(rounded_value)
        self.enforcedValueChanged.emit(rounded_value)

    def _snap_to_nearest_step(self):
        """Snap to nearest step when user releases slider"""
        current = self.value()
        rounded = self._round_to_step(current)
        if rounded != current:
            self.setValue(rounded)

    def _round_to_step(self, value):
        """Calculate nearest valid step value within range"""
        step = self.singleStep()
        if step <= 0:
            return value

        min_val = self.minimum()
        max_val = self.maximum()

        # Handle edge case where range isn't divisible by step
        steps = round((value - min_val) / step)
        rounded = min_val + steps * step

        # Clamp to valid range
        return max(min(rounded, max_val), min_val)


class SizeInputDialog(QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Input Model Dimensions")

        # Create form layout
        layout = QFormLayout(self)

        # Create spin boxes with defaults and constraints
        self.depth_input = QSpinBox()
        self.depth_input.setRange(1, 10000)
        self.depth_input.setValue(128)

        self.height_input = QSpinBox()
        self.height_input.setRange(1, 10000)
        self.height_input.setValue(128)

        self.width_input = QSpinBox()
        self.width_input.setRange(1, 10000)
        self.width_input.setValue(128)

        # Add inputs to layout
        layout.addRow("Depth:", self.depth_input)
        layout.addRow("Height:", self.height_input)
        layout.addRow("Width:", self.width_input)

        # Add OK/Cancel buttons
        buttons = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addWidget(buttons)

    def getValues(self):
        return (self.depth_input.value(),
                self.height_input.value(),
                self.width_input.value())


class TensorVisualizer(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setFocusPolicy(Qt.FocusPolicy.ClickFocus)
        # Set up logging
        TensorVisualizer.setup_logging('Application')
        self.task_runner = TaskRunner(self)
        self.username = None
        self.password = None
        self.computer_id = None
        self.powershell_widget = None
        self.gpt_button = None
        self.terminal_button = None
        self.export_plot_spec_dialog = None
        self.export_petro_dialog = None
        self.export_well_dialog = None
        self.export_dialog = None
        self.init_custom_cursor()
        self.light_color_1 = "#B2B2B2"
        self.light_color_2 = "#CACACA"
        self.light_color_3 = "#333333"
        self.light_color_4 = "#EDEDED"
        self.light_color_5 = "#F0F0F0"
        self.light_color_6 = "#D9D9D9"
        self.light_color_7 = "#E8E8E8"
        self.light_color_8 = "#5C5C5C"
        self.light_color_9 = "#999999"
        self.light_color_10 = "#353535"
        # Define the unique colors used in the stylesheet
        self.darkColor1 = "#353535"
        self.darkColor2 = "#404040"
        self.darkColor3 = "#5C5C5C"
        self.darkColor4 = "#FFFFFF"
        self.darkColor5 = "white"  # alias for #FFFFFF
        self.darkColor6 = "Gainsboro"
        self.darkColor7 = "#555555"  # alias for #555
        self.darkColor8 = "#B2B2B2"
        self.darkColor9 = "#4C4C4C"
        self.darkColor10 = "#999999"
        self.darkColor11 = "#E8E8E8"
        self.darkColor12 = "#333333"  # alias for #333
        self.darkColor13 = "#DDDDDD"  # alias for #ddd
        self.loaded_file_paths = {}  # Initialize an empty dictionary for file paths
        self.recent_files_path = 'json.json'
        self.load_recent_files()  # Add this line to call the method on startup
        self.file_name = None
        self.intensity_tolerance = 0.6
        self.max_vertical_jump = 2
        self.isDarkTheme = False
        self.view_type = {0: '2D'}
        self.kmeans_dict = {}
        self.num_clusters = {}
        # Initialize the dictionary to store tensors
        self.tensor_dict = {}
        # UI elements
        self.grid_color = {}
        # Initialize state variables for Grid and Axes
        self.grid_active = True
        self.compass_active = True
        self.cell_mesh = False
        self.point_mesh = False
        self.three_d_color_bar_active = True
        self.tab_volume_items = {}
        self.grids = {}  # Dictionary to store grid instances for each tab
        self.multiprocessing_threshold = 181000000
        self.shade_threshold = 0.5
        self.window_size = 100
        self.three_d_interpolation = 1
        self.two_d_interpolation = 8
        self.last_selected_color = self.load_background_color()  # Default color
        self.last_grid_color = self.adjust_grid_color(self.last_selected_color)
        self.tab_3D_state = {}  # Dictionary to track the 3D plot state for each tab
        # Initialize the dictionary to store image items
        self.tensor_image_items = {}
        # Placeholder for tensor data
        self.tensor_data = None
        self.metadata = {}
        self.create_widgets()
        self.dark_mode_end_time, self.dark_mode_start_time = TensorVisualizer.get_sunrise_sunset_times()
        # Set up a timer to check the time every minute
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_theme_based_on_time)
        self.timer.start(60000)  # Check every 60 seconds
        self.update_theme_based_on_time()  # Initial theme update
        self.setWindowTitle("SeismicFlow")
        self.setWindowIcon(QIcon('icon.png'))  # Set the icon for the dialog
        self.cbar = {}
        self.interpolated_slice_2d = {}
        # Link the HistogramLUTItem to the ImageItem
        self.tab_last_plotted_data_name = {}  # Dictionary to track the file name of the last data plotted on each tab
        self.color_mapp = None
        self.units_dict = {}  # Add this to store units for each file
        self.splitter_state = self.splitter.saveState()
        self.well_color_mapping = 'jet'
        # Initialize the dictionary
        self.model_dict = {}
        self.well_deviation_dict = {}
        self.petro_models = {}
        self.petro_color_mapping = 'jet'
        self.tensor_metadata = {}  # New dictionary to store metadata
        self.checkshot_dict = {}
        self.welltop_dict = {}
        self.well_head_dict = {}
        self.well_calc_dialogue = None
        self.seismic_calc_dialogue = None
        self.well_header_info_library = {}
        self.geo_objects_color = (0.0, 0.502, 0.0, 1.0)  # RGBA for a moderately dark green
        self.graph_brush = (75, 75, 75)
        self.geophysical_Object_CMap = 'jet'
        self.terminal_processes = []  # Store the opened terminal processes
        self.browser_gpt_dock_widget = None
        self.terminal_dock_widget = None
        self.plugins_folder = os.path.join(os.path.dirname(__file__), 'plugins')
        # Create plugins folder if it doesn't exist
        os.makedirs(self.plugins_folder, exist_ok=True)
        TensorVisualizer.check_and_delete_trash()

    @staticmethod
    def check_and_delete_trash():
        internal_folder = os.path.join(os.getcwd(), "_internal")
        trash_folder = os.path.join(internal_folder, "trash")

        if os.path.exists(trash_folder) and os.path.isdir(trash_folder):
            try:
                shutil.rmtree(trash_folder)
            except Exception as e:
                pass

    def check_for_plugins(self):
        """Check if there are any valid plugin files in the plugins folder."""
        if not os.path.exists(self.plugins_folder):
            return False

        for filename in os.listdir(self.plugins_folder):
            if filename.endswith('.py') and not filename.startswith('_'):
                file_path = os.path.join(self.plugins_folder, filename)
                if self.is_valid_plugin_file(file_path):
                    return True

        return False

    def is_valid_plugin_file(self, file_path):
        """
        Check if a Python file is a valid plugin (executable script).

        Valid plugins have executable code at module level (not just function/class definitions).
        Pure library modules (only functions/classes) are excluded.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content)

            has_executable_code = False
            has_main_block = False
            function_definitions = 0
            class_definitions = 0

            for node in tree.body:
                if isinstance(node, (ast.Expr, ast.Assign, ast.AugAssign, ast.AnnAssign)):
                    has_executable_code = True
                elif isinstance(node, ast.If):
                    # Check for if __name__ == "__main__": pattern
                    if self._is_main_guard(node):
                        has_main_block = True
                    else:
                        has_executable_code = True
                elif isinstance(node, ast.FunctionDef):
                    function_definitions += 1
                elif isinstance(node, ast.ClassDef):
                    class_definitions += 1
                elif isinstance(node, (ast.For, ast.While, ast.With, ast.Try)):
                    has_executable_code = True

            # It's a plugin if it has executable code or a main block
            # Exclude pure library modules (only definitions, no executable code)
            if has_main_block or has_executable_code:
                return True

            # Pure library modules are not plugins
            if (function_definitions > 0 or class_definitions > 0) and not has_executable_code:
                return False

            return False

        except Exception:
            return False

    def _is_main_guard(self, node):
        """Check if an If node is the __name__ == "__main__" pattern."""
        if not isinstance(node.test, ast.Compare):
            return False

        if not (len(node.test.ops) == 1 and isinstance(node.test.ops[0], ast.Eq)):
            return False

        if not (isinstance(node.test.left, ast.Name) and node.test.left.id == '__name__'):
            return False

        if not (len(node.test.comparators) == 1 and
                isinstance(node.test.comparators[0], ast.Constant) and
                node.test.comparators[0].value == '__main__'):
            return False

        return True

    def populate_plugins_menu(self):
        """Scan plugins folder and populate the plugins menu."""
        # Clear existing menu items
        self.plugins_menu.clear()

        if not os.path.exists(self.plugins_folder):
            os.makedirs(self.plugins_folder, exist_ok=True)
            self._add_no_plugins_message()
            return

        # Find all valid plugin files
        plugins = []

        for filename in os.listdir(self.plugins_folder):
            if filename.endswith('.py') and not filename.startswith('_'):
                file_path = os.path.join(self.plugins_folder, filename)

                if self.is_valid_plugin_file(file_path):
                    plugin_name = os.path.splitext(filename)[0]
                    display_name = plugin_name.replace('_', ' ').title()
                    plugins.append({
                        'name': plugin_name,
                        'display_name': display_name,
                        'file_path': file_path
                    })

        # Sort alphabetically
        plugins.sort(key=lambda x: x['display_name'])

        # Add each plugin as a menu action
        if plugins:
            for plugin in plugins:
                action = QAction(plugin['display_name'], self)
                action.triggered.connect(
                    lambda checked, p=plugin: self.execute_plugin(p['file_path'], p['name'])
                )
                self.plugins_menu.addAction(action)
        else:
            self._add_no_plugins_message()

    def _add_no_plugins_message(self):
        """Add a disabled menu item showing no plugins are available."""
        action = QAction("No plugins found", self)
        action.setEnabled(False)
        self.plugins_menu.addAction(action)

        self.plugins_menu.addSeparator()

        # Add helpful action to open plugins folder
        open_folder_action = QAction("📁 Open Plugins Folder", self)
        open_folder_action.triggered.connect(self._open_plugins_folder)
        self.plugins_menu.addAction(open_folder_action)

    def _open_plugins_folder(self):
        """Open the plugins folder in the system file explorer."""
        import platform
        import subprocess

        if not os.path.exists(self.plugins_folder):
            os.makedirs(self.plugins_folder, exist_ok=True)

        system = platform.system()
        try:
            if system == 'Windows':
                os.startfile(self.plugins_folder)
            elif system == 'Darwin':  # macOS
                subprocess.run(['open', self.plugins_folder])
            else:  # Linux and others
                subprocess.run(['xdg-open', self.plugins_folder])
        except Exception as e:
            QMessageBox.warning(
                self,
                "Could Not Open Folder",
                f"Failed to open plugins folder:\n{self.plugins_folder}\n\nError: {e}"
            )

    def execute_plugin(self, file_path, plugin_name):
        """
        Load and execute a plugin file.

        The plugin runs with access to:
        - self: The main application instance (full access to app state)
        - __plugin_name__: The name of the current plugin
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()

            # Create execution namespace
            plugin_globals = {
                '__builtins__': __builtins__,
                '__name__': '__main__',
                '__file__': file_path,
                '__plugin_name__': plugin_name,
                'self': self,  # Full access to application
                'app': self,  # Alias for convenience
            }

            # Execute the plugin
            exec(code, plugin_globals)

        except Exception as e:
            self._show_plugin_error(plugin_name, e)

    def _show_plugin_error(self, plugin_name, error):
        """Display a user-friendly error message when a plugin fails."""
        import traceback

        error_detail = ''.join(traceback.format_exception(type(error), error, error.__traceback__))

        msg_box = QMessageBox(self)
        msg_box.setIcon(QMessageBox.Icon.Critical)
        msg_box.setWindowTitle("Plugin Error")
        msg_box.setText(
            f'Plugin "{plugin_name}" encountered an error during execution.\n\n'
            f'Error: {type(error).__name__}: {str(error)}'
        )
        msg_box.setDetailedText(error_detail)
        msg_box.exec()

    @staticmethod
    def setup_logging(process):
        # Assuming you already have a logs directory
        base_log_dir = os.path.join(os.getcwd(), "logs")

        # Create a subdirectory for console logs
        console_log_dir = os.path.join(base_log_dir, "console_logs")
        os.makedirs(console_log_dir, exist_ok=True)

        # Create a log file with the current timestamp
        log_filename = f"{process}_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
        log_file_path = os.path.join(console_log_dir, log_filename)

        # Redirect stdout and stderr to the log file
        sys.stdout = open(log_file_path, "w")
        sys.stderr = open(log_file_path, "w")

        print(f"Logging started. All {process.lower()} output will be written to this file.")

    def open_terminal(self):
        """ Asks user if they want to open CMD or PowerShell, sets environment, and opens the terminal. """

        # Check if the dock already exists
        if self.terminal_dock_widget is not None:
            self.terminal_dock_widget.raise_()
            return

        dialog = QDialog(self)
        dialog.setWindowTitle("Select Terminal")
        layout = QVBoxLayout()

        label = QLabel("Choose terminal type:")
        layout.addWidget(label)

        combo = QComboBox()
        combo.addItems(["CMD", "PowerShell"])
        layout.addWidget(combo)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()

        def handle_terminal_dock_close(event):
            self.terminal_dock_widget = None  # Reset the dock widget when it's closed
            event.accept()  # Ensure the close event is accepted

        if dialog.exec() == QDialog.DialogCode.Accepted:
            terminal_type = combo.currentText()

            self.terminal_dock_widget = QDockWidget()
            self.terminal_dock_widget.setFloating(False)
            self.terminal_dock_widget.setFeatures(QDockWidget.DockWidgetFeature.DockWidgetClosable |
                                                  QDockWidget.DockWidgetFeature.DockWidgetMovable |
                                                  QDockWidget.DockWidgetFeature.DockWidgetFloatable)
            self.terminal_dock_widget.setAllowedAreas(Qt.DockWidgetArea.AllDockWidgetAreas)

            # Add a short delay
            QTimer.singleShot(100, lambda: self.create_powershell_widget(terminal_type))

            self.addDockWidget(Qt.DockWidgetArea.RightDockWidgetArea, self.terminal_dock_widget)

            main_window_size = self.size()
            dock_width = int(main_window_size.width() // 3.75)
            self.resizeDocks([self.terminal_dock_widget], [dock_width], Qt.Orientation.Horizontal)
            self.terminal_dock_widget.closeEvent = handle_terminal_dock_close

    def create_powershell_widget(self, terminal_type):
        self.powershell_widget = PowerShellWindow(terminal_type)
        self.terminal_dock_widget.setWidget(self.powershell_widget)

    def resizeEvent(self, event):
        # Adjust the initial size of the splitter
        try:
            if self.terminal_dock_widget is not None:
                main_window_size = self.size()
                dock_width = int(main_window_size.width() // 3.75)  # Set to one-fourth of the main window width
                self.resizeDocks([self.terminal_dock_widget], [dock_width], Qt.Orientation.Horizontal)
                super().resizeEvent(event)
            else:
                super().resizeEvent(event)
        except Exception as e:
            super().resizeEvent(event)

    @staticmethod
    def get_location():
        try:
            # Use an IP-based geolocation API to get the user's latitude and longitude
            response = requests.get('http://ip-api.com/json', timeout=5)
            data = response.json()

            # Extract latitude and longitude from the JSON data
            lat = data['lat']
            lon = data['lon']

            return {float(lat)}, {float(lon)}
        except Exception as e:
            return None, None

    @staticmethod
    def fetch_sun_times(lat, lon):
        try:
            # Use the Sunrise-Sunset API to get sunrise and sunset times
            response = requests.get(f'https://api.sunrise-sunset.org/json?lat={lat}&lng={lon}&formatted=0', timeout=5)
            data = response.json()

            # Extract sunrise and sunset times from the JSON data
            sunrise_utc = data['results']['sunrise']
            sunset_utc = data['results']['sunset']

            # Convert the UTC times to local times
            sunrise_local = datetime.fromisoformat(sunrise_utc)
            sunset_local = datetime.fromisoformat(sunset_utc)

            return sunrise_local.time(), sunset_local.time()
        except Exception as e:
            print(f"Error: Unable to retrieve sunrise and sunset times: {e}")
            return None, None

    @staticmethod
    def get_sunrise_sunset_times():
        # Get user location
        lat, lon = TensorVisualizer.get_location()
        if lat is not None and lon is not None:
            # Fetch sunrise and sunset times
            sunrise, sunset = TensorVisualizer.fetch_sun_times(lat, lon)
            if sunrise is not None and sunset is not None:
                return QTime(sunrise.hour, sunrise.minute), QTime(sunset.hour, sunset.minute)

        # Fallback values if fetching fails
        return QTime(6, 0), QTime(20, 0)  # 6:00 AM and 8:00 PM fallback

    def init_custom_cursor(self):
        # Load your custom cursor image
        pixmap = QPixmap('busy.png')
        scaled_pixmap = pixmap.scaled(QSize(20, 20), Qt.AspectRatioMode.KeepAspectRatio,
                                      Qt.TransformationMode.SmoothTransformation)
        # Create the custom cursor
        self.custom_cursor = QCursor(scaled_pixmap, -1, -1)

    def create_widgets(self):
        central_widget = QWidget(self)
        self.setCentralWidget(central_widget)

        # Store a reference to central_widget
        self.central_widget = central_widget

        layout = QHBoxLayout(central_widget)
        layout.setContentsMargins(0, 10, 0, 0)  # Adjust margins as needed

        # Create a splitter to hold the left and right containers
        splitter = QSplitter(Qt.Orientation.Horizontal)
        layout.addWidget(splitter)

        # Create a container for the left section
        left_container = QWidget()
        left_layout = QVBoxLayout(left_container)

        self.statusBar().setFixedHeight(20)

        # Disable the size grip
        self.statusBar().setSizeGripEnabled(False)

        # Create a vertical progress bar for memory usage
        self.memory_progress_bar = QProgressBar(self)
        self.memory_progress_bar.setOrientation(Qt.Orientation.Horizontal)
        # Set a fixed size for the progress bar
        self.memory_progress_bar.setFixedSize(20, 10)  # Adjust width and height as needed
        self.memory_progress_bar.setRange(0, 100)  # Set the range (0-100%)
        self.memory_progress_bar.setValue(0)  # Initialize to 0%
        self.memory_progress_bar.setTextVisible(False)  # Hide percentage text
        self.memory_progress_bar.setSizePolicy(QSizePolicy.Policy.Fixed, QSizePolicy.Policy.Fixed)

        # Set a unique object name for styling
        self.memory_progress_bar.setObjectName("memoryProgressBar")
        self.apply_custom_stylesheet(self.memory_progress_bar, 0)

        # Create a RAM label
        self.ram_label = QLabel("RAM", self)
        self.ram_label.setStyleSheet("font-weight: bold;")
        self.ram_label.setSizePolicy(QSizePolicy.Policy.Fixed, QSizePolicy.Policy.Fixed)

        # Create stop button for the status bar (hidden by default)
        self.status_stop_btn = QPushButton(self)
        self.status_stop_btn.setText("Stop")
        self.status_stop_btn.setMaximumSize(32, 15)  # But not go too small
        self.status_stop_btn.setStyleSheet("font-weight: bold; font-size: 8px;")
        self.status_stop_btn.setSizePolicy(QSizePolicy.Policy.Fixed, QSizePolicy.Policy.Fixed)
        self.status_stop_btn.clicked.connect(self.on_stop_task)
        self.status_stop_btn.hide()  # Hidden initially

        # Create the busy GIF label
        self.busy_label = QLabel(self)
        self.busy_movie = QMovie("busy.gif")  # Replace with actual GIF path
        self.busy_movie.setScaledSize(QSize(15, 15))  # Scale to 24x24
        self.busy_label.setMovie(self.busy_movie)
        self.busy_label.setMaximumSize(15, 15)  # Ensure it stays small
        self.busy_label.setSizePolicy(QSizePolicy.Policy.Fixed, QSizePolicy.Policy.Fixed)
        self.busy_movie.start()  # Start animation
        self.busy_label.hide()  # Hide initially

        self.status_bar_widget = QWidget()
        bar_layout = QHBoxLayout()
        bar_layout.setContentsMargins(0, 0, 10, 0)  # Use the same margin as the label
        bar_layout.addWidget(self.busy_label)  # Insert after progress bar
        bar_layout.addWidget(self.status_stop_btn)
        bar_layout.addWidget(self.ram_label)
        bar_layout.addWidget(self.memory_progress_bar)
        self.status_bar_widget.setLayout(bar_layout)
        self.statusBar().addPermanentWidget(self.status_bar_widget)

        # Update memory usage and responsiveness periodically
        self.update_memory_usage()

        self.menubar = self.menuBar()
        self.file_menu = QMenu("File", self)
        self.file_menu.aboutToShow.connect(self.update_recent_files_submenu)

        # Create a submenu for "Recent Files"
        self.recent_files_menu = QMenu("Recent Files", self)
        self.update_recent_files_submenu()  # Populate the submenu initially

        # Add '3D Canvas Appearance' menu with submenus
        open_menu = QMenu("Open", self)
        self.file_menu.addMenu(open_menu)

        # Add 'Grid' submenu with state-dependent text
        self.open_action = QAction("Numpy", self)
        self.open_action.triggered.connect(self.load_tensor)
        open_menu.addAction(self.open_action)

        # Add 'Grid' submenu with state-dependent text
        self.opens_action = QAction("SEGY", self)
        self.opens_action.triggered.connect(self.process_seismic_data)
        open_menu.addAction(self.opens_action)

        # Add 'Grid' submenu with state-dependent text
        self.load_las_file_action = QAction("Well Logs", self)
        self.load_las_file_action.triggered.connect(self.load_las_file)
        open_menu.addAction(self.load_las_file_action)

        # Add 'Grid' submenu with state-dependent text
        self.load_text_file_action = QAction("Check Shots", self)
        self.load_text_file_action.triggered.connect(self.load_text_file)
        open_menu.addAction(self.load_text_file_action)

        # Add 'Grid' submenu with state-dependent text
        self.load_top_file_action = QAction("Well Tops", self)
        self.load_top_file_action.triggered.connect(self.load_top_file)
        open_menu.addAction(self.load_top_file_action)

        # Add 'Grid' submenu with state-dependent text
        self.load_well_head_action = QAction("Well Heads", self)
        self.load_well_head_action.triggered.connect(self.load_well_head)
        open_menu.addAction(self.load_well_head_action)

        exit_action = QAction("Exit", self)
        exit_action.triggered.connect(self.close)  # Connect to your file handling function

        self.file_menu.addSeparator()  # Add another separator
        self.file_menu.addAction(exit_action)
        # Add the "File" menu to the menu bar
        self.menubar.addMenu(self.file_menu)

        # Create the 'View' menu
        view_menu = self.menuBar().addMenu("View")

        # Add 'Choose Dark Mode Times' action
        choose_times_action = QAction("Choose Dark Mode Schedule", self)
        choose_times_action.triggered.connect(self.choose_dark_mode_times)
        view_menu.addAction(choose_times_action)

        # Add '3D Canvas Appearance' menu with submenus
        app_appearance_menu = QMenu("Application Themes", self)
        view_menu.addMenu(app_appearance_menu)

        # Add 'Grid' submenu with state-dependent text
        self.default_theme_action = QAction("Default", self)
        self.default_theme_action.triggered.connect(self.set_default_shades)
        app_appearance_menu.addAction(self.default_theme_action)

        # Add 'Grid' submenu with state-dependent text
        self.update_colors_based_on_selection_action = QAction("Theme Color", self)
        self.update_colors_based_on_selection_action.triggered.connect(self.update_colors_based_on_selection)
        app_appearance_menu.addAction(self.update_colors_based_on_selection_action)

        # Add 'Grid' submenu with state-dependent text
        self.geophysical_objects_color_action = QAction("Geobody Color", self)
        self.geophysical_objects_color_action.triggered.connect(self.geophysical_objects_color)
        app_appearance_menu.addAction(self.geophysical_objects_color_action)

        # Add 'Grid' submenu with state-dependent text
        self.graph_brush_color_action = QAction("Graph Color", self)
        self.graph_brush_color_action.triggered.connect(self.graph_brush_color)
        app_appearance_menu.addAction(self.graph_brush_color_action)

        # Add 'Grid' submenu with state-dependent text
        self.geo_object_cmap_action = QAction("Geobody Color Mapping", self)
        self.geo_object_cmap_action.triggered.connect(self.geo_object_cmap)
        app_appearance_menu.addAction(self.geo_object_cmap_action)

        # Add 'Axes' submenu with state-dependent text
        self.canvas_color_action = QAction("Canvas Background Color", self)
        self.canvas_color_action.triggered.connect(self.canvas_color)
        view_menu.addAction(self.canvas_color_action)

        # Add 'Toggle Canvas' action with dynamic text
        self.toggle_canvas_action = QAction("3D View", self)
        view_menu.addAction(self.toggle_canvas_action)

        # Add '3D Canvas Appearance' menu with submenus
        canvas_appearance_menu = QMenu("3D Canvas Settings", self)
        view_menu.addMenu(canvas_appearance_menu)

        # Add 'Grid' submenu with state-dependent text
        self.three_d_interpolation_action = QAction("3D Interpolation", self)
        self.three_d_interpolation_action.triggered.connect(self.three_d_interpolation_setting)
        canvas_appearance_menu.addAction(self.three_d_interpolation_action)

        # Add 'Grid' submenu with state-dependent text
        self.grid_action = QAction("Deactivate Axis" if self.grid_active else "Activate Axis", self)
        self.grid_action.triggered.connect(self.toggle_grid)
        canvas_appearance_menu.addAction(self.grid_action)

        # Add 'Grid' submenu with state-dependent text
        self.toggle_three_d_color_bar_action = QAction("Deactivate Color Bar" if self.three_d_color_bar_active else "Activate Color Bar", self)
        self.toggle_three_d_color_bar_action.triggered.connect(self.toggle_three_d_color_bar)
        canvas_appearance_menu.addAction(self.toggle_three_d_color_bar_action)

        # Add 'Compass' submenu with state-dependent text
        self.compass_action = QAction("Deactivate Compass" if self.compass_active else "Activate Compass", self)
        self.compass_action.triggered.connect(self.toggle_compass)
        canvas_appearance_menu.addAction(self.compass_action)

        # Add 'Axes' submenu with state-dependent text
        self.grid_color_action = QAction("Choose Axis Color", self)
        self.grid_color_action.triggered.connect(self.apply_grid_color)
        canvas_appearance_menu.addAction(self.grid_color_action)

        # Add 'Shade Threshold' Options
        self.shade_threshold_action = QAction("Choose Shade Threshold ", self)
        self.shade_threshold_action.triggered.connect(self.set_shade_threshold)
        canvas_appearance_menu.addAction(self.shade_threshold_action)

        # Create 'Probe Options' submenu under 'Canvas Appearance'
        probe_options_menu = canvas_appearance_menu.addMenu("Probe Options")

        # Add 'Cell Mesh' action to 'Probe Options' submenu
        self.cell_mesh_action = QAction(
            "Deactivate Cell Mesh" if self.cell_mesh else "Activate Cell Mesh", self)
        self.cell_mesh_action.triggered.connect(self.set_cell_mesh)
        probe_options_menu.addAction(self.cell_mesh_action)

        # Add 'Point Mesh' action to 'Probe Options' submenu
        self.point_mesh_action = QAction(
            "Deactivate Point Mesh" if self.point_mesh else "Activate Point Mesh", self)
        self.point_mesh_action.triggered.connect(self.set_point_mesh)
        probe_options_menu.addAction(self.point_mesh_action)

        # Add '3D Canvas Appearance' menu with submenus
        two_d_canvas_settings_menu = QMenu("2D Canvas Settings", self)
        view_menu.addMenu(two_d_canvas_settings_menu)

        # Add 'Grid' submenu with state-dependent text
        self.two_d_interpolation_action = QAction("2D Interpolation", self)
        self.two_d_interpolation_action.triggered.connect(self.two_d_interpolation_setting)
        two_d_canvas_settings_menu.addAction(self.two_d_interpolation_action)

        self.processing_options_settings_action = QAction("Multiprocessing Settings", self)
        self.processing_options_settings_action.triggered.connect(self.processing_options)
        view_menu.addAction(self.processing_options_settings_action)

        # Add 'Axes' submenu with state-dependent text
        self.horizon_extraction_settings_action = QAction("Horizon Extraction Settings", self)
        self.horizon_extraction_settings_action.triggered.connect(self.horizon_extraction_settings)
        view_menu.addAction(self.horizon_extraction_settings_action)

        # Add 'Choose Background Color' action
        self.toggle_layout_action = QAction("Hide Control Panel", self)
        self.toggle_layout_action.triggered.connect(self.toggle_left_layout)
        view_menu.addAction(self.toggle_layout_action)

        # Create the 'View' menu
        tools_menu = self.menuBar().addMenu("Tools")

        # Create the terminal button
        self.terminal_button = QToolButton(self)
        self.terminal_button.setAutoRaise(True)
        self.terminal_button.setToolTip("Terminal")
        self.terminal_button.clicked.connect(self.open_terminal)
        main_terminal_container = QWidget(self)
        main_terminal_layout = QHBoxLayout(main_terminal_container)
        main_terminal_layout.setContentsMargins(2, 2, 2, 2)
        main_terminal_layout.addWidget(self.terminal_button)
        main_terminal_container.setLayout(main_terminal_layout)
        self.menubar.setCornerWidget(main_terminal_container, Qt.Corner.TopRightCorner)

        # Add '3D Canvas Appearance' menu with submenus
        denoise_menu = QMenu("Denoising", self)
        tools_menu.addMenu(denoise_menu)

        # Add 'Axes' submenu with state-dependent text
        self.denoise_unet_prediction_action = QAction("Neural Network", self)
        self.denoise_unet_prediction_action.triggered.connect(self.denoise_unet_prediction)
        denoise_menu.addAction(self.denoise_unet_prediction_action)

        # Add 'Axes' submenu with state-dependent text
        self.non_local_means_denoising_3d_action = QAction("Non-local Means", self)
        self.non_local_means_denoising_3d_action.triggered.connect(self.non_local_means_denoising_3d)
        denoise_menu.addAction(self.non_local_means_denoising_3d_action)

        # Add 'Axes' submenu with state-dependent text
        self.bm3d_denoising_3d_action = QAction("BM3D", self)
        self.bm3d_denoising_3d_action.triggered.connect(self.bm3d_denoising_3d)
        denoise_menu.addAction(self.bm3d_denoising_3d_action)

        # Add '3D Canvas Appearance' menu with submenus
        seismic_attribute_menu = QMenu("Seismic Attributes", self)
        tools_menu.addMenu(seismic_attribute_menu)

        # Add '3D Canvas Appearance' menu with submenus
        morphological_operations_menu = QMenu("Morphological Operations", self)
        tools_menu.addMenu(morphological_operations_menu)

        # Add 'Axes' submenu with state-dependent text
        self.connected_component_analysis = QAction("Connected Component Analysis", self)
        self.connected_component_analysis.triggered.connect(self.connected_component_analysis_filtering)
        morphological_operations_menu.addAction(self.connected_component_analysis)

        self.binery_closing_analysis = QAction("Binary Closing", self)
        self.binery_closing_analysis.triggered.connect(self.binary_closing)
        morphological_operations_menu.addAction(self.binery_closing_analysis)

        # Add 'Axes' submenu with state-dependent text
        self.median_filter_3d_action = QAction("Median", self)
        self.median_filter_3d_action.triggered.connect(self.median_filter)
        morphological_operations_menu.addAction(self.median_filter_3d_action)

        self.threshold_4d_data_action = QAction("Binary Thresholding", self)
        self.threshold_4d_data_action.triggered.connect(self.threshold_4d_data)
        morphological_operations_menu.addAction(self.threshold_4d_data_action)

        # Add 'Axes' submenu with state-dependent text
        self.fault_prediction_action = QAction("Geobody Segmentation", self)
        self.fault_prediction_action.triggered.connect(self.fault_prediction)
        tools_menu.addAction(self.fault_prediction_action)

        # Add 'Axes' submenu with state-dependent text
        self.thresholded_amplitude_action = QAction("Anomaly Detection", self)
        self.thresholded_amplitude_action.triggered.connect(self.thresholded_amplitude)
        tools_menu.addAction(self.thresholded_amplitude_action)

        # Add 'Axes' submenu with state-dependent text
        self.kmeans_amplitude_action = QAction("Seismic KMeans Clustering", self)
        self.kmeans_amplitude_action.triggered.connect(self.kmeans_amplitude)
        tools_menu.addAction(self.kmeans_amplitude_action)

        # Add 'Axes' submenu with state-dependent text
        self.facies_prediction_action = QAction("Zone Segmentation", self)
        self.facies_prediction_action.triggered.connect(self.facies_prediction)
        tools_menu.addAction(self.facies_prediction_action)

        # Add 'Axes' submenu with state-dependent text
        self.tensor_cutter_action = QAction("Seismic Image Slicer", self)
        self.tensor_cutter_action.triggered.connect(self.tensor_cutter)
        tools_menu.addAction(self.tensor_cutter_action)

        # Add 'Axes' submenu with state-dependent text
        self.rotate_tensor_ui_action = QAction("Seismic Image Rotator", self)
        self.rotate_tensor_ui_action.triggered.connect(self.rotate_tensor_ui)
        tools_menu.addAction(self.rotate_tensor_ui_action)

        # Add 'Axes' submenu with state-dependent text
        self.open_plot_dialog_action = QAction("Well Analysis", self)
        self.open_plot_dialog_action.triggered.connect(self.open_plot_dialog)
        tools_menu.addAction(self.open_plot_dialog_action)

        # Add 'Axes' submenu with state-dependent text
        self.prepare_for_training_action = QAction("Petrophysical Characterization", self)
        self.prepare_for_training_action.triggered.connect(self.open_petro_dialog)
        tools_menu.addAction(self.prepare_for_training_action)

        # Add 'Axes' submenu with state-dependent text
        self.show_calculator_action = QAction("Well Calculator", self)
        self.show_calculator_action.triggered.connect(self.show_calculator)
        tools_menu.addAction(self.show_calculator_action)

        # Add 'Axes' submenu with state-dependent text
        self.show_seismic_calculator_action = QAction("Seismic Calculator", self)
        self.show_seismic_calculator_action.triggered.connect(self.show_seismic_calculator)
        tools_menu.addAction(self.show_seismic_calculator_action)

        # Add 'plugins' menu with submenus
        self.plugins_menu = QMenu("Plugins", self)
        # Connect menu's aboutToShow signal to refresh plugins dynamically
        self.plugins_menu.aboutToShow.connect(self.populate_plugins_menu)
        tools_menu.addMenu(self.plugins_menu)

        # Add 'Axes' submenu with state-dependent text
        self.Instantaneous_Phase_action = QAction("Instantaneous Phase", self)
        self.Instantaneous_Phase_action.triggered.connect(self.instantaneous_phase)
        seismic_attribute_menu.addAction(self.Instantaneous_Phase_action)

        # Add 'Axes' submenu with state-dependent text
        self.Instantaneous_frequency_action = QAction("Instantaneous Frequency", self)
        self.Instantaneous_frequency_action.triggered.connect(self.instantaneous_frequency)
        seismic_attribute_menu.addAction(self.Instantaneous_frequency_action)

        # Add 'Axes' submenu with state-dependent text
        self.inst_bandwidth_action = QAction("Instantaneous Bandwidth", self)
        self.inst_bandwidth_action.triggered.connect(self.inst_bandwidth)
        seismic_attribute_menu.addAction(self.inst_bandwidth_action)

        # Add 'Axes' submenu with state-dependent text
        self.dominant_frequency_action = QAction("Dominant Frequency", self)
        self.dominant_frequency_action.triggered.connect(self.dominant_frequency)
        seismic_attribute_menu.addAction(self.dominant_frequency_action)

        # Add 'Axes' submenu with state-dependent text
        self.cosine_of_phase_action = QAction("Cosine of Phase", self)
        self.cosine_of_phase_action.triggered.connect(self.cosine_of_phase)
        seismic_attribute_menu.addAction(self.cosine_of_phase_action)

        # Add 'Axes' submenu with state-dependent text
        self.envelope_action = QAction("Envelope", self)
        self.envelope_action.triggered.connect(self.envelope)
        seismic_attribute_menu.addAction(self.envelope_action)

        # Add 'Axes' submenu with state-dependent text
        self.sweetness_action = QAction("Sweetness", self)
        self.sweetness_action.triggered.connect(self.sweetness)
        seismic_attribute_menu.addAction(self.sweetness_action)

        # Add 'Axes' submenu with state-dependent text
        self.apparent_polarity_action = QAction("Apparent Polarity", self)
        self.apparent_polarity_action.triggered.connect(self.apparent_polarity)
        seismic_attribute_menu.addAction(self.apparent_polarity_action)

        # Add 'Axes' submenu with state-dependent text
        self.rms_amplitude_action = QAction("Rms Amplitude", self)
        self.rms_amplitude_action.triggered.connect(self.rms_amplitude)
        seismic_attribute_menu.addAction(self.rms_amplitude_action)

        # Add 'Axes' submenu with state-dependent text
        self.gst_gpu_action = QAction("Gradient Structure Tensor GPU", self)
        self.gst_gpu_action.triggered.connect(self.gst_gpu)
        seismic_attribute_menu.addAction(self.gst_gpu_action)

        # Add 'Axes' submenu with state-dependent text
        self.spectral_decomposition_action = QAction("Spectral Decomposition", self)
        self.spectral_decomposition_action.triggered.connect(self.spectral_decomposition)
        seismic_attribute_menu.addAction(self.spectral_decomposition_action)

        # Add 'Axes' submenu with state-dependent text
        self.calculate_amplitude_spectrum_action = QAction("Amplitude Spectrum", self)
        self.calculate_amplitude_spectrum_action.triggered.connect(self.calculate_amplitude_spectrum)
        seismic_attribute_menu.addAction(self.calculate_amplitude_spectrum_action)

        # Create the 'Help' menu
        help_menu = self.menuBar().addMenu("Help")

        # Add 'Choose Dark Mode Times' action
        about_action = QAction("About", self)
        about_action.triggered.connect(self.about)
        help_menu.addAction(about_action)

        self.treeWidget = QTreeWidget()
        self.treeWidget.setHeaderLabel('Data')
        self.treeWidget.setMaximumHeight(180)  # Adjust the height as needed
        left_layout.addWidget(self.treeWidget)  # Add the tree widget to the layout
        self.populateList()
        self.treeWidget.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
        self.treeWidget.customContextMenuRequested.connect(self.openMenu)

        left_layout.addSpacing(10)

        self.dim1_entry = QLineEdit()
        self.dim2_entry = QLineEdit()
        self.index_dim_entry = QLineEdit()

        # Add buttons for selecting plotting direction
        self.time_slice_button = QPushButton("Time", checkable=True, clicked=self.set_time_slice)
        self.cross_line_button = QPushButton("CrossLine", checkable=True, clicked=self.set_cross_line)
        self.inline_button = QPushButton("InLine", checkable=True, clicked=self.set_inline)
        self.Three_D_button = QPushButton("3D", checkable=True, clicked=self.three_d)

        # Create button group for exclusive behavior
        self.button_group = QButtonGroup()
        self.button_group.addButton(self.time_slice_button)
        self.button_group.addButton(self.cross_line_button)
        self.button_group.addButton(self.inline_button)
        self.button_group.addButton(self.Three_D_button)

        self.Three_D_button.hide()
        self.time_slice_button.hide()
        self.cross_line_button.hide()
        self.inline_button.hide()

        left_layout.addWidget(self.time_slice_button)
        left_layout.addWidget(self.inline_button)
        left_layout.addWidget(self.cross_line_button)
        left_layout.addWidget(self.Three_D_button)

        self.index_label = QLabel("Interval:")
        self.index_entry = StepSpinBox()
        left_layout.addWidget(self.index_label)
        left_layout.addWidget(self.index_entry)
        # Create a slider for the interval index
        self.index_slider = StepSlider(Qt.Orientation.Horizontal)
        self.index_slider.setTickPosition(QSlider.TickPosition.NoTicks)

        # Sync slider and spinbox
        self.index_slider.valueChanged.connect(self.index_entry.setValue)
        self.index_entry.valueChanged.connect(self.index_slider.setValue)

        self.index_label.hide()
        self.index_entry.hide()
        self.index_slider.hide()

        # Add the slider to the UI layout
        left_layout.addWidget(self.index_slider)

        # Initially hide the Channel Index entry box
        self.channel_index_label = QLabel("Channel Index:")
        self.channel_index_entry = QSpinBox()
        self.channel_index_label.hide()
        self.channel_index_entry.hide()
        left_layout.addWidget(self.channel_index_label)
        left_layout.addWidget(self.channel_index_entry)

        # Create a slider for the interval index
        self.Channel_slider = QSlider(Qt.Orientation.Horizontal)
        self.Channel_slider.setTickPosition(QSlider.TickPosition.NoTicks)

        # Sync slider and spinbox
        self.Channel_slider.valueChanged.connect(self.channel_index_entry.setValue)
        self.channel_index_entry.valueChanged.connect(self.Channel_slider.setValue)

        self.Channel_slider.hide()

        # Add the slider to the UI layout
        left_layout.addWidget(self.Channel_slider)

        self.plot_button = QPushButton("Plot", clicked=self.plot_tensor)

        self.plot_button.hide()

        left_layout.addWidget(self.plot_button)

        left_layout.addStretch()
        splitter.addWidget(left_container)

        # Create a container for the plot on the right
        plot_container = QWidget()
        plot_layout = QVBoxLayout(plot_container)

        # Create a QTabWidget to hold different visualizations
        self.tab_widget = QTabWidget()

        # Create a font with a smaller size for the tab labels
        tab_font = QFont()
        tab_font.setPointSize(8)  # Adjust the font size as needed

        # Apply the smaller font to the tab labels
        self.tab_widget.setStyleSheet("QTabBar::tab { font-size: 8pt; }")

        # Create a tab for the rounded canvas
        self.plot_tab = QWidget()
        self.plot_layout = QVBoxLayout(self.plot_tab)
        self.rounded_canvas = RoundedCanvas(self)
        self.plot_layout.addWidget(self.rounded_canvas)
        self.tab_widget.addTab(self.plot_tab, f"2D Plot 1 ")

        # Set the closeable tabs
        self.tab_widget.setTabsClosable(True)
        self.tab_widget.tabCloseRequested.connect(self.close_tab)

        # Create an "Add Tab" button
        add_tab_button = QToolButton(self)
        add_tab_button.setText("+")  # The "+" symbol to indicate adding a new tab
        add_tab_button.setAutoRaise(True)  # Flat button style

        # Connect the "Add Tab" button to the add_new_tab function
        add_tab_button.clicked.connect(lambda: self.add_tab())

        # Create a larger container to hold both individual containers
        container_widget = QWidget()
        layout = QHBoxLayout(container_widget)
        layout.addWidget(add_tab_button)
        layout.setContentsMargins(0, 0, 0, 4)  # Adjust the margins as needed
        container_widget.setLayout(layout)

        # Add the button to the top-right corner of the tab widget
        self.tab_widget.setCornerWidget(container_widget, Qt.Corner.TopRightCorner)

        # Add the tab widget to the layout
        plot_layout.addWidget(self.tab_widget)

        plot_layout.setContentsMargins(10, 10, 10, 0)

        # Remove the close button from the first tab
        self.tab_widget.tabBar().setTabButton(0, QTabBar.ButtonPosition.RightSide, None)

        # Connect the signal for tab changes
        self.tab_widget.currentChanged.connect(self.on_tab_change)

        splitter.addWidget(plot_container)

        splitter.setStretchFactor(0, 2)
        splitter.setStretchFactor(1, 5)
        self.splitter = splitter  # Store a reference to the splitter

        self.rounded_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
        self.rounded_canvas.customContextMenuRequested.connect(self.show_context_menu2)
        self.toggle_canvas_action.triggered.connect(self.toggle_canvas_from_menu)

    def on_task_started(self):
        """Update UI when a task starts."""
        self.status_stop_btn.show()
        self.busy_label.show()

    def on_task_ended(self):
        """Update UI when a task ends."""
        self.status_stop_btn.hide()
        self.busy_label.hide()

    def on_stop_task(self):
        """Handle stop button click."""
        if self.task_runner.stop_task():
            self.status_stop_btn.hide()
            self.busy_label.hide()
            QApplication.restoreOverrideCursor()

    def geo_object_cmap(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("Geobody Color Mapping")
        layout = QVBoxLayout()

        label = QLabel("Select Color Map:")
        layout.addWidget(label)

        combo = QComboBox()
        combo.addItems(['jet', 'jet_r', 'turbo', 'turbo_r', 'hsv', 'hsv_r', 'gist_rainbow', 'gist_rainbow_r','gist_ncar_r',
             'tab20', 'tab20_r'])
        layout.addWidget(combo)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()
        if dialog.exec() == QDialog.DialogCode.Accepted:
            self.geophysical_Object_CMap = combo.currentText()

    def graph_brush_color(self):
        # Enable the QColorDialog to allow alpha (opacity) selection
        color = QColorDialog.getColor(QColor(0, 0, 0, 255), self,
                                      options=QColorDialog.ColorDialogOption.ShowAlphaChannel)
        if color.isValid():
            # Convert the color to a tuple with RGBA values scaled to 255
            rgba_color = (color.red(), color.green(), color.blue(), color.alpha())
            self.graph_brush = rgba_color  # Store the RGBA color

    def geophysical_objects_color(self):
        color = QColorDialog.getColor(parent=self)
        if color.isValid():
            # Convert the color to a tuple with RGBA values
            rgba_color = (color.redF(), color.greenF(), color.blueF(), 1.0)  # Fully opaque alpha channel
            self.geo_objects_color = rgba_color

    def horizon_extraction_settings(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("Horizon Extraction Settings")

        layout = QVBoxLayout(dialog)

        # Intensity Tolerance
        intensity_layout = QHBoxLayout()
        intensity_label = QLabel("Intensity Tolerance (0-1):")
        self.intensity_slider = QSlider(Qt.Orientation.Horizontal)
        self.intensity_slider.setRange(0, 100)  # 0-1 scaled to 0-100
        self.intensity_slider.setValue(int(self.intensity_tolerance * 100))
        self.intensity_box = QLineEdit(f"{self.intensity_tolerance:.2f}")
        intensity_layout.addWidget(intensity_label)
        intensity_layout.addWidget(self.intensity_slider)
        intensity_layout.addWidget(self.intensity_box)
        layout.addLayout(intensity_layout)

        def update_intensity_box(value):
            self.intensity_box.setText(f"{value / 100:.2f}")

        def update_intensity_slider(text):
            try:
                value = float(text)
                self.intensity_slider.setValue(int(value * 100))
            except ValueError:
                pass

        self.intensity_slider.valueChanged.connect(update_intensity_box)
        self.intensity_box.textChanged.connect(update_intensity_slider)

        # Max Vertical Jump
        vertical_layout = QHBoxLayout()
        vertical_label = QLabel("Max Vertical Jump:")
        self.vertical_slider = QSlider(Qt.Orientation.Horizontal)
        self.vertical_slider.setRange(0, 1000)  # Arbitrary upper limit
        self.vertical_slider.setValue(self.max_vertical_jump)
        self.vertical_box = QLineEdit(str(self.max_vertical_jump))
        vertical_layout.addWidget(vertical_label)
        vertical_layout.addWidget(self.vertical_slider)
        vertical_layout.addWidget(self.vertical_box)
        layout.addLayout(vertical_layout)

        def update_vertical_box(value):
            self.vertical_box.setText(str(value))

        def update_vertical_slider(text):
            try:
                value = int(text)
                self.vertical_slider.setValue(value)
            except ValueError:
                pass

        self.vertical_slider.valueChanged.connect(update_vertical_box)
        self.vertical_box.textChanged.connect(update_vertical_slider)

        # Buttons
        button_layout = QHBoxLayout()
        ok_button = QPushButton("OK")
        cancel_button = QPushButton("Cancel")
        button_layout.addWidget(ok_button)
        button_layout.addWidget(cancel_button)
        layout.addLayout(button_layout)

        def accept():
            self.intensity_tolerance = float(self.intensity_box.text())
            self.max_vertical_jump = int(self.vertical_box.text())
            dialog.accept()

        def reject():
            dialog.reject()

        ok_button.clicked.connect(accept)
        cancel_button.clicked.connect(reject)

        dialog.exec()

    def two_d_interpolation_setting(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("2D Interpolation")
        layout = QVBoxLayout()

        label = QLabel("Choose Interpolation Factor")
        layout.addWidget(label)

        # Add a spin box for integer input
        spinBox = QSpinBox()
        spinBox.setMinimum(1)
        spinBox.setMaximum(20)
        spinBox.setValue(self.two_d_interpolation)
        layout.addWidget(spinBox)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()
        if dialog.exec() == QDialog.DialogCode.Accepted:
            self.two_d_interpolation = spinBox.value()

    def three_d_interpolation_setting(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("3D Interpolation")
        layout = QVBoxLayout()

        label = QLabel("Choose Interpolation Factor")
        layout.addWidget(label)

        # Add a spin box for integer input
        spinBox = QSpinBox()
        spinBox.setMinimum(1)
        spinBox.setMaximum(20)
        spinBox.setValue(self.three_d_interpolation)
        layout.addWidget(spinBox)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()
        if dialog.exec() == QDialog.DialogCode.Accepted:
            self.three_d_interpolation = spinBox.value()

    def toggle_left_layout(self):
        if self.splitter.sizes()[0] == 0:
            # Restore the splitter state to the previously saved state
            self.splitter.restoreState(self.splitter_state)
            self.toggle_layout_action.setText("Hide Control Panel")
        else:
            # Save the current splitter state before hiding the left layout
            self.splitter_state = self.splitter.saveState()
            # Collapse the left layout by setting its width to 0
            self.splitter.setSizes([0, self.splitter.sizes()[1]])
            self.toggle_layout_action.setText("Show Control Panel")

    def set_channel_slider_range(self, max_value):
        self.Channel_slider.setMinimum(0)
        self.Channel_slider.setMaximum(max_value - 1)  # Adjusted to match zero-based indexing
        # Set the maximum value of the spinbox
        self.channel_index_entry.setMaximum(max_value - 1)
        self.channel_index_entry.setMinimum(0)
        self.channel_index_entry.setValue(0)

    def about(self):
        dialog = QDialog(self)
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.setWindowTitle('About Seismic Flow')
        dialog.setWindowIcon(QIcon('icon.png'))  # Set the icon for the dialog

        # The Main layout is horizontal, with two sections
        mainLayout = QHBoxLayout()

        # Left layout for the image
        leftLayout = QVBoxLayout()
        imageLabel = QLabel(self)
        pixmap = QPixmap('icon.png')  # Load the image
        imageLabel.setPixmap(
            pixmap.scaled(100, 100, Qt.AspectRatioMode.KeepAspectRatio, Qt.TransformationMode.SmoothTransformation))
        leftLayout.addWidget(imageLabel)
        leftLayout.setAlignment(Qt.AlignmentFlag.AlignTop)
        leftLayout.setContentsMargins(0, 0, 5, 5)  # Adjust margins as needed
        # Right layout for the text
        rightLayout = QVBoxLayout()

        # Title label with larger font and bold
        titleLabel = QLabel('Seismic Flow')
        titleFont = QFont('Arial', 18, QFont.Weight.Bold)
        titleLabel.setFont(titleFont)
        titleLabel.setAlignment(Qt.AlignmentFlag.AlignCenter)
        rightLayout.addWidget(titleLabel)

        # Version and developer info with a smaller font
        infoFont = QFont('Arial', 10)
        current_version = "1.0"
        versionLabel = QLabel(f'Version {current_version}')
        versionLabel.setFont(infoFont)
        rightLayout.addWidget(versionLabel)

        developerLabel = QLabel('Developed by Dr. Matin Mahzad')
        developerLabel.setFont(infoFont)
        rightLayout.addWidget(developerLabel)

        def openEmailApp(event):
            # Open the default email client
            QDesktopServices.openUrl(QUrl("mailto:matinmahzad@yahoo.com"))

        emailLabel = QLabel('Contact: MatinMahzad@yahoo.com')
        emailLabel.setFont(infoFont)
        rightLayout.addWidget(emailLabel)

        # Directly set the cursor to a pointing hand cursor
        emailLabel.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))
        # Connect the function to trigger the email app when clicked

        emailLabel.mousePressEvent = openEmailApp

        # Add copyright notice with italic font
        copyrightLabel = QLabel('© 2024 Matin Mahzad. All rights reserved.')
        copyrightFont = QFont('Arial', 8, QFont.Weight.Normal, italic=True)
        copyrightLabel.setFont(copyrightFont)
        rightLayout.addWidget(copyrightLabel)

        # Add layouts to the main layout
        mainLayout.addLayout(leftLayout)
        mainLayout.addLayout(rightLayout)

        # Set dialog layout
        dialog.setLayout(mainLayout)

        # Show the dialog
        dialog.exec()

    def openMenu(self, position):
        item = self.treeWidget.itemAt(position)

        if item is not None:
            # Create the menu
            menu = QMenu(self)

            # Add actions
            if isinstance(self.tensor_data, np.ndarray):
                menu.addAction('Information', self.analysis)
                menu.addAction('Map', self.show_area_on_map_osm_seismic)
                menu.addAction('Save as Numpy', self.save_tensor)
                menu.addAction('Save as SEGY', self.numpy_to_segy)
                menu.addAction('Upscale Well Logs', self.add_wells_to_seismic)
                menu.addAction('Annotate', self.tensor_Annotator)
                menu.addAction('Choose Template', self.select_template)
            else:
                menu.addAction('Map', self.show_area_on_map_osm_well)
                menu.addAction('Show Table', self.show_csv_in_dialog)
                menu.addAction('Save as LAS', self.save_to_las)
                menu.addAction('Save as CSV', self.save_tensor)
                menu.addAction('Set Deviation', self.parse_deviation_file)

                # Check for information in the well_head_dict and deviation
                # Determine if the item is a parent or child and return the appropriate key
                parent_item = item.parent()
                if parent_item is None:
                    # This is a parent item
                    tensor_key = item.text(0).strip()
                else:

                    # Get the parent key and the template value to match
                    parent_key = parent_item.text(0).strip()
                    target_template = item.text(0).strip()

                    # Get the value of 'name' for the parent item
                    parent_name_or_source = self.well_header_info_library[parent_key].get('Well Name')

                    # Search for a key that meets both conditions
                    matching_key = None
                    for key, value in self.well_header_info_library.items():
                        # Check if 'name' or 'source' matches the parent's 'name'
                        name_or_source = value.get('Well Name')
                        if name_or_source == parent_name_or_source and value.get('Template') == target_template:
                            matching_key = key
                            break  # Stop once we find the first matching key

                    tensor_key = matching_key

                log_name = self.well_header_info_library[tensor_key].get('Well Name').strip()

                # Check if well-head or deviation data exists for this key
                if (tensor_key is not None and (
                        tensor_key in self.well_deviation_dict or tensor_key in self.well_header_info_library)) or \
                        (log_name is not None and log_name in self.well_head_dict):
                    menu.addAction('Well Head Info', lambda: self.show_well_info_tooltip(tensor_key))

                if tensor_key is not None and tensor_key in self.tensor_dict:
                    if self.tensor_dict[tensor_key] is not None:
                        if 'Surface' in self.tensor_dict[tensor_key].columns:
                            menu.addAction('Create Categories', lambda: self.Create_Categories(tensor_key))

            menu.exec(self.treeWidget.viewport().mapToGlobal(position))

    def show_area_on_map_osm_well(self):
        """
        Opens a new window with an OpenStreetMap showing the specified area using Leaflet.

        Args:
            x_min, y_min, x_max, y_max: Coordinates defining the bounding box (longitude, latitude)
            source_crs: Coordinate reference system of the input coordinates (assumed EPSG:4326)
            area_label: Label for the selected area
        """

        def dms_to_dd(dms_str):
            """Convert DMS string (e.g., '30°01\'12.5019"N') to decimal degrees."""
            parts = dms_str.split('°')
            degrees = float(parts[0])
            parts = parts[1].split('\'')
            minutes = float(parts[0])
            parts = parts[1].split('"')
            seconds = float(parts[0])
            direction = parts[1]
            dd = degrees + minutes / 60 + seconds / 3600
            if direction in ['S', 'W']:
                dd = -dd
            return dd

        # Helper function to check if a value is valid (not None, not empty string)
        def is_valid(value):
            return value is not None and value != ''

        # Check if either dictionary exists for self.file_name
        if self.file_name not in self.well_head_dict and self.file_name not in self.well_deviation_dict:
            QMessageBox.information(self, "Missing Data", "Please load deviation or wellhead file for this dataset.")
            return

        # Define sets in priority order
        set_a_keys = ['Latitude', 'Longitude',
                      'Coordinate reference system Latitude, Longitude']  # WellHeadDict lat/lon
        set_b_keys = ['Surface X', 'Surface Y', 'Coordinate reference system X, Y']  # WellHeadDict X,Y
        set_c_keys = ['Well Head X Coordinate', 'Well Head Y Coordinate', 'Coordinate System']  # WellDeviationDict X,Y

        priority_sets = [
            (set_a_keys, self.well_head_dict),  # Set A
            (set_b_keys, self.well_head_dict),  # Set B
            (set_c_keys, self.well_deviation_dict)  # Set C
        ]

        # Initialize variables
        lon = lat = None
        set_processed = False

        # Step 1: Check for complete sets in priority order
        for keys, dictionary in priority_sets:
            if self.file_name in dictionary:
                metadata = dictionary[self.file_name]
                # Check if all keys are present and have valid values
                if all(key in metadata and is_valid(metadata[key]) for key in keys):
                    try:
                        if keys == set_a_keys:
                            # Process Set A (lat/lon)
                            lat_str = metadata['Latitude']
                            lon_str = metadata['Longitude']
                            lat = dms_to_dd(lat_str) if isinstance(lat_str, str) else float(lat_str)
                            lon = dms_to_dd(lon_str) if isinstance(lon_str, str) else float(lon_str)
                            crs_latlon = extract_epsg(metadata['Coordinate reference system Latitude, Longitude'])
                            if crs_latlon != 'EPSG:4326':
                                if crs_latlon is None:
                                    QMessageBox.warning(self, "Unconventional CRS",
                                                        "The coordinate reference system format is unconventional. Expected format: 'EPSG:code'.")
                                    return
                                transformer = Transformer.from_crs(crs_latlon, 'EPSG:4326', always_xy=True)
                                lon, lat = transformer.transform(lon, lat)
                        else:
                            # Process Set B or Set C (X,Y)
                            x_key, y_key, crs_key = keys
                            x = float(metadata[x_key])
                            y = float(metadata[y_key])
                            crs_xy = extract_epsg(metadata[crs_key])
                            if crs_xy is None:
                                QMessageBox.warning(self, "Unconventional CRS",
                                                    "The coordinate reference system format is unconventional. Expected format: 'EPSG:code'.")
                                return
                            transformer = Transformer.from_crs(crs_xy, 'EPSG:4326', always_xy=True)
                            lon, lat = transformer.transform(x, y)
                        set_processed = True
                        break
                    except (ValueError, TypeError):
                        # If conversion fails (e.g., invalid float), treat as incomplete
                        pass

        # Step 2: If no complete set, find the first partial set in priority order
        if not set_processed:
            for keys, dictionary in priority_sets:
                if self.file_name in dictionary:
                    metadata = dictionary[self.file_name]
                    # Check if any key is present with a valid value (partial set)
                    if any(key in metadata and is_valid(metadata[key]) for key in keys):
                        # Open dialog to complete the partial set
                        dialog = QDialog(self)
                        layout = QFormLayout(dialog)
                        inputs = {}
                        # Special handling for CRS input if missing or invalid
                        crs_key = keys[-1]
                        for key in keys:
                            if key == crs_key and (crs_key not in metadata or not metadata[crs_key]):
                                pass
                            else:
                                value = metadata.get(key, '')
                                # Only pre-fill if the value is valid
                                input_field = QLineEdit(str(value) if is_valid(value) else '')
                                layout.addRow(f"{key}:", input_field)
                                inputs[key] = input_field
                        if not is_valid(metadata.get(crs_key)):
                            crs_input = QComboBox(dialog)
                            common_crs = ["EPSG:4326", "EPSG:4269", "EPSG:4267", "EPSG:4230", "EPSG:4200", "EPSG:4151", "EPSG:4617", "EPSG:26713", "EPSG:26913", "EPSG:23031", "EPSG:5070", "EPSG:102003", "EPSG:27700", "EPSG:2193", "EPSG:2036", 'EPSG:32601', 'EPSG:32602', 'EPSG:32603', 'EPSG:32604', 'EPSG:32605', 'EPSG:32606', 'EPSG:32607', 'EPSG:32608', 'EPSG:32609', 'EPSG:32610', 'EPSG:32611', 'EPSG:32612', 'EPSG:32613', 'EPSG:32614', 'EPSG:32615', 'EPSG:32616', 'EPSG:32617', 'EPSG:32618', 'EPSG:32619', 'EPSG:32620', 'EPSG:32621', 'EPSG:32622', 'EPSG:32623', 'EPSG:32624', 'EPSG:32625', 'EPSG:32626', 'EPSG:32627', 'EPSG:32628', 'EPSG:32629', 'EPSG:32630', 'EPSG:32631', 'EPSG:32632', 'EPSG:32633', 'EPSG:32634', 'EPSG:32635', 'EPSG:32636', 'EPSG:32637', 'EPSG:32638', 'EPSG:32639', 'EPSG:32640', 'EPSG:32641', 'EPSG:32642', 'EPSG:32643', 'EPSG:32644', 'EPSG:32645', 'EPSG:32646', 'EPSG:32647', 'EPSG:32648', 'EPSG:32649', 'EPSG:32650', 'EPSG:32651', 'EPSG:32652', 'EPSG:32653', 'EPSG:32654', 'EPSG:32655', 'EPSG:32656', 'EPSG:32657', 'EPSG:32658', 'EPSG:32659', 'EPSG:32660', 'EPSG:32701', 'EPSG:32702', 'EPSG:32703', 'EPSG:32704', 'EPSG:32705', 'EPSG:32706', 'EPSG:32707', 'EPSG:32708', 'EPSG:32709', 'EPSG:32710', 'EPSG:32711', 'EPSG:32712', 'EPSG:32713', 'EPSG:32714', 'EPSG:32715', 'EPSG:32716', 'EPSG:32717', 'EPSG:32718', 'EPSG:32719', 'EPSG:32720', 'EPSG:32721', 'EPSG:32722', 'EPSG:32723', 'EPSG:32724', 'EPSG:32725', 'EPSG:32726', 'EPSG:32727', 'EPSG:32728', 'EPSG:32729', 'EPSG:32730', 'EPSG:32731', 'EPSG:32732', 'EPSG:32733', 'EPSG:32734', 'EPSG:32735', 'EPSG:32736', 'EPSG:32737', 'EPSG:32738', 'EPSG:32739', 'EPSG:32740', 'EPSG:32741', 'EPSG:32742', 'EPSG:32743', 'EPSG:32744', 'EPSG:32745', 'EPSG:32746', 'EPSG:32747', 'EPSG:32748', 'EPSG:32749', 'EPSG:32750', 'EPSG:32751', 'EPSG:32752', 'EPSG:32753', 'EPSG:32754', 'EPSG:32755', 'EPSG:32756', 'EPSG:32757', 'EPSG:32758', 'EPSG:32759', 'EPSG:32760']
                            crs_input.addItems(common_crs)
                            crs_input.setEditable(True)
                            layout.addRow(f"{crs_key}:", crs_input)
                            inputs[crs_key] = crs_input
                        # Add dialog buttons
                        button_box = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
                        button_box.accepted.connect(dialog.accept)
                        button_box.rejected.connect(dialog.reject)
                        layout.addRow(button_box)
                        if dialog.exec() == QDialog.DialogCode.Accepted:
                            # Update metadata with user inputs
                            for key, input_field in inputs.items():
                                value = input_field.currentText() if isinstance(input_field,
                                                                                QComboBox) else input_field.text()
                                metadata[key] = value if value else None  # Store None if empty
                            # Re-check if the set is now complete
                            if all(is_valid(metadata.get(key)) for key in keys):
                                try:
                                    if keys == set_a_keys:
                                        lat_str = metadata['Latitude']
                                        lon_str = metadata['Longitude']
                                        lat = dms_to_dd(lat_str) if isinstance(lat_str, str) else float(lat_str)
                                        lon = dms_to_dd(lon_str) if isinstance(lon_str, str) else float(lon_str)
                                        crs_latlon = extract_epsg(
                                            metadata['Coordinate reference system Latitude, Longitude'])
                                        if crs_latlon != 'EPSG:4326':
                                            if crs_latlon is None:
                                                QMessageBox.warning(self, "Unconventional CRS",
                                                                    "The coordinate reference system format is unconventional. Expected format: 'EPSG:code'.")
                                                return
                                            transformer = Transformer.from_crs(crs_latlon, 'EPSG:4326', always_xy=True)
                                            lon, lat = transformer.transform(lon, lat)
                                    else:
                                        x_key, y_key, crs_key = keys
                                        x = float(metadata[x_key])
                                        y = float(metadata[y_key])
                                        crs_xy = extract_epsg(metadata[crs_key])
                                        if crs_xy is None:
                                            QMessageBox.warning(self, "Unconventional CRS",
                                                                "The coordinate reference system format is unconventional. Expected format: 'EPSG:code'.")
                                            return
                                        transformer = Transformer.from_crs(crs_xy, 'EPSG:4326', always_xy=True)
                                        lon, lat = transformer.transform(x, y)
                                    set_processed = True
                                except (ValueError, TypeError):
                                    pass  # If still invalid, skip
                        break  # Stop after attempting the first partial set

        # Step 3: If no set was processed, inform the user
        if not set_processed:
            QMessageBox.information(self, "Missing Data",
                                    "No sufficient metadata found. Please provide the necessary information.")
            return

        # Set up the window
        window = QMainWindow(self)
        window.setWindowTitle(f"Map - {self.file_name}")
        window.setGeometry(100, 100, 800, 600)

        # Create central widget and layout
        central_widget = QWidget(window)
        window.setCentralWidget(central_widget)
        layout = QVBoxLayout(central_widget)
        layout.setContentsMargins(0, 0, 0, 0)

        # Add QWebEngineView
        web_view = QWebEngineView(window)
        layout.addWidget(web_view)

        # HTML content with Leaflet and OSM, using transformed coordinates
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
          <meta charset="utf-8">
          <title>{self.file_name}</title>
          <link rel="stylesheet" href="https://unpkg.com/leaflet@1.7.1/dist/leaflet.css" />
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.draw/1.0.4/leaflet.draw.css" />
          <script src="https://unpkg.com/leaflet@1.7.1/dist/leaflet.js"></script>
          <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.draw/1.0.4/leaflet.draw.js"></script>
          <style>
              html, body, #map {{
                  width: 100%;
                  height: 100%;
                  margin: 0;
                  padding: 0;
              }}
          </style>
        </head>
        <body>
          <div id="map"></div>
          <script>
              // Variables from Python (transformed coordinates)
              var lat = {lat};
              var lon = {lon};
              var area_label = "{self.file_name}";

              // Initialize the map
              var map = L.map('map').setView([lat, lon], 12);

              // Add OSM tiles
                var osm = L.tileLayer('https://{{s}}.tile.openstreetmap.org/{{z}}/{{x}}/{{y}}.png', {{
                    attribution: '© <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }}).addTo(map);

                // Add OpenTopoMap as an alternative layer
                var opentopo = L.tileLayer('https://{{s}}.tile.opentopomap.org/{{z}}/{{x}}/{{y}}.png', {{
                    attribution: 'Map data: © <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors, <a href="http://viewfinderpanoramas.org">SRTM</a> | Map style: © <a href="https://opentopomap.org">OpenTopoMap</a> (<a href="https://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA</a>)'
                }});

                // SATELLITE/AERIAL LAYERS
                var ESRI = L.tileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{{z}}/{{y}}/{{x}}', {{
                    attribution: 'Tiles © Esri | Source: Esri, Maxar, Earthstar Geographics'
                }});

                // Layer control
                var baseLayers = {{
                    "OpenStreetMap": osm,
                    "OpenTopoMap": opentopo,
                    "ESRI World Imagery": ESRI
                }};
              L.control.layers(baseLayers).addTo(map);

              // Add marker for the point
              var marker = L.marker([lat, lon]).addTo(map);
              marker.bindPopup("<b>" + area_label + "</b><br>Coordinates: " + lon.toFixed(6) + ", " + lat.toFixed(6)).openPopup();

              // Set up drawing tools
              var drawnItems = new L.FeatureGroup();
              map.addLayer(drawnItems);

              var drawControl = new L.Control.Draw({{
                  edit: {{ featureGroup: drawnItems }},
                  draw: {{
                      polyline: {{ shapeOptions: {{ color: '#FF0000', weight: 3 }} }},
                      rectangle: {{ shapeOptions: {{ color: '#00FF00', weight: 2, fillOpacity: 0.2 }} }},
                      circle: {{ shapeOptions: {{ color: '#FFFF00', weight: 2, fillOpacity: 0.2 }} }},
                      polygon: {{ shapeOptions: {{ color: '#FF00FF', weight: 2, fillOpacity: 0.2 }} }},
                      marker: {{ icon: L.icon({{ iconUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/images/marker-icon.png', iconSize: [25, 41], iconAnchor: [12, 41] }}) }}
                  }}
              }});
              map.addControl(drawControl);

              // Area calculation function (simple spherical approximation)
              function calculateArea(latlngs) {{
                  var area = 0;
                  var earthRadius = 6371000; // meters
                  var pointsCount = latlngs.length;
                  if (pointsCount < 3) return 0;
                  for (var i = 0; i < pointsCount; i++) {{
                      var p1 = latlngs[i];
                      var p2 = latlngs[(i + 1) % pointsCount];
                      area += (p2.lng - p1.lng) * (2 + Math.sin(p1.lat * Math.PI / 180) + Math.sin(p2.lat * Math.PI / 180));
                  }}
                  area = area * earthRadius * earthRadius / 2.0;
                  return Math.abs(area);
              }}

              // Handle drawing events
                map.on('draw:created', function (e) {{
                    var type = e.layerType,
                        layer = e.layer;

                    if (type === 'polyline') {{
                        var latlngs = layer.getLatLngs();
                        var distance = 0;
                        for (var i = 0; i < latlngs.length - 1; i++) {{
                            distance += latlngs[i].distanceTo(latlngs[i+1]);
                        }}
                        layer.bindPopup('Length: ' + (distance / 1000).toFixed(2) + ' km').openPopup();
                    }} else if (type === 'polygon' || type === 'rectangle') {{
                        var latlngs = layer.getLatLngs()[0];
                        var area = L.GeometryUtil.geodesicArea(latlngs);
                        layer.bindPopup('Area: ' + (area / 1000000).toFixed(2) + ' km²').openPopup();
                    }} else if (type === 'circle') {{
                        var radius = layer.getRadius();
                        var area = Math.PI * radius * radius;
                        layer.bindPopup('Radius: ' + (radius / 1000).toFixed(2) + ' km<br>Area: ' + (area / 1000000).toFixed(2) + ' km²').openPopup();
                    }}

                    drawnItems.addLayer(layer);
                }});

              // Clear all button
              var ClearControl = L.Control.extend({{
                  options: {{ position: 'topright' }},
                  onAdd: function (map) {{
                      var container = L.DomUtil.create('div', 'leaflet-bar leaflet-control leaflet-control-custom');
                      container.style.backgroundColor = 'white';
                      container.style.padding = '5px';
                      container.style.cursor = 'pointer';
                      container.innerHTML = 'Clear All';
                      container.onclick = function(){{ drawnItems.clearLayers(); }};
                      return container;
                  }}
              }});
              map.addControl(new ClearControl());
          </script>
        </body>
        </html>
        """

        # Load HTML into QWebEngineView
        web_view.setHtml(html)

        # Show the window
        window.show()

    def show_area_on_map_osm_seismic(self):
        """
        Opens a new window with an OpenStreetMap showing the specified area using Leaflet.

        Args:
            x_min, y_min, x_max, y_max: Coordinates defining the bounding box (longitude, latitude)
            source_crs: Coordinate reference system of the input coordinates (assumed EPSG:4326)
            area_label: Label for the selected area
        """

        # Extract the min and max values from the filename
        metadata = self.metadata[self.file_name]

        # Define the required metadata keys
        required_keys = ['origin', 'xline_end', 'inline_end', 'crs']

        # Check if any required fields are missing or empty
        missing = [key for key in required_keys if not metadata.get(key)]

        # If any fields are missing, open a dialog to get the values
        if missing:
            dialog = QDialog(self)
            layout = QFormLayout(dialog)

            # Create input fields for tuples, pre-filling with existing values if available
            # Origin tuple (x, y)
            origin_x_input = QLineEdit(str(metadata.get('origin', (0, 0))[0] if 'origin' in metadata else ''))
            origin_y_input = QLineEdit(str(metadata.get('origin', (0, 0))[1] if 'origin' in metadata else ''))

            # X-line end tuple (x, y)
            xline_end_x_input = QLineEdit(str(metadata.get('xline_end', (0, 0))[0] if 'xline_end' in metadata else ''))
            xline_end_y_input = QLineEdit(str(metadata.get('xline_end', (0, 0))[1] if 'xline_end' in metadata else ''))

            # In-line end tuple (x, y)
            inline_end_x_input = QLineEdit(
                str(metadata.get('inline_end', (0, 0))[0] if 'inline_end' in metadata else ''))
            inline_end_y_input = QLineEdit(
                str(metadata.get('inline_end', (0, 0))[1] if 'inline_end' in metadata else ''))

            # Create a combo box for CRS with common options
            crs_input = QComboBox(dialog)
            common_crs = ['EPSG:32601', 'EPSG:32602', 'EPSG:32603', 'EPSG:32604', 'EPSG:32605']
            crs_input.addItems(common_crs)
            if metadata.get('crs'):  # Pre-select the existing CRS if it's there
                crs_input.setCurrentText(extract_epsg(metadata['crs']))
            crs_input.setEditable(True)  # Allow custom CRS input

            # Add the input fields to the layout with labels
            layout.addRow('Origin X:', origin_x_input)
            layout.addRow('Origin Y:', origin_y_input)
            layout.addRow('Cross-line End X:', xline_end_x_input)
            layout.addRow('Cross-line End Y:', xline_end_y_input)
            layout.addRow('In-line End X:', inline_end_x_input)
            layout.addRow('In-line End Y:', inline_end_y_input)
            layout.addRow('CRS:', crs_input)

            # Add OK and Cancel buttons
            button_box = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
            button_box.accepted.connect(dialog.accept)
            button_box.rejected.connect(dialog.reject)
            layout.addRow(button_box)

            dialog.setLayout(layout)

            # Show the dialog and update metadata if the user clicks OK
            if dialog.exec() == QDialog.DialogCode.Accepted:
                # Store as tuples
                metadata['origin'] = (float(origin_x_input.text()), float(origin_y_input.text()))
                metadata['xline_end'] = (float(xline_end_x_input.text()), float(xline_end_y_input.text()))
                metadata['inline_end'] = (float(inline_end_x_input.text()), float(inline_end_y_input.text()))
                if not metadata.get('crs'):
                    metadata['crs'] = crs_input.currentText()
            else:
                # If the user cancels, exit the function
                return

        source_crs = extract_epsg(metadata['crs'])
        if source_crs is None:
            QMessageBox.warning(self, "Unconventional CRS",
                                "The coordinate reference system format is unconventional. Expected format: 'EPSG:code'.")
            return

        # Get the three points from metadata
        origin = metadata['origin']  # (x, y) tuple for origin point
        xline_end = metadata['xline_end']  # (x, y) tuple for x-line end point
        inline_end = metadata['inline_end']  # (x, y) tuple for in-line end point

        # Calculate the fourth corner of the rectangle
        # The fourth point is at origin + (xline_end - origin) + (inline_end- origin)
        fourth_corner = (
            origin[0] + (xline_end[0] - origin[0]) + (inline_end[0] - origin[0]),
            origin[1] + (xline_end[1] - origin[1]) + (inline_end[1] - origin[1])
        )

        # Extract individual coordinates for all four corners
        origin_x, origin_y = origin
        xline_x, xline_y = xline_end
        inline_x, inline_y = inline_end
        fourth_x, fourth_y = fourth_corner

        # Transform coordinates from source_crs to EPSG:4326 (Leaflet's lat/lon)
        transformer = Transformer.from_crs(source_crs, 'EPSG:4326', always_xy=True)

        # Transform all four corners to lat/lon
        origin_lon, origin_lat = transformer.transform(origin_x, origin_y)
        xline_lon, xline_lat = transformer.transform(xline_x, xline_y)
        inline_lon, inline_lat = transformer.transform(inline_x, inline_y)
        fourth_lon, fourth_lat = transformer.transform(fourth_x, fourth_y)

        # Calculate center in source CRS, then transform to lat/lon
        center_x = (origin_x + fourth_x) / 2
        center_y = (origin_y + fourth_y) / 2
        center_lon, center_lat = transformer.transform(center_x, center_y)

        # Set up the window
        window = QMainWindow(self)
        window.setWindowTitle(f"Map - {self.file_name}")
        window.setGeometry(100, 100, 800, 600)

        # Create central widget and layout
        central_widget = QWidget(window)
        window.setCentralWidget(central_widget)
        layout = QVBoxLayout(central_widget)
        layout.setContentsMargins(0, 0, 0, 0)

        # Add QWebEngineView
        web_view = QWebEngineView(window)
        layout.addWidget(web_view)

        # HTML content with Leaflet and OSM, using transformed coordinates
        html = f"""
        <!DOCTYPE html>
        <html>

        <head>
            <meta charset="utf-8">
            <title>{self.file_name}</title>
            <link rel="stylesheet" href="https://unpkg.com/leaflet@1.7.1/dist/leaflet.css" />
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.draw/1.0.4/leaflet.draw.css" />
            <script src="https://unpkg.com/leaflet@1.7.1/dist/leaflet.js"></script>
            <script src="https://unpkg.com/leaflet-geometryutil@0.9.3/src/leaflet.geometryutil.js"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.draw/1.0.4/leaflet.draw.js"></script>
            <style>
                html, body, #map {{
                    width: 100%;
                    height: 100%;
                    margin: 0;
                    padding: 0;
                }}
            </style>
        </head>

        <body>
            <div id="map"></div>
            <script>
                // Variables from Python (transformed coordinates for all four corners)
                var origin_lat = {origin_lat};
                var origin_lon = {origin_lon};
                var xline_lat = {xline_lat};
                var xline_lon = {xline_lon};
                var inline_lat = {inline_lat};
                var inline_lon = {inline_lon};
                var fourth_lat = {fourth_lat};
                var fourth_lon = {fourth_lon};
                var center_lat = {center_lat};
                var center_lon = {center_lon};
                var area_label = "{self.file_name}";

                // Initialize the map
                var map = L.map('map').setView([center_lat, center_lon], 12);

                // Add OSM tiles
                var osm = L.tileLayer('https://{{s}}.tile.openstreetmap.org/{{z}}/{{x}}/{{y}}.png', {{
                    attribution: '© <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }}).addTo(map);

                // Add OpenTopoMap as an alternative layer
                var opentopo = L.tileLayer('https://{{s}}.tile.opentopomap.org/{{z}}/{{x}}/{{y}}.png', {{
                    attribution: 'Map data: © <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors, <a href="http://viewfinderpanoramas.org">SRTM</a> | Map style: © <a href="https://opentopomap.org">OpenTopoMap</a> (<a href="https://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA</a>)'
                }});

                // SATELLITE/AERIAL LAYERS
                var ESRI = L.tileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{{z}}/{{y}}/{{x}}', {{
                    attribution: 'Tiles © Esri | Source: Esri, Maxar, Earthstar Geographics'
                }});

                // Layer control
                var baseLayers = {{
                    "OpenStreetMap": osm,
                    "OpenTopoMap": opentopo,
                    "ESRI World Imagery": ESRI
                }};
                L.control.layers(baseLayers).addTo(map);

                // Area calculation function (simple spherical approximation) - kept as is since it's not used now
                function calculateArea(latlngs) {{
                    var area = 0;
                    var earthRadius = 6371000; // meters
                    var pointsCount = latlngs.length;
                    if (pointsCount < 3) return 0;
                    for (var i = 0; i < pointsCount; i++) {{
                        var p1 = latlngs[i];
                        var p2 = latlngs[(i + 1) % pointsCount];
                        area += (p2[1] - p1[1]) * (2 + Math.sin(p1[0] * Math.PI / 180) + Math.sin(p2[0] * Math.PI / 180));
                    }}
                    area = area * earthRadius * earthRadius / 2.0;
                    return Math.abs(area);
                }}

                // Create rectangle using all four corners
                var rectanglePoints = [
                    [origin_lat, origin_lon],
                    [xline_lat, xline_lon],
                    [fourth_lat, fourth_lon],
                    [inline_lat, inline_lon]
                ];
                var polygon = L.polygon(rectanglePoints, {{color: "#0000FF", weight: 2, fillOpacity: 0.2}}).addTo(map);

                // Calculate area using Leaflet's GeometryUtil directly (no try-catch since it works)
                var tempLatLngs = rectanglePoints.map(coord => L.latLng(coord[0], coord[1]));
                var areaInSquareMeters = L.GeometryUtil.geodesicArea(tempLatLngs);

                // Add center marker with area information
                var marker = L.marker([center_lat, center_lon]).addTo(map);
                marker.bindPopup("<b>Center of " + area_label + "</b><br>Coordinates: " + center_lon.toFixed(6) + ", " + center_lat.toFixed(6) + "<br>Area: " + (areaInSquareMeters / 1000000).toFixed(2) + " km²").openPopup();

                // Fit map to rectangle bounds
                map.fitBounds(polygon.getBounds());

                // Set up drawing tools
                var drawnItems = new L.FeatureGroup();
                map.addLayer(drawnItems);

                var drawControl = new L.Control.Draw({{
                    edit: {{ featureGroup: drawnItems }},
                    draw: {{
                        polyline: {{ shapeOptions: {{ color: '#FF0000', weight: 3 }} }},
                        rectangle: {{ shapeOptions: {{ color: '#00FF00', weight: 2, fillOpacity: 0.2 }} }},
                        circle: {{ shapeOptions: {{ color: '#FFFF00', weight: 2, fillOpacity: 0.2 }} }},
                        polygon: {{ shapeOptions: {{ color: '#FF00FF', weight: 2, fillOpacity: 0.2 }} }},
                        marker: {{ icon: L.icon({{ iconUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/images/marker-icon.png', iconSize: [25, 41], iconAnchor: [12, 41] }}) }}
                    }}
                }});
                map.addControl(drawControl);

                // Handle drawing events
                map.on('draw:created', function (e) {{
                    var type = e.layerType,
                        layer = e.layer;

                    if (type === 'polyline') {{
                        var latlngs = layer.getLatLngs();
                        var distance = 0;
                        for (var i = 0; i < latlngs.length - 1; i++) {{
                            distance += latlngs[i].distanceTo(latlngs[i+1]);
                        }}
                        layer.bindPopup('Length: ' + (distance / 1000).toFixed(2) + ' km').openPopup();
                    }} else if (type === 'polygon' || type === 'rectangle') {{
                        var latlngs = layer.getLatLngs()[0];
                        var area = L.GeometryUtil.geodesicArea(latlngs);
                        layer.bindPopup('Area: ' + (area / 1000000).toFixed(2) + ' km²').openPopup();
                    }} else if (type === 'circle') {{
                        var radius = layer.getRadius();
                        var area = Math.PI * radius * radius;
                        layer.bindPopup('Radius: ' + (radius / 1000).toFixed(2) + ' km<br>Area: ' + (area / 1000000).toFixed(2) + ' km²').openPopup();
                    }}

                    drawnItems.addLayer(layer);
                }});

                // Clear all button
                var ClearControl = L.Control.extend({{
                    options: {{ position: 'topright' }},
                    onAdd: function (map) {{
                        var container = L.DomUtil.create('div', 'leaflet-bar leaflet-control leaflet-control-custom');
                        container.style.backgroundColor = 'white';
                        container.style.padding = '5px';
                        container.style.cursor = 'pointer';
                        container.innerHTML = 'Clear All';
                        container.onclick = function(){{ drawnItems.clearLayers(); }};
                        return container;
                    }}
                }});
                map.addControl(new ClearControl());
            </script>
        </body>
        </html>
        """

        # Load HTML into QWebEngineView
        web_view.setHtml(html)

        # Show the window
        window.show()

    def select_template(self):
        # Create a new dialog to select Template Type
        template_dialog = QDialog(self)
        template_dialog.setWindowTitle("Select Template Type")
        layout = QVBoxLayout(template_dialog)
        template_type_combo = QComboBox()
        template_type_combo.addItems([
            'Seismic',
            'App. Polarity',
            'Inst. Frequency',
            'Cos Phase',
            'Inst. Phase',
            'Envelope',
            'Inst. Bandwidth',
            'Dom. Frequency',
            'Sweetness',
            'RMS Amplitude',
            'Geobodies',
            'Anomalies',
            'Probe',
            'Upscaled',
            'Clusters',
            'Zones',
            'Annotations',
            'Other'
        ])

        layout.addWidget(template_type_combo)

        select_button = QPushButton("OK")
        select_button.clicked.connect(template_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        template_dialog.setLayout(layout)
        template_dialog.adjustSize()
        template_dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        template_dialog.setWindowFlags(
            template_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

        QApplication.restoreOverrideCursor()
        result = template_dialog.exec()  # Use exec() to block until the dialog is closed
        if result == QDialog.DialogCode.Accepted:

            if template_type_combo.currentText() == 'Other':
                template_name, ok = QInputDialog.getText(self, "Input Dialog", "Enter Template name:")
                if ok:
                    template_type = template_name
                else:
                    template_type = 'Seismic'
            else:
                template_type = template_type_combo.currentText()

            # Step 4: Update the 'template' field in the metadata
            self.metadata[self.file_name]['template'] = template_type

            self.add_tensor(self.file_name, self.tensor_data)

    def Create_Categories(self, tensor_key):
        # Get the corresponding CSV DataFrame
        csv_df = self.tensor_dict[tensor_key]  # Initialize 'labels' column with placeholder values
        csv_df['Labels'] = -999.25

        # Variable to keep track of the label number
        label_number = 0

        # Iterate over the DataFrame
        for i, value in enumerate(csv_df['Surface']):
            if isinstance(value, str):  # Check if the value is a string
                label_number += 1  # Increment the label number
            if label_number > 0:  # Start assigning labels once we find a string
                csv_df.at[i, 'Labels'] = label_number

        self.add_tensor(tensor_key, csv_df)
        QMessageBox.information(self, "Action Completed", "Categories have been created successfully.")

    def show_well_info_tooltip(self, tensor_key):
        """Display and allow editing of well and deviation information in a dialog."""
        log_name = self.well_header_info_library[tensor_key].get('Well Name').strip()

        well_info = self.well_head_dict.get(log_name, {}) if log_name in self.well_head_dict else None
        deviation_info = self.well_deviation_dict.get(tensor_key,
                                                      {}) if tensor_key in self.well_deviation_dict else None
        well_header_info = self.well_header_info_library.get(tensor_key,
                                                             {}) if tensor_key in self.well_header_info_library else None

        # QDialog to display the information
        dialog = QDialog(self)
        dialog.setWindowFlags(
            dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)
        dialog.setWindowTitle("Edit Well and Deviation Information")
        dialog.setMinimumSize(400, 300)

        scroll_area = QScrollArea(dialog)
        scroll_area.setWidgetResizable(True)

        scroll_widget = QWidget()
        scroll_layout = QVBoxLayout(scroll_widget)

        editable_fields = {}  # Store QLineEdit fields for later updates

        # Function to add editable sections
        def add_editable_section(title, info_dict):
            nonlocal editable_fields
            if info_dict:
                header = QLabel(f"<b>{title}:</b>")
                scroll_layout.addWidget(header)
                for key, value in info_dict.items():
                    # Special handling for 'Deviation Data' in Deviation Information
                    if title == "Deviation Information" and key == "Deviation Data":
                        # Create label for vel_path
                        label = QLabel(f"<b>{key}</b>:")
                        scroll_layout.addWidget(label)

                        # Create DataFrame view
                        table_view = QTableView()
                        model = PandaswellinfoModel(value)
                        table_view.setModel(model)

                        # Make table editable
                        table_view.setEditTriggers(QAbstractItemView.EditTrigger.DoubleClicked |
                                                   QAbstractItemView.EditTrigger.EditKeyPressed)

                        # Set appropriate height based on number of rows
                        num_rows = min(15, len(value))  # limit to 15 rows max for display, rest will scroll
                        table_height = (num_rows * 30) + 30
                        table_view.setMinimumHeight(table_height)

                        scroll_layout.addWidget(table_view)

                        # Store the model reference for later updates
                        editable_fields[(title, key)] = model

                        # Connect to data changed signal for immediate saves
                        model.dataChanged.connect(lambda: handle_dataframe_change(title, key, model))

                    else:
                        # Normal handling for other fields
                        label = QLabel(f"<b>{key}</b>:")
                        editor = QLineEdit(str(value))
                        scroll_layout.addWidget(label)
                        scroll_layout.addWidget(editor)
                        editable_fields[(title, key)] = editor

                        # Connect to editingFinished signal for immediate saves
                        editor.editingFinished.connect(lambda editor=editor, section=title, k=key:
                                                       handle_line_edit_change(section, k, editor))

        # Handler for QLineEdit changes
        def handle_line_edit_change(section, key, editor):
            new_value = editor.text()
            original_value = str(get_original_value(section, key))

            # Only proceed if value actually changed
            if new_value != original_value:
                confirmation = QMessageBox.question(
                    dialog,
                    "Confirm Change",
                    f"Do you want to save the changes to {key}?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                )

                if confirmation == QMessageBox.StandardButton.Yes:
                    # Save the changes
                    if section == "Well Information" and well_info is not None:
                        well_info[key] = new_value
                    elif section == "Deviation Information" and deviation_info is not None:
                        deviation_info[key] = new_value
                    elif section == "Well Header" and well_header_info is not None:
                        well_header_info[key] = new_value
                else:
                    # Revert to original value
                    editor.setText(original_value)

        # Helper to get original value
        def get_original_value(section, key):
            if section == "Well Information" and well_info is not None:
                return well_info.get(key, "")
            elif section == "Deviation Information" and deviation_info is not None:
                return deviation_info.get(key, "")
            elif section == "Well Header" and well_header_info is not None:
                return well_header_info.get(key, "")
            return ""

        # Handler for DataFrame changes
        def handle_dataframe_change(section, key, model):
            if model.is_modified():
                confirmation = QMessageBox.question(
                    dialog,
                    "Confirm Changes",
                    f"You've made changes to the {key} data. Do you want to save these changes?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                )

                if confirmation == QMessageBox.StandardButton.Yes:
                    # Update the dataframe value in the dictionary
                    if section == "Deviation Information" and deviation_info is not None:
                        deviation_info[key] = model.get_data()
                else:
                    # Reset to original data
                    model.reset_to_original()

        add_editable_section("Well Information", well_info)
        add_editable_section("Deviation Information", deviation_info)
        add_editable_section("Well Header", well_header_info)

        scroll_area.setWidget(scroll_widget)

        main_layout = QVBoxLayout()
        main_layout.addWidget(scroll_area)

        dialog.setLayout(main_layout)
        dialog.show()

    def tensor_Annotator(self):
        original_shape = self.tensor_data.shape
        # Extract the depth, height, and width from the original shape
        depth, height, width, _ = original_shape

        num_channels, ok = QInputDialog.getInt(
            self, "Number of annotations",
            f"Enter the number of annotations:",
            min=1, max=100
        )
        if not ok:
            return

        # Create a new tensor with the specified number of channels, filled with zeros
        new_tensor = np.zeros((depth, height, width, num_channels))

        self.tensor_data = new_tensor
        self.metadata[f"{self.file_name}_Annotations"] = self.metadata[self.file_name].copy()
        self.metadata[f"{self.file_name}_Annotations"]['template'] = "Annotations"
        self.add_tensor(f"{self.file_name}_Annotations", self.tensor_data)

    def show_tensorboard(self, log_dir):
        # Start TensorBoard as a subprocess
        port_number = str(6006 + random.randint(0, 100))

        # Assuming you already have a logs directory
        base_log_dir = os.path.join(os.getcwd(), "logs")

        # Create a subdirectory for console logs
        console_log_dir = os.path.join(base_log_dir, "console_logs")
        os.makedirs(console_log_dir, exist_ok=True)

        # Create a log file with the current timestamp
        log_filename = f"tensorboard_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
        log_file_path = os.path.join(console_log_dir, log_filename)

        # Open the log file for writing
        log_file = open(log_file_path, "w")

        # Start the TensorBoard process and redirect stdout and stderr to the log file
        self.tensorboard_process = subprocess.Popen(
            ['tensorboard', '--logdir', log_dir, '--port', port_number],
            stdout=log_file,  # Redirect stdout to the log file
            stderr=subprocess.STDOUT,  # Redirect stderr to the same log file
            creationflags=subprocess.CREATE_NO_WINDOW  # Hide the console window
        )

        # Create a QDialog for displaying TensorBoard
        self.tensorboard_dialog = QDialog(self)
        self.tensorboard_dialog.setWindowTitle("TensorBoard")

        self.tensorboard_dialog.resize(1200, 800)

        # Ensure the dialog can be maximized and minimized independently
        self.tensorboard_dialog.setWindowFlags(
            self.tensorboard_dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        # Create the web view to display TensorBoard
        self.tensorboard_view = QWebEngineView()
        url = f'http://localhost:{port_number}'
        self.tensorboard_view.setUrl(QUrl(url))  # Convert string URL to QUrl

        # Connect the downloadRequested signal to handle downloads
        self.tensorboard_view.page().profile().downloadRequested.connect(self.handle_download)

        # Set up the layout and add the web view
        layout = QVBoxLayout()
        layout.addWidget(self.tensorboard_view)
        self.tensorboard_dialog.setLayout(layout)

        # Poll for TensorBoard availability and then load the page
        self.check_tensorboard_availability(url)

        # Connect the finished signal of the dialog to a method that terminates TensorBoard
        self.tensorboard_dialog.finished.connect(self.terminate_tensorboard)

        # Show the dialog
        self.tensorboard_dialog.exec()

    def terminate_tensorboard(self):
        if self.tensorboard_process:
            try:
                os.kill(self.tensorboard_process.pid, signal.SIGINT)
                self.tensorboard_process.wait()
            except Exception as e:
                self.tensorboard_process.terminate()
                self.tensorboard_process.wait()
            finally:
                self.tensorboard_process = None

    def check_tensorboard_availability(self, url):

        def load_tensorboard():
            try:
                response = requests.get(url)
                if response.status_code == 200:
                    # TensorBoard is ready, load the page
                    self.tensorboard_view.setUrl(QUrl(url))
                else:
                    # Retry after a short delay
                    QTimer.singleShot(500, load_tensorboard)
            except requests.exceptions.ConnectionError:
                # Retry after a short delay if connection failed
                QTimer.singleShot(500, load_tensorboard)

        # Start the loading process
        load_tensorboard()

    def handle_download(self, download_item: QWebEngineDownloadRequest):
        # Get the original file name and extension
        original_filename = download_item.downloadFileName()
        original_extension = QFileInfo(original_filename).suffix()  # Extract the file extension
        # Open a QFileDialog to select the save location and optionally modify the file name
        full_path, _ = QFileDialog.getSaveFileName(
            self,
            "Save File As",
            original_filename,
            f"Files (*.{original_extension})"
        )

        # If the user cancels the dialog, full_path will be an empty string
        if full_path:
            # Ensure the selected path includes the original extension
            if not full_path.endswith(f".{original_extension}"):
                full_path += f".{original_extension}"

            # Set the file path for the download item
            download_item.setDownloadFileName(full_path)
            download_item.accept()

    def petro_context_menu_event(self, event):
        # Create the main context menu
        menu = QMenu(self)

        # "Color Mapping" menu item with submenu
        colormap_menu = menu.addMenu("Color Mapping")

        # List of colormap options
        color_mappings = [
            'jet', 'jet_r', 'turbo', 'turbo_r', 'hsv', 'hsv_r', 'gist_rainbow', 'gist_rainbow_r','gist_ncar_r',
             'tab20', 'tab20_r'
        ]

        # Add colormap options to the submenu
        for color_mapping in color_mappings:
            action = QAction(color_mapping, self.petro_widget)
            action.triggered.connect(lambda checked, cm=color_mapping: self.change_petro_colormap(cm))
            colormap_menu.addAction(action)

        # "Export" menu item
        export_action = QAction("Export", self.petro_widget)
        export_action.triggered.connect(self.show_petro_export_dialog)
        menu.addAction(export_action)

        # Add the colormap submenu to the main menu
        menu.addMenu(colormap_menu)

        # Show the context menu at the position of the event
        menu.exec(event.screenPos())

    def show_petro_export_dialog(self):
        # Create and show the export dialog
        self.export_petro_dialog = exportDialog.ExportDialog(self.petro_widget.scene())
        if not self.isDarkTheme:
            stylesheet = """
            QWidget {{
                background-color: {light_color_2};
                color: {light_color_3};
            }}
            QListWidget {{
                background-color: {light_color_5}; /* Light grey background */
                color: {light_color_3}; /* Dark grey text */
            }}
            QListWidget::item:selected {{
                background-color: {light_color_2}; /* Slightly darker grey for selected item */
                color: {light_color_3}; /* Dark grey text for consistency */
            }}
            QTreeWidget {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QTreeWidget QMenu {{
                background-color: {light_color_5}; /* Menu background */
                color: {light_color_3}; /* Menu text */
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QComboBox:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox QAbstractItemView {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QDialog {{
                background-color: {light_color_1}; /* Background color */
                color: {light_color_3}; /* Selection text color */
            }}
            QAbstractItemView {{
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QScrollArea {{
                background-color: {light_color_5}; /* Scroll area background */
                color: {light_color_3}; /* Scroll area text */
            }}
            QScrollArea QWidget {{
                background-color: {light_color_5}; /* Content background */
                color: {light_color_3}; /* Content text */
            }}
            QScrollBar:vertical {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            QScrollBar:horizontal {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            """.format(
                light_color_1=self.light_color_1,
                light_color_2=self.light_color_2,
                light_color_3=self.light_color_3,
                light_color_4=self.light_color_4,
                light_color_5=self.light_color_5,
                light_color_6=self.light_color_6,
                light_color_7=self.light_color_7,
                light_color_8=self.light_color_8,
                light_color_9=self.light_color_9,
                light_color_10=self.light_color_10,
            )
        else:
            stylesheet = """
            QWidget {{
                background-color: {darkColor1};
                color: {darkColor8};
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {darkColor1};
                color: {darkColor5};
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QComboBox:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox QAbstractItemView {{
                background-color: {darkColor3}; /* Same as the combo box background */
                color: {darkColor5}; /* Text color */
                selection-background-color: {darkColor9}; /* Background color when an item is selected */
                selection-color: {darkColor5}; /* Text color when an item is selected */
            }}
            QTreeWidget {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QTreeWidget QMenu {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QDialog {{
                background-color: {darkColor1};
            }}
            QAbstractItemView {{
                selection-background-color: {darkColor3};
                selection-color: {darkColor5};
            }}
            QScrollArea {{
                background-color: {darkColor2}; /* Dark grey background for the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollArea QWidget {{
                background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollBar:vertical {{
                background-color: {darkColor2};
            }}
            QScrollBar:horizontal {{
                background-color: {darkColor2};
            }}
            """.format(
                darkColor1=self.darkColor1,
                darkColor2=self.darkColor2,
                darkColor3=self.darkColor3,
                darkColor4=self.darkColor4,
                darkColor5=self.darkColor5,
                darkColor6=self.darkColor6,
                darkColor7=self.darkColor7,
                darkColor8=self.darkColor8,
                darkColor9=self.darkColor9,
                darkColor10=self.darkColor10,
                darkColor11=self.darkColor11,
                darkColor12=self.darkColor12,
                darkColor13=self.darkColor13,
            )
        self.export_petro_dialog.setStyleSheet(stylesheet)
        self.export_petro_dialog.show(self.petro_widget.plotItem)

    def change_petro_colormap(self, color_mapping):
        self.petro_color_mapping = color_mapping

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.petro_color_mapping, source='matplotlib')

        # Map labels to colors and convert to QColor for pyqtgraph
        colors = cmap.map(self.petro_color_data, mode='qcolor')

        # Create a list of brushes for each color
        brushes = list(map(pg.mkBrush, colors))

        # Update the brushes of the existing scatter plot item
        scatter = self.petro_widget.listDataItems()[0]  # Assuming the scatter plot is the first item
        scatter.setBrush(brushes)

        # Update the ColorBarItem for the color bar
        if hasattr(self, 'petro_color_bar') and self.petro_color_bar is not None:
            self.petro_color_bar.setColorMap(cmap)

    def open_petro_dialog(self):
        # Check if the dialog already exists and is open
        if hasattr(self, 'petro_dialog') and self.petro_dialog.isVisible():
            # Bring the existing dialog to the front
            self.petro_dialog.activateWindow()
            return

        # Create a new dialog only if one doesn't exist or isn't visible
        self.petro_dialog = QDialog(self)
        self.petro_dialog.resize(1000, 600)
        self.petro_dialog.setWindowTitle("Petrophysical Characterization")
        # Ensure the dialog can be maximized and minimized independently
        self.petro_dialog.setWindowFlags(
            self.petro_dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        # Set up the main layout
        splitter = QSplitter(Qt.Orientation.Horizontal, self.petro_dialog)

        # Left layout: Scrollable results list, ComboBox, Plot button
        left_widget = QWidget(self.petro_dialog)  # Changed parent to self.petro_dialog
        left_layout = QVBoxLayout(left_widget)

        # Scrollable area for results
        result_area = QScrollArea(self.petro_dialog)  # Changed parent to self.petro_dialog
        result_area.setWidgetResizable(True)
        result_area.setFixedHeight(200)  # Set the height to a shorter value
        result_widget = QWidget(self.petro_dialog)
        result_layout = QVBoxLayout(result_widget)
        result_area.setWidget(result_widget)
        left_layout.addWidget(result_area)

        # ComboBox for selecting analysis type
        analysis_type_combo = QComboBox(self.petro_dialog)
        analysis_type_combo.addItems(["Regression", "Classification", "Regression Prediction" ,"Classification Prediction"])
        left_layout.addWidget(analysis_type_combo)

        # Plot button
        plot_button = QPushButton("Run")
        plot_button.clicked.connect(lambda: self.handle_petro_button(analysis_type_combo, result_layout))
        left_layout.addWidget(plot_button)

        # Spacer to push everything to the top
        spacer = QSpacerItem(20, 40, QSizePolicy.Policy.Minimum, QSizePolicy.Policy.Expanding)
        left_layout.addItem(spacer)

        splitter.addWidget(left_widget)  # Add the left layout to the splitter

        right_widget = QWidget(self.petro_dialog)
        right_layout = QVBoxLayout(right_widget)

        # Right layout: PyQtGraph plot area
        self.petro_widget = pg.PlotWidget()  # Assuming self.plot_widget is still a class attribute
        self.petro_widget.setBackground(self.last_selected_color)
        self.petro_widget.setMenuEnabled(False)
        self.petro_widget.scene().contextMenuEvent = self.petro_context_menu_event
        self.petro_widget.hideAxis('left')
        self.petro_widget.hideAxis('bottom')

        # Add padding around the plot widget
        right_layout.setContentsMargins(10, 10, 10, 10)  # Add margins (left, top, right, bottom) as needed
        right_layout.addWidget(self.petro_widget)

        splitter.addWidget(right_widget)  # Add the right layout to the splitter

        # Set initial sizes (ratios) for the splitter
        splitter.setSizes([300, 700])  # Adjust these values as needed to set the initial ratio

        # Use the splitter as the main layout
        dialog_layout = QVBoxLayout(self.petro_dialog)
        dialog_layout.addWidget(splitter)
        self.petro_dialog.setLayout(dialog_layout)

        self.petro_dialog.finished.connect(self.reset_petro_color_bar)
        self.petro_dialog.show()

    def reset_petro_color_bar(self):
        self.petro_color_bar = None

    def handle_petro_button(self, analysis_type_combo, result_layout):
        # Determine the selected analysis type
        analysis_type = analysis_type_combo.currentText()

        if analysis_type == "Regression":
            self.prepare_for_training(result_layout, analysis_type)
        elif analysis_type == "Classification":
            self.prepare_for_training(result_layout, analysis_type)
        elif analysis_type == "Classification Prediction":
            self.process_and_predict_petro()
        elif analysis_type == "Regression Prediction":
            self.process_and_predict_petro_regression()

    def add_wells_to_seismic(self):
        # Create a new dialog to select CSV files
        csv_selection_dialog = QDialog(self)
        csv_selection_dialog.setWindowTitle("Select Wells")

        layout = QVBoxLayout(csv_selection_dialog)

        csv_list_widget = QListWidget()
        csv_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
        for key in self.tensor_dict:
            if isinstance(self.tensor_dict[key], pd.DataFrame):
                csv_list_widget.addItem(key)

        layout.addWidget(csv_list_widget)

        select_button = QPushButton("OK")
        select_button.clicked.connect(csv_selection_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        csv_selection_dialog.setLayout(layout)
        if csv_selection_dialog.exec() == QDialog.DialogCode.Rejected:
            return  # Exit function if dialog was closed without selection

        # Get selected CSV files
        selected_items = csv_list_widget.selectedItems()
        selected_keys = [item.text() for item in selected_items]
        self.handle_csv_selection(selected_keys, selected_items)

    def handle_csv_selection(self, selected_keys, selected_items):
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            sampling_interval = self.metadata[self.file_name].get('sampling_interval_ms')

            selected_files = [item.text() for item in selected_items]

            if len(selected_files) == 0:
                QApplication.restoreOverrideCursor()
                QMessageBox.warning(self, "Warning", "Please select at least one CSV file.")
                return

            def valid_columns(df):
                """Return columns that are not entirely NaN or negative after converting to numeric."""
                valid_columns = []
                for col in df.columns:
                    numeric_col = pd.to_numeric(df[col], errors='coerce')
                    # Check if the column is not entirely NaN and does not contain only negative values
                    if not numeric_col.isna().all() and not (numeric_col < 0).all():
                        valid_columns.append(col)
                return valid_columns

            if len(selected_files) == 1:
                # If only one file is selected, directly use its columns
                data_frame = self.tensor_dict[selected_files[0]].copy()  # Use a copy
                columns = valid_columns(data_frame)
            else:
                # If multiple files are selected, find common columns
                data_frames = [self.tensor_dict[file].copy() for file in selected_files]
                common_columns = set(valid_columns(data_frames[0]))
                for df in data_frames[1:]:
                    common_columns.intersection_update(valid_columns(df))
                if not common_columns:
                    QApplication.restoreOverrideCursor()
                    QMessageBox.warning(self, "Warning", "No common columns found among the selected CSV files.")
                    return
                columns = list(common_columns)

            common_columns_dialog = QDialog(self)
            common_columns_dialog.setWindowTitle("Select Columns")

            layout = QVBoxLayout(common_columns_dialog)

            common_columns_list = QListWidget()
            common_columns_list.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
            for col in columns:
                common_columns_list.addItem(col)
            layout.addWidget(common_columns_list)

            plot_button = QPushButton("OK")
            plot_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
            plot_button.clicked.connect(common_columns_dialog.accept)  # Close the current dialog
            layout.addWidget(plot_button)

            common_columns_dialog.setLayout(layout)
            QApplication.restoreOverrideCursor()
            if common_columns_dialog.exec() == QDialog.DialogCode.Rejected:
                return  # Exit function if dialog was closed without selection
            QApplication.setOverrideCursor(self.custom_cursor)
            # Get selected feature inputs
            selected_features = [item.text() for item in common_columns_list.selectedItems()]

            # Initialize the tensor with the correct number of features (channels)
            num_features = len(selected_features)
            all_nan_tensor = np.full((*self.tensor_data.shape[:3], num_features), -999.25, dtype=np.float32)

            # Check for corresponding samples in selected columns
            if selected_features:
                # Read all selected CSVs into a combined DataFrame
                combined_df = pd.concat([self.tensor_dict[file].copy() for file in selected_files])

                numeric_selected_features = [feature for feature in selected_features if feature != 'Surface']

                # Convert selected columns to numeric and exclude NaNs and -999.25
                combined_df[numeric_selected_features] = combined_df[numeric_selected_features].apply(pd.to_numeric,
                                                                                                      errors='coerce')
                combined_df = combined_df.replace(-999.25, np.nan)

                # Count rows where all selected columns have valid (non-NaN) values
                valid_rows = combined_df.dropna(subset=numeric_selected_features)
                num_valid_points = len(valid_rows)

                if num_valid_points == 0:
                    QApplication.restoreOverrideCursor()
                    QMessageBox.warning(self, "No Valid Data Points",
                                        "No data points have values for all selected columns.")
                    return

            def find_and_calculate_one_way_time(df, units_dict):
                vp_col = None
                dt_col = None

                # Function to display the CSV viewer and capture user selection
                def show_csv_in_dialog(trace_headers):
                    selected_columns = []  # Initialize the selected columns list here
                    selected_column_name = None  # Initialize the selected column name to None

                    class TableView(QTableView):
                        def __init__(self, parent=None):
                            super(TableView, self).__init__(parent)
                            self.setSelectionMode(QTableView.SelectionMode.MultiSelection)

                            # Set model and connect the selection model after the model is set
                            self.setModel(create_pandas_model(trace_headers))
                            self.selectionModel().selectionChanged.connect(self.selection_changed)

                        def selection_changed(self, selected, deselected):
                            nonlocal selected_columns  # Refer to the selected_columns defined in the outer function
                            nonlocal selected_column_name  # Use the nonlocal keyword to modify the outer variable

                            for index in selected.indexes():
                                if len(selected_columns) == 0:  # If no column has been selected yet
                                    selected_columns.append(index.column())
                                    selected_column_name = trace_headers.columns[index.column()]  # Get the column name

                            for index in deselected.indexes():
                                if index.column() in selected_columns:
                                    selected_columns.remove(index.column())
                                    selected_column_name = None  # Reset the column name if deselected

                    dialog = QDialog(self)
                    dialog.setWindowTitle("Select The One-Way Time column")
                    dialog.resize(800, 600)
                    layout = QVBoxLayout(dialog)

                    table_view = TableView(dialog)
                    layout.addWidget(table_view)

                    dialog.setLayout(layout)
                    QApplication.restoreOverrideCursor()
                    if dialog.exec() == QDialog.DialogCode.Rejected:
                        return
                    QApplication.setOverrideCursor(self.custom_cursor)
                    return selected_column_name  # Return the selected column name

                def create_pandas_model(trace_headers):
                    class PandasModel(QAbstractTableModel):
                        def __init__(self, data):
                            super(PandasModel, self).__init__()
                            self._data = data

                        def rowCount(self, parent=None):
                            return self._data.shape[0]

                        def columnCount(self, parent=None):
                            return self._data.shape[1]

                        def data(self, index, role=Qt.ItemDataRole.DisplayRole):
                            if role == Qt.ItemDataRole.DisplayRole:
                                return str(self._data.iloc[index.row(), index.column()])
                            return None

                        def headerData(self, section, orientation, role=Qt.ItemDataRole.DisplayRole):
                            if role == Qt.ItemDataRole.DisplayRole:
                                if orientation == Qt.Orientation.Horizontal:
                                    return str(self._data.columns[section])
                                if orientation == Qt.Orientation.Vertical:
                                    return str(self._data.index[section])
                            return None

                    return PandasModel(trace_headers)

                # Look for VP columns with various casings
                for col in df.columns:
                    if any(variant in col.lower() for variant in ['vp', 'vpm', 'interval velocity']):
                        vp_col = col
                        break  # Stop once a VP column is found

                # If no VP column, look for DT columns
                if not vp_col:
                    for col in df.columns:
                        if any(variant in col.lower() for variant in ['dt']):
                            dt_col = col
                            break  # Stop once a DT column is found

                if vp_col:
                    # Use VP column, ensure it's in m/s
                    vp_unit = units_dict.get(vp_col, "").lower()
                    if vp_unit and "m" not in vp_unit:
                        QApplication.restoreOverrideCursor()
                        QMessageBox.warning(self, "Warning", "VP unit is not meters per second.")
                    # Replace -999.25 with NaN and interpolate
                    df['Velocity'] = pd.to_numeric(df[vp_col].copy(), errors='coerce').replace(-999.25, np.nan)

                    # Prepare the data for interpolation/extrapolation
                    valid_indices = np.where(~df['Velocity'].isna())[0]
                    valid_values = df['Velocity'].dropna().values

                    # Perform cubic interpolation/extrapolation
                    cubic_interp = interp1d(valid_indices, valid_values, kind='cubic', fill_value="extrapolate")
                    df['Velocity'] = cubic_interp(np.arange(len(df['Velocity'])))

                elif dt_col:
                    # Convert DT to VP
                    dt_unit = units_dict.get(dt_col, "").lower()
                    df['Velocity'] = pd.to_numeric(df[dt_col].copy(), errors='coerce').replace(-999.25, np.nan)

                    # Prepare the data for interpolation/extrapolation
                    valid_indices = np.where(~df['Velocity'].isna())[0]
                    valid_values = df['Velocity'].dropna().values

                    # Perform cubic interpolation/extrapolation
                    cubic_interp = interp1d(valid_indices, valid_values, kind='cubic', fill_value="extrapolate")
                    df['Velocity'] = cubic_interp(np.arange(len(df['Velocity'])))

                    df['Velocity'] = df['Velocity'].apply(lambda dt: get_velocity_from_dt(dt, dt_unit))
                else:
                    QApplication.restoreOverrideCursor()
                    QMessageBox.warning(self, "Warning",
                                        "No suitable VP or DT column found. Please select the One-Way Time column.")

                    df['OneWayTime'] = df[show_csv_in_dialog(df)]

                    return df

                # Calculate one-way time
                df['OneWayTime'] = 0.0

                def identify_depth_column():
                    """Identify the depth column, typically the first column, and return its name."""
                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                    for name in possible_names:
                        if name in df.columns:
                            return name
                    return df.columns[0]  # If no common name matches, assume the first column is depth

                depth = identify_depth_column()

                # Convert the depth column to numeric
                df[depth] = pd.to_numeric(df[depth], errors='coerce')

                # Calculate the one-way time for the first row
                first_depth = df.iloc[0][depth]
                first_velocity = df.iloc[0]['Velocity']
                df.at[0, 'OneWayTime'] = (first_depth / first_velocity) * 1000  # Time from surface to first depth
                cumulative_time = df.at[0, 'OneWayTime']  # Start cumulative time

                # Calculate the one-way time for subsequent rows
                for i in range(1, len(df)):
                    depth_diff = df.iloc[i][depth] - df.iloc[i - 1][depth]
                    velocity = df.iloc[i]['Velocity']
                    incremental_time = (depth_diff / velocity) * 1000  # Convert to milliseconds
                    cumulative_time += incremental_time
                    df.at[i, 'OneWayTime'] = cumulative_time

                return df

            def get_velocity_from_dt(dt, dt_unit):
                if "ft" in dt_unit:
                    return 1 / (dt / 1_000_000) * 0.3048  # Convert microseconds/foot to m/s
                elif "m" in dt_unit:
                    return 1 / (dt / 1_000_000)  # Convert microseconds/meter to m/s
                else:
                    raise ValueError("Unknown DT unit.")

            def check_and_process_one_way_time(df, units_dict):
                one_way_time_col = None

                # Look for One-Way Time columns with various casings and formats.
                # Remember to lowercase anything you put in the list, or else it won't work.
                for col in df.columns:
                    if any(variant in col.lower() for variant in
                           ['onewaytime', 'one-way-time', 'one_way_time', 'owt', 'one-waytime1', 'generaltime1']):
                        one_way_time_col = col
                        break  # Stop once a One-Way Time column is found

                if one_way_time_col:
                    df['OneWayTime'] = pd.to_numeric(df[one_way_time_col].copy(), errors='coerce')
                else:
                    # If no One-Way Time column found, calculate it
                    df = find_and_calculate_one_way_time(df, units_dict)

                return df

            if not selected_features:
                QApplication.restoreOverrideCursor()
                QMessageBox.critical(self, "Error", "Please select at least one feature")
                return

            for selected_key in selected_keys:

                # Get the corresponding CSV DataFrame
                csv_df = self.tensor_dict[selected_key]

                units_dict = self.units_dict[selected_key].copy()

                csv_df = check_and_process_one_way_time(csv_df, units_dict)

                csv_df['OneWayTime'] = csv_df['OneWayTime'].apply(pd.to_numeric, errors='coerce')

                numeric_selected_features = [feature for feature in selected_features if feature != 'Surface']

                csv_df[numeric_selected_features] = csv_df[numeric_selected_features].apply(pd.to_numeric,
                                                                                            errors='coerce')

                # Get deviation data for the selected key
                deviation_data = self.well_deviation_dict.get(selected_key, None)

                if deviation_data:

                    # Convert selected columns to numeric and exclude NaNs and -999.25
                    csv_df['OneWayTime'] = csv_df['OneWayTime'].replace(-999.25, np.nan)

                    # Get indices of valid (non-NaN) values
                    valid_indices = np.where(~csv_df['OneWayTime'].isna())[0]
                    valid_values = csv_df['OneWayTime'].dropna()

                    # Create an interpolation function with linear extrapolation
                    interp_func = interp1d(valid_indices, valid_values, fill_value="extrapolate", kind='cubic')

                    # Apply the interpolation function across the full index range of the DataFrame
                    csv_df['OneWayTime'] = interp_func(np.arange(len(csv_df)))

                    # Multiply the 'One-waytime1' column by 2
                    csv_df['TwoWayTime'] = csv_df['OneWayTime'] * 2

                    self.units_dict[selected_key]['OneWayTime'] = 'ms'

                    self.units_dict[selected_key]['TwoWayTime'] = 'ms'

                    valid_rows = csv_df['TwoWayTime']

                    metadata = self.metadata[self.file_name]

                    # Get the starting time from metadata (adjust key based on actual metadata structure)
                    start_value = metadata.get('time_range', [0, 0, 0])[1]

                    t_start = max(0, -start_value)

                    selected_features_Filtered = selected_features.copy()

                    # Check if 'formations' is in the selected features
                    if 'Surface' in selected_features:
                        # Initialize 'labels' column with placeholder values
                        csv_df['Labels'] = -999.25

                        # Variable to keep track of the label number
                        label_number = 0

                        # Iterate over the DataFrame
                        for i, value in enumerate(csv_df['Surface']):
                            if isinstance(value, str):  # Check if the value is a string
                                label_number += 1  # Increment the label number
                            if label_number > 0:  # Start assigning labels once we find a string
                                csv_df.at[i, 'Labels'] = label_number

                        selected_features_Filtered.append('Labels')

                        selected_features_Filtered.remove('Surface')

                    for col_idx, col_name in enumerate(selected_features_Filtered):
                        for d in range(all_nan_tensor.shape[0]):

                            # Calculate the actual depth for this index
                            current_depth = d * sampling_interval

                            # Get the tensor positions for this depth
                            tensor_positions = self.map_depth_to_tensor_positions(current_depth,
                                                                                        selected_key)

                            if tensor_positions is None:
                                QApplication.restoreOverrideCursor()
                                QMessageBox.critical(self, "Error",
                                                     "Well log coordinates are outside the survey area. Verify the coordinates.")
                                return

                            x_position, y_position = tensor_positions

                            # Calculate the time interval for this depth index
                            target_time = t_start + current_depth
                            next_target_time = t_start + (d + 1) * sampling_interval

                            time_mask = (valid_rows >= target_time) & (valid_rows < next_target_time)
                            rows_in_interval = csv_df[time_mask]

                            if not rows_in_interval.empty:
                                feature_values = rows_in_interval[col_name]
                                non_placeholder_values = feature_values[feature_values != -999.25]

                                if not non_placeholder_values.empty:
                                    # First check if the dtype is already integer - fastest path
                                    if pd.api.types.is_integer_dtype(non_placeholder_values.dtype):
                                        # Definitely use mode for integer types
                                        mean_value = non_placeholder_values.mode()
                                    else:
                                        # For float types, check if all values are effectively integers
                                        # This vectorized operation checks if any value has a non-zero decimal part
                                        has_decimals = (non_placeholder_values % 1).abs().gt(1e-10).any()

                                        if not has_decimals:
                                            # All values are effectively integers (just stored as float)
                                            mean_value = non_placeholder_values.mode()
                                        else:
                                            # True continuous data with decimal values
                                            mean_value = non_placeholder_values.mean()
                                else:
                                    mean_value = -999.25

                                # Handle potential Series result from mode calculation
                                if isinstance(mean_value, pd.Series):
                                    mean_value = mean_value.iloc[0]

                                # Assign the value to the tensor at the calculated position
                                all_nan_tensor[d, x_position, y_position, col_idx] = mean_value

                else:
                    selected_dict_key = selected_key.replace("_", "-")
                    # If deviation data does not exist, use well head data
                    well_head_data = self.well_head_dict.get(selected_dict_key, None)

                    if well_head_data:
                        xtop = well_head_data['Surface X']
                        ytop = well_head_data['Surface Y']
                    else:
                        # If the key isn't found, but the dictionary has entries, show selection dialog
                        if self.well_head_dict:
                            available_wh_keys = list(self.well_head_dict.keys())
                            QApplication.restoreOverrideCursor()
                            selected_wh_item, ok = QInputDialog.getItem(
                                self,
                                "Select Well Head Key",
                                f"Key '{selected_dict_key}' not found. Please select the correct key:",
                                available_wh_keys,
                                0,  # Default to first item
                                False  # Non-editable combo box
                            )
                            QApplication.setOverrideCursor(self.custom_cursor)
                            if ok and selected_wh_item:
                                well_head_data = self.well_head_dict[selected_wh_item]
                                xtop = well_head_data['Surface X']
                                ytop = well_head_data['Surface Y']
                            else:
                                # User cancelled the dialog
                                QApplication.restoreOverrideCursor()
                                raise ValueError(f"Please load well head or deviation file for well: {selected_key}")
                        else:
                            # Dictionary is empty, raise the original error
                            QApplication.restoreOverrideCursor()
                            raise ValueError(f"Please load well head or deviation file for well: {selected_key}")

                    # Map xtop and ytop to tensor position
                    tensor_positions = self.map_coordinates_to_tensor(xtop, ytop)

                    if tensor_positions is None:
                        QApplication.restoreOverrideCursor()
                        QMessageBox.critical(self, "Error",
                                             "Well log coordinates are outside the survey area. Verify the coordinates.")
                        return

                    x_position, y_position = tensor_positions

                    # Convert selected columns to numeric and exclude NaNs and -999.25
                    csv_df['OneWayTime'] = csv_df['OneWayTime'].replace(-999.25, np.nan)

                    # Get indices of valid (non-NaN) values
                    valid_indices = np.where(~csv_df['OneWayTime'].isna())[0]
                    valid_values = csv_df['OneWayTime'].dropna()

                    # Create an interpolation function with linear extrapolation
                    interp_func = interp1d(valid_indices, valid_values, fill_value="extrapolate", kind='cubic')

                    # Apply the interpolation function across the full index range of the DataFrame
                    csv_df['OneWayTime'] = interp_func(np.arange(len(csv_df)))

                    # Multiply the 'One-waytime1' column by 2
                    csv_df['TwoWayTime'] = csv_df['OneWayTime'] * 2

                    self.units_dict[selected_key]['OneWayTime'] = 'ms'

                    self.units_dict[selected_key]['TwoWayTime'] = 'ms'

                    valid_rows = csv_df['TwoWayTime']

                    metadata = self.metadata[self.file_name]

                    # Get the starting time from metadata (adjust key based on actual metadata structure)
                    start_value = metadata.get('time_range', [0, 0, 0])[1]

                    t_start = max(0, -start_value)

                    selected_features_Filtered = selected_features.copy()

                    # Check if 'formations' is in the selected features
                    if 'Surface' in selected_features:
                        # Initialize 'labels' column with placeholder values
                        csv_df['Labels'] = -999.25

                        # Variable to keep track of the label number
                        label_number = 0

                        # Iterate over the DataFrame
                        for i, value in enumerate(csv_df['Surface']):
                            if isinstance(value, str):  # Check if the value is a string
                                label_number += 1  # Increment the label number
                            if label_number > 0:  # Start assigning labels once we find a string
                                csv_df.at[i, 'Labels'] = label_number

                        selected_features_Filtered.append('Labels')

                        selected_features_Filtered.remove('Surface')

                    for col_idx, col_name in enumerate(selected_features_Filtered):

                        for d in range(all_nan_tensor.shape[0]):

                            # Calculate the time interval for this depth index
                            target_time = t_start + d * sampling_interval
                            next_target_time = t_start + (d + 1) * sampling_interval

                            time_mask = (valid_rows >= target_time) & (valid_rows < next_target_time)
                            rows_in_interval = csv_df[time_mask]

                            if not rows_in_interval.empty:
                                feature_values = rows_in_interval[col_name]
                                non_placeholder_values = feature_values[feature_values != -999.25]

                                if not non_placeholder_values.empty:
                                    # First check if the dtype is already integer - fastest path
                                    if pd.api.types.is_integer_dtype(non_placeholder_values.dtype):
                                        # Definitely use mode for integer types
                                        mean_value = non_placeholder_values.mode()
                                    else:
                                        # For float types, check if all values are effectively integers
                                        # This vectorized operation checks if any value has a non-zero decimal part
                                        has_decimals = (non_placeholder_values % 1).abs().gt(1e-10).any()

                                        if not has_decimals:
                                            # All values are effectively integers (just stored as float)
                                            mean_value = non_placeholder_values.mode()
                                        else:
                                            # True continuous data with decimal values
                                            mean_value = non_placeholder_values.mean()
                                else:
                                    mean_value = -999.25

                                # Check if mode_result is a Series (which can happen if there are multiple modes)
                                if isinstance(mean_value, pd.Series):
                                    # Take the first value of the Series, which is the most frequent one
                                    mean_value = mean_value.iloc[0]

                                # Now mean_value is guaranteed to be a single scalar value
                                # This can be safely assigned to a tensor element
                                all_nan_tensor[d, x_position, y_position, col_idx] = mean_value

            self.tensor_data = all_nan_tensor
            selected_features_str = " ".join(selected_features)
            self.metadata[f"{self.file_name}_Upscaled {selected_features_str}"] = self.metadata[self.file_name].copy()
            self.metadata[f"{self.file_name}_Upscaled {selected_features_str}"]['template'] = f"Upscaled {selected_features_str}"
            self.add_tensor(f"{self.file_name}_Upscaled {selected_features_str}", self.tensor_data)
            QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "An error occurred", f"Error: {e}")

    def map_depth_to_tensor_positions(self, depth, selected_key):
        """
        Maps depth to tensor positions using the same coordinate transformation logic as map_coordinates_to_tensor.
        This handles multiple points from the deviation data by first interpolating x,y at the given depth.
        """
        # Get the deviation data DataFrame
        deviation_data = self.well_deviation_dict[selected_key]['Deviation Data']

        # Sort the dataframe by MD if it's not already sorted
        deviation_data = deviation_data.sort_values('MD')

        # Find entries for interpolation
        lower_entry = deviation_data[deviation_data['MD'] <= depth]
        upper_entry = deviation_data[deviation_data['MD'] > depth]

        if lower_entry.empty:
            # If depth is before any data point, use the first entry
            x_val = deviation_data.iloc[0]['X']
            y_val = deviation_data.iloc[0]['Y']
        elif upper_entry.empty:
            # If depth is beyond all data points, use the last entry
            x_val = deviation_data.iloc[-1]['X']
            y_val = deviation_data.iloc[-1]['Y']
        else:
            # Get the entries for interpolation
            lower = lower_entry.iloc[-1]  # Last entry less than or equal to depth
            upper = upper_entry.iloc[0]  # First entry greater than depth

            # Calculate interpolation factor
            factor = (depth - lower['MD']) / (upper['MD'] - lower['MD']) if upper['MD'] != lower['MD'] else 0

            # Linearly interpolate X and Y values
            x_val = lower['X'] + factor * (upper['X'] - lower['X'])
            y_val = lower['Y'] + factor * (upper['Y'] - lower['Y'])

        # Now use the same mapping logic as map_coordinates_to_tensor for the interpolated point
        # Extract the min and max values from the filename
        metadata = self.metadata[self.file_name]

        # Extract tensor dimensions
        n_inlines = self.tensor_data.shape[1]
        n_xlines = self.tensor_data.shape[2]

        # Extract the three corners
        x1, y1 = metadata['origin']  # (0,0) in normalized coords
        x2, y2 = metadata['xline_end']  # (0,1) in normalized coords
        x3, y3 = metadata['inline_end']  # (1,0) in normalized coords

        # Calculate the fourth corner by vector addition
        # Last corner = Origin + (End of inline - Origin) + (End of crossline - Origin)
        x4 = x1 + (x3 - x1) + (x2 - x1)
        y4 = y1 + (y3 - y1) + (y2 - y1)

        # Create arrays of known points and their corresponding tensor indices
        # We know the corners map to the corners of the tensor
        xy_points = np.array([
            [x1, y1],  # Maps to (0, 0)
            [x2, y2],  # Maps to (0, n_xlines-1)
            [x3, y3],  # Maps to (n_inlines-1, 0)
            [x4, y4]  # Maps to (n_inlines-1, n_xlines-1)
        ])

        tensor_indices = np.array([
            [0, 0],
            [0, n_xlines - 1],
            [n_inlines - 1, 0],
            [n_inlines - 1, n_xlines - 1]
        ])

        # Check if point is inside the quadrilateral using Delaunay triangulation
        tri = Delaunay(xy_points)
        point_inside = tri.find_simplex(np.array([[x_val, y_val]])) >= 0

        if not point_inside:
            return None

        # Use griddata for the inverse mapping
        # This will interpolate tensor indices at our query point
        interpolated_indices = griddata(
            xy_points,  # Known physical coordinates
            tensor_indices,  # Known tensor indices
            np.array([[x_val, y_val]]),  # Query point
            method='linear'  # Linear interpolation (effectively bilinear for our quad)
        )

        # Extract the interpolated indices
        inline_idx, xline_idx = interpolated_indices[0]

        # Round to nearest integer and ensure within bounds
        inline_idx = int(np.round(inline_idx))
        xline_idx = int(np.round(xline_idx))

        # Bounds checking
        if not (0 <= inline_idx < n_inlines and 0 <= xline_idx < n_xlines):
            return None

        return inline_idx, xline_idx

    def map_coordinates_to_tensor(self, x, y):
        """
        Maps physical (x, y) coordinates to tensor indices using only three corner points.
        """

        # Extract the min and max values from the filename
        metadata = self.metadata[self.file_name]

        # Extract tensor dimensions
        n_inlines = self.tensor_data.shape[1]
        n_xlines = self.tensor_data.shape[2]

        # Extract the three corners
        x1, y1 = metadata['origin']  # (0,0) in normalized coords
        x2, y2 = metadata['xline_end']  # (0,1) in normalized coords
        x3, y3 = metadata['inline_end']  # (1,0) in normalized coords

        # Calculate the fourth corner by vector addition
        # Last corner = Origin + (End of inline - Origin) + (End of crossline - Origin)
        x4 = x1 + (x3 - x1) + (x2 - x1)
        y4 = y1 + (y3 - y1) + (y2 - y1)

        # Create arrays of known points and their corresponding tensor indices
        # We know the corners map to the corners of the tensor
        xy_points = np.array([
            [x1, y1],  # Maps to (0, 0)
            [x2, y2],  # Maps to (0, n_xlines-1)
            [x3, y3],  # Maps to (n_inlines-1, 0)
            [x4, y4]  # Maps to (n_inlines-1, n_xlines-1)
        ])

        tensor_indices = np.array([
            [0, 0],
            [0, n_xlines - 1],
            [n_inlines - 1, 0],
            [n_inlines - 1, n_xlines - 1]
        ])

        # Check if point is inside the quadrilateral using Delaunay triangulation
        tri = Delaunay(xy_points)
        point_inside = tri.find_simplex(np.array([[x, y]])) >= 0

        if not point_inside:
            return None

        # Use griddata for the inverse mapping
        # This will interpolate tensor indices at our query point
        interpolated_indices = griddata(
            xy_points,  # Known physical coordinates
            tensor_indices,  # Known tensor indices
            np.array([[x, y]]),  # Query point
            method='linear'  # Linear interpolation (effectively bilinear for our quad)
        )

        # Extract the interpolated indices
        inline_idx, xline_idx = interpolated_indices[0]

        # Round to nearest integer and ensure within bounds
        inline_idx = int(np.round(inline_idx))
        xline_idx = int(np.round(xline_idx))

        # Bounds checking
        if not (0 <= inline_idx < n_inlines and 0 <= xline_idx < n_xlines):
            return None

        return inline_idx, xline_idx

    def process_and_predict_petro(self):
        try:

            def add_new_model_to_dict():
                """Add a new model with metadata to the dictionary."""

                # Step 1: Browse for the model file
                model_file_path, _ = QFileDialog.getOpenFileName(self, 'Select Model File', '',
                                                                 'Model Files (*.pkl *.h5 *.pth);;All Files (*)')
                if not model_file_path:
                    return  # Cancel if no file selected

                # Step 2: Ask for model type, including an "Else" option
                if model_file_path.endswith(".pkl"):
                    model_type_name = "Machine Learning"
                elif model_file_path.endswith(".h5"):
                    model_type_name = "TF Neural Network"
                elif model_file_path.endswith(".pth"):
                    model_type_name = "torch Neural Network"
                else:
                    QMessageBox.critical(self, "Error", f"Unsupported model format")
                    return  # Cancel if no input

                # Step 4: Determine whether to load the model
                if model_file_path.endswith(".pkl"):
                    try:
                        model = joblib.load(model_file_path)
                    except Exception as e:
                        QMessageBox.critical(self, "Error", f"Failed to load model: {str(e)}")
                        return
                else:
                    model = None  # Neural network models remain uninitialized

                # Step 5: Get features
                # Get children of the selected parent key
                child_keys = []
                parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                    selected_parent_key].get('source')
                for key, value in self.metadata.items():
                    name_or_source = value.get('name') or value.get('source')
                    template = value.get('template').strip()
                    if name_or_source == parent_name_or_source and template != "Seismic" and not template.startswith(
                            'Upscaled'):
                        child_keys.append(key)

                # Display child keys of the selected parent key for feature input selection in a dialog
                feature_input_list_widget_dialog = QDialog(self)
                feature_input_list_widget_dialog.setWindowTitle("Select Feature Inputs")

                layout = QVBoxLayout(feature_input_list_widget_dialog)
                feature_input_list_widget = QListWidget()
                feature_input_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
                for key in child_keys:
                    child_name = self.metadata[key].get('template').strip()
                    feature_input_list_widget.addItem(child_name)
                layout.addWidget(feature_input_list_widget)

                ok_button = QPushButton("OK")
                ok_button.clicked.connect(feature_input_list_widget_dialog.accept)
                layout.addWidget(ok_button)

                feature_input_list_widget_dialog.setLayout(layout)

                if child_keys:
                    if feature_input_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                        return

                chosen_feature_input_keys = [selected_parent_key]

                selected_child_input_keys = []
                for item in feature_input_list_widget.selectedItems():
                    for key in child_keys:
                        if self.metadata[key].get('template').strip() == item.text():
                            selected_child_input_keys.append(key)

                chosen_feature_input_keys.extend(selected_child_input_keys)
                features = chosen_feature_input_keys

                # Step 6: Get target variable
                target, ok = QInputDialog.getText(
                    self, "Target Variable", "Enter the target variable:"
                )
                if not ok or not target.strip():
                    return
                target = target.strip()

                # Step 7: Ask for dimensionality reduction type
                dim_types = ["None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"]
                dim_type, ok = QInputDialog.getItem(
                    self, "Dimensionality Reduction", "Select type:", dim_types, editable=False
                )
                if not ok:
                    return
                dim_type = dim_type if dim_type != "None" else None

                # Step 8: If dimensionality reduction is selected, ask for number of components
                n_components = None
                if dim_type:
                    n_components, ok = QInputDialog.getInt(
                        self, "Number of Components",
                        f"Enter the number of components (max {len(features)}):",
                        min=1, max=len(features)
                    )
                    if not ok:
                        return

                # Step 9: Add model to the dictionary
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": model_file_path,
                    "dim_type": dim_type,
                    "n_components": n_components,
                }
                combined_key = f"{model_type_name}_{dim_type if dim_type else 'None'}"

                if combined_key not in self.petro_models:
                    self.petro_models[combined_key] = []

                self.petro_models[combined_key].append(model_metadata)

                QMessageBox.information(self, "Success", "Model successfully added to the dictionary!")

            def select_feature_inputs(selected_parent_key):
                # Get children of the selected parent key
                child_keys = []
                parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                    selected_parent_key].get('source')
                for key, value in self.metadata.items():
                    name_or_source = value.get('name') or value.get('source')
                    template = value.get('template').strip()
                    if name_or_source == parent_name_or_source and template != "Seismic" and not template.startswith(
                            'Upscaled'):
                        child_keys.append(key)

                # Display child keys of the selected parent key for feature input selection in a dialog
                feature_input_list_widget_dialog = QDialog(self)
                feature_input_list_widget_dialog.setWindowTitle("Select Feature Inputs")

                layout = QVBoxLayout(feature_input_list_widget_dialog)
                feature_input_list_widget = QListWidget()
                feature_input_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
                for key in child_keys:
                    child_name = self.metadata[key].get('template').strip()
                    feature_input_list_widget.addItem(child_name)
                layout.addWidget(feature_input_list_widget)

                ok_button = QPushButton("OK")
                ok_button.clicked.connect(feature_input_list_widget_dialog.accept)
                layout.addWidget(ok_button)

                feature_input_list_widget_dialog.setLayout(layout)

                if child_keys:
                    if feature_input_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                        return

                chosen_feature_input_keys = [selected_parent_key]

                selected_child_input_keys = []
                for item in feature_input_list_widget.selectedItems():
                    for key in child_keys:
                        if self.metadata[key].get('template').strip() == item.text():
                            selected_child_input_keys.append(key)

                chosen_feature_input_keys.extend(selected_child_input_keys)

                return chosen_feature_input_keys

            # Step 1: Extract keys from self.tensor_dict that correspond to NumPy tensors
            tensor_keys = [key for key, value in self.tensor_dict.items() if isinstance(value, np.ndarray)]

            # Step 2: Get parent keys from the QTreeWidget
            parent_keys = []
            root = self.treeWidget.invisibleRootItem()
            child_count = root.childCount()

            for i in range(child_count):
                parent_item = root.child(i)
                parent_key = parent_item.text(0)
                if parent_key in tensor_keys:
                    parent_keys.append(parent_key)

            # Display parent keys for selection in a dialog
            parent_list_widget_dialog = QDialog(self)
            parent_list_widget_dialog.setWindowTitle("Select Parent")

            layout = QVBoxLayout(parent_list_widget_dialog)
            parent_list_widget = QListWidget()
            parent_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for key in parent_keys:
                parent_list_widget.addItem(key)
            layout.addWidget(parent_list_widget)

            ok_button = QPushButton("OK")
            ok_button.clicked.connect(parent_list_widget_dialog.accept)
            layout.addWidget(ok_button)

            add_new_model_to_dict_button = QPushButton("Import Model")
            add_new_model_to_dict_button.clicked.connect(add_new_model_to_dict)
            layout.addWidget(add_new_model_to_dict_button)

            parent_list_widget_dialog.setLayout(layout)
            if parent_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_parent_key = parent_list_widget.currentItem().text()
            selected_feature_input_keys = select_feature_inputs(selected_parent_key)

            # Step 3: Find Target Variables Based on Selected Features in the Model Dictionary
            potential_target_variables = set()
            for model_type, models in self.petro_models.items():
                for model_info in models:
                    if set(selected_feature_input_keys) == set(model_info['features']):
                        potential_target_variables.add(tuple(model_info['target']))  # Convert list to tuple

            if not potential_target_variables:
                QMessageBox.warning(self, "No Matching Models",
                                    "No models are available for the selected feature inputs.")
                return

            # Step 5: Model Selection Based on Features
            available_models = []
            for model_type, models in self.petro_models.items():
                for model_info in models:
                    if set(selected_feature_input_keys) == set(model_info['features']):
                        if isinstance(model_info['target'], str):
                            # Keep it as a string if it's already a string
                            target_variable = model_info['target']
                        else:
                            # Convert to tuple only if it's not a string
                            target_variable = tuple(model_info['target'])

                        available_models.append((model_type, model_info, target_variable))

            if not available_models:
                QMessageBox.warning(self, "No Matching Model",
                                    "No trained models available for the selected features.")
                return

            model_selection_dialog = QDialog(self)
            model_selection_dialog.setWindowTitle("Select Model for Prediction")
            layout = QVBoxLayout(model_selection_dialog)

            model_list_widget = QListWidget()
            model_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for model_type, model_info, target_variable in available_models:
                # Check if target_variable is a list
                if isinstance(target_variable, str):
                    target = target_variable
                else:
                    target = ' '.join(target_variable)
                # Add both model type and target variable to the list
                model_list_widget.addItem(f"{model_type} model (Target: {target})")

            layout.addWidget(model_list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(model_selection_dialog.accept)
            layout.addWidget(select_button)

            model_selection_dialog.setLayout(layout)
            if model_selection_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_model_items = model_list_widget.selectedItems()
            if len(selected_model_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select a model.")
                return

            # Extract the selected model information
            selected_model_index = model_list_widget.currentRow()
            selected_model_type, selected_model_info, selected_target_variable = available_models[selected_model_index]

            # Get the model object
            model = selected_model_info['model']
            file_path = selected_model_info['file_path']
            targets = selected_model_info['target']
            dimensionality_reduction_type_combo = selected_model_info['dim_type']
            n_components = selected_model_info['n_components']

            # Step 6: Data Preprocessing
            feature_inputs = []

            # Collect all tensors for the selected keys
            for key in selected_feature_input_keys:
                feature_inputs.append(np.copy(self.tensor_dict[key]))

            # Check if the list is empty
            if not feature_inputs:
                QMessageBox.warning(self, "Warning", "No tensors available for the selected feature inputs.")
                return

            # Extract the shape of one tensor as a reference (excluding the channel dimension)
            reference_shape = feature_inputs[0].shape[:3]  # Get the Depth, Height, Width

            # Flatten each tensor
            flattened_feature_inputs = [tensor.flatten() for tensor in feature_inputs]

            # Stack flattened tensors into a 2D array
            X = np.stack(flattened_feature_inputs, axis=1)

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Standard Scaler", "Min-Max Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Rejected:
                return

            normalization_choice = combo.currentText()

            selected_model_type = selected_model_type.split('_')[0]

            if selected_model_type in ["ResNet", "TCN", "Keras Regressor", "Hyper Parameter", "TabNet",
                                       "Keras Classifier", "TF Neural Network", "torch Neural Network"]:
                model = None
                if selected_model_type in ["ResNet", "TCN", "TabNet", "torch Neural Network"]:
                    # Set up the QInputDialog
                    dialog = QInputDialog(self)
                    dialog.setWindowTitle("Batch Size Input")
                    dialog.setLabelText("Please enter the batch size:")
                    dialog.setIntRange(1, 100000)  # Set the range of acceptable values
                    if selected_model_type in ["TCN", "TabNet"]:
                        dialog.setIntValue(100000)  # Set the initial value
                    else:
                        dialog.setIntValue(10000)  # Set the initial value
                    # Execute the dialog and get the batch size
                    if dialog.exec() == QInputDialog.DialogCode.Accepted:
                        batch_size = dialog.intValue()
                    else:
                        return
                else:
                    batch_size = None
            else:
                batch_size = None

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir = os.path.join("logs", "Tensorflow", "autoencoder_DR_predict_petro",
                                       datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir = None

            y_pred = self.task_runner.run_task(TensorVisualizer.run_petro_inference_neural_net, X,
                                               selected_model_type, file_path, batch_size, model,
                                               dimensionality_reduction_type_combo, n_components, normalization_choice,
                                               log_dir)

            if y_pred is not None:
                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir)

                # Determine the number of channels
                if y_pred.ndim == 1:
                    num_channels = 1
                else:
                    num_channels = y_pred.shape[1]

                # Calculate the total number of elements in the reference shape
                total_elements = np.prod(reference_shape)

                # Ensure the number of samples in y_pred matches the total elements in the reference shape
                if y_pred.shape[0] != total_elements:
                    raise ValueError(
                        "The number of samples in y_pred does not match the total elements in the reference shape.")

                # Reshape y_pred to the desired shape
                new_shape = (*reference_shape, num_channels)
                reshaped_y_pred = y_pred.reshape(new_shape)

                self.tensor_data = reshaped_y_pred
                # Store the modified tensor with an informative key
                if isinstance(targets, str):
                    target_items = targets
                else:
                    target_items = ' '.join(targets)
                self.metadata[f"{selected_parent_key}_Predicted {target_items} {selected_model_type}"] = self.metadata[
                    selected_parent_key].copy()
                self.metadata[f"{selected_parent_key}_Predicted {target_items} {selected_model_type}"][
                    'template'] = f"Predicted {target_items} {selected_model_type}"
                self.add_tensor(f"{selected_parent_key}_Predicted {target_items} {selected_model_type}", self.tensor_data)
                QMessageBox.information(self, "Prediction Complete",
                                        "The prediction process has finished successfully.")

        except Exception as e:
            traceback.print_exc()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def run_petro_inference_neural_net(X, selected_model_type, file_path, batch_size, model,
                                       dimensionality_reduction_type_combo, n_components, normalization_choice,
                                       log_dir):

        # Set up logging
        TensorVisualizer.setup_logging('Petro_Inference')

        def load_model_by_type(file_path, selected_model_type):
            """
            Loads the model from the specified file path based on its type.

            :param file_path: Path to the saved model file.
            :param selected_model_type: Type of the model (e.g., "Keras", "PyTorch", "Sklearn").
            :return: The loaded model object.
            """
            if selected_model_type in ["Keras Regressor", "Hyper Parameter", "Keras Classifier",
                                       "TF Neural Network"]:
                # Load the Keras model
                return load_model(file_path, compile=False)
            elif selected_model_type in ["ResNet", "TCN", "TabNet", "torch Neural Network"]:
                # Load the whole PyTorch model
                return torch.load(file_path)

        dtype = X.dtype

        if normalization_choice == "Min-Max Scaler":
            scaler = MinMaxScaler()
            X_clean = scaler.fit_transform(X)
        elif normalization_choice == "Standard Scaler":
            scaler = StandardScaler()
            X_clean = scaler.fit_transform(X)
        else:
            X_clean = X

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X_clean)

            X_clean = pca.transform(X_clean)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X_clean = tsne.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X_clean)
            X_clean = ica.transform(X_clean)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X_clean = grp.fit_transform(X_clean)
        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X_clean.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X_clean, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val), callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X_clean = encoder.predict(X_clean)

        else:
            pass

        # Predict using the fitted model
        if selected_model_type in ["Keras Regressor", "Keras Classifier"]:
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)
            y_pred = model.predict(X_clean)

        elif selected_model_type in ["Hyper Parameter", "TF Neural Network"]:
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)
            if X_clean.shape[1] > 1:
                X_clean = np.reshape(X_clean, (-1, 1, 3))

            y_pred = model.predict(X_clean)

        elif selected_model_type == "TabNet":
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)

            y_pred_list = []
            for i in range(0, len(X_clean), batch_size):
                batch_X = X_clean[i:i + batch_size]
                y_pred_batch = model.predict(batch_X)
                y_pred_list.append(y_pred_batch)

            # Concatenate all batch predictions
            y_pred = np.concatenate(y_pred_list, axis=0)

        elif selected_model_type in ["TCN", "ResNet", "torch Neural Network"]:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            # Load and prepare your model
            model = load_model_by_type(file_path, selected_model_type)
            model.to(device)
            model.eval()

            # Convert X_clean from a NumPy array to a PyTorch tensor
            X_val_tensor = torch.tensor(X_clean.reshape(X_clean.shape[0], 1, -1), dtype=torch.float32)

            # Create a DataLoader to handle batches of data
            val_dataset = TensorDataset(X_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            y_pred_list = []

            # Disable gradient calculation for inference
            with torch.no_grad():
                for batch in val_loader:
                    batch_X = batch[0].to(device)
                    y_pred_batch = model(batch_X).cpu().numpy()
                    y_pred_list.append(y_pred_batch)

            # Concatenate all batch predictions
            y_pred = np.concatenate(y_pred_list, axis=0)
        else:
            y_pred = model.predict(X_clean)

        """If y_pred has more than one channel, apply argmax along the last axis."""
        if y_pred.ndim > 1 and y_pred.shape[-1] > 1:
            y_pred = np.argmax(y_pred, axis=-1, keepdims=True)

        return y_pred.astype(dtype)

    @staticmethod
    def run_petro_categorization_task(X, y, train_test, num_lstm_layers_min, regression_type_combo, epochs,
                                      max_trials, num_lstm_layers_max, num_lstm_layers_step, lstm_units_min,
                                      lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                      l2_lstm_step, min_dropout_lstm, max_dropout_lstm, dropout_lstm_layer_step,
                                      dropout_lstm_min, dropout_lstm_max, dropout_lstm_step, num_conv_layers_min,
                                      num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                      conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                      conv_kernel_size_max, conv_kernel_size_step, l2_conv_min, l2_conv_max,
                                      l2_conv_step, min_dropout_conv, max_dropout_conv, dropout_conv_layer_step,
                                      dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                      num_attention_layers_min, num_attention_layers_max, num_attention_layers_step,
                                      attention_heads_min, attention_heads_max, attention_heads_step,
                                      attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                                      l2_attention_min, l2_attention_max, l2_attention_step,
                                      min_dropout_attention, max_dropout_attention, dropout_attention_layer_step,
                                      dropout_attention_min, dropout_attention_max, dropout_attention_step,
                                      num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                                      dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                      l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                                      dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                      dropout_dense_step, learning_rate_min, learning_rate_max,
                                      learning_rate_step, log_dir, batch_size, file_path, output_dim,
                                      dimensionality_reduction_type_combo, n_components, log_dir_dr):

        # Set up logging
        TensorVisualizer.setup_logging('Petro_Classification')

        dtype = y.dtype

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X)

            X = pca.transform(X)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X = tsne.fit_transform(X)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X)
            X = ica.transform(X)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X = grp.fit_transform(X)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Set up TensorBoard callback
            tensorboard_callback = TensorBoard(log_dir=log_dir_dr, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val),
                            callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X = encoder.predict(X)

        # Split into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=train_test / 100, random_state=42)

        class MetricsCallback(Callback):
            def __init__(self):
                super(MetricsCallback, self).__init__()
                self.best_accuracy = 0
                self.best_top_k = 0
                self.best_crossentropy = float('inf')  # Initialize as infinity because lower is better

            def on_epoch_end(self, epoch, logs=None):
                if logs is not None:
                    # Update best accuracy
                    current_accuracy = logs.get('sparse_categorical_accuracy')
                    if current_accuracy is not None and current_accuracy > self.best_accuracy:
                        self.best_accuracy = current_accuracy

                    # Update best top-k accuracy
                    current_top_k_accuracy = logs.get('sparse_top_k_categorical_accuracy')
                    if current_top_k_accuracy is not None and current_top_k_accuracy > self.best_top_k:
                        self.best_top_k = current_top_k_accuracy

                    # Update best cross-entropy loss (lower is better)
                    current_crossentropy = logs.get('sparse_categorical_crossentropy')
                    if current_crossentropy is not None and current_crossentropy < self.best_crossentropy:
                        self.best_crossentropy = current_crossentropy

                    # Store metrics in model
                    self.model.best_accuracy = self.best_accuracy
                    self.model.best_top_k = self.best_top_k
                    self.model.best_crossentropy = self.best_crossentropy

        metrics_callback = MetricsCallback()

        # Function to create the Keras model for multi-class classification
        def create_keras_model():
            inputs = Input(shape=(X_train.shape[1],))
            x = Dense(128, activation='relu')(inputs)
            x = Dense(64, activation='relu')(x)
            x = Dense(32, activation='relu')(x)

            # Output layer
            num_classes = len(np.unique(y_train))  # Assuming y_train is integer encoded
            # Modify your output layer like this:
            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

            # Define a custom clipping function
            def clip_output(x):
                epsilon = 1e-7
                return tf.clip_by_value(x, epsilon, 1 - epsilon)

            # Apply a custom Lambda layer using the function instead of a lambda
            outputs = tf.keras.layers.Lambda(clip_output)(outputs)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=0.001, clipnorm=1.0
                ),
                loss='sparse_categorical_crossentropy',
                metrics=[
                    'sparse_categorical_accuracy',
                    'sparse_top_k_categorical_accuracy',
                    'sparse_categorical_crossentropy'
                ]
            )
            return model

        # Define the hypermodel function using the Functional API
        def build_model(hp):
            input_shape = (X_train.shape[1], X_train.shape[2])
            inputs = tf.keras.Input(shape=input_shape)
            x = inputs

            # Add LSTM layers with Dropout and L2 regularization
            num_lstm_layers = hp.Int('num_lstm_layers', min_value=num_lstm_layers_min,
                                     max_value=num_lstm_layers_max,
                                     step=num_lstm_layers_step)

            for i in range(num_lstm_layers):
                x = tf.keras.layers.LSTM(
                    units=hp.Int(f'lstm_units_{i}', min_value=lstm_units_min, max_value=lstm_units_max,
                                 step=lstm_units_step),
                    return_sequences=True,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_lstm_{i}', min_value=l2_lstm_min, max_value=l2_lstm_max, step=l2_lstm_step)
                    )
                )(x)

                num_dropout_lstm = hp.Int(f'num_dropout_lstm_{i}', min_value=min_dropout_lstm,
                                          max_value=max_dropout_lstm, step=dropout_lstm_layer_step)
                for _ in range(num_dropout_lstm):
                    dropout_rate = hp.Float(f'dropout_rate_lstm_{i}', min_value=dropout_lstm_min,
                                            max_value=dropout_lstm_max, step=dropout_lstm_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add Conv1D layers with Dropout and L2 regularization
            num_conv_layers = hp.Int('num_conv_layers', min_value=num_conv_layers_min,
                                     max_value=num_conv_layers_max,
                                     step=num_conv_layers_step)

            for i in range(num_conv_layers):
                x = tf.keras.layers.Conv1D(
                    filters=hp.Int(f'conv_filters_{i}', min_value=conv_filters_min, max_value=conv_filters_max,
                                   step=conv_filters_step),
                    kernel_size=hp.Int(f'conv_kernel_size_{i}', min_value=conv_kernel_size_min,
                                       max_value=conv_kernel_size_max, step=conv_kernel_size_step),
                    activation='relu',
                    padding='same',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_conv_{i}', min_value=l2_conv_min, max_value=l2_conv_max, step=l2_conv_step)
                    )
                )(x)

                num_dropout_conv = hp.Int(f'num_dropout_conv_{i}', min_value=min_dropout_conv,
                                          max_value=max_dropout_conv, step=dropout_conv_layer_step)
                for _ in range(num_dropout_conv):
                    dropout_rate = hp.Float(f'dropout_rate_conv_{i}', min_value=dropout_conv_min,
                                            max_value=dropout_conv_max, step=dropout_conv_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add MultiHeadAttention layers with Dropout and L2 regularization
            num_attention_layers = hp.Int('num_attention_layers', min_value=num_attention_layers_min,
                                          max_value=num_attention_layers_max, step=num_attention_layers_step)

            for i in range(num_attention_layers):
                num_heads = hp.Int(f'num_attention_heads_{i}', min_value=attention_heads_min,
                                   max_value=attention_heads_max, step=attention_heads_step)
                key_dim = hp.Int(f'attention_key_dim_{i}', min_value=attention_key_dim_min,
                                 max_value=attention_key_dim_max, step=attention_key_dim_step)

                attention_layer = tf.keras.layers.MultiHeadAttention(
                    num_heads=num_heads,
                    key_dim=key_dim,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_attention_{i}', min_value=l2_attention_min, max_value=l2_attention_max,
                                 step=l2_attention_step)
                    )
                )
                attention_output = attention_layer(query=x, value=x)
                x = tf.keras.layers.Add()([x, attention_output])  # Residual connection

                num_dropout_attention = hp.Int(f'num_dropout_attention_{i}', min_value=min_dropout_attention,
                                               max_value=max_dropout_attention, step=dropout_attention_layer_step)
                for _ in range(num_dropout_attention):
                    dropout_rate = hp.Float(f'dropout_rate_attention_{i}', min_value=dropout_attention_min,
                                            max_value=dropout_attention_max, step=dropout_attention_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Flatten the output for Dense layers
            x = tf.keras.layers.Flatten()(x)

            # Add Dense layers with Dropout and L2 regularization
            num_dense_layers = hp.Int('num_dense_layers', min_value=num_dense_layers_min,
                                      max_value=num_dense_layers_max, step=num_dense_layers_step)

            for i in range(num_dense_layers):
                x = tf.keras.layers.Dense(
                    units=hp.Int(f'dense_units_{i}', min_value=dense_units_min, max_value=dense_units_max,
                                 step=dense_units_step),
                    activation='relu',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_dense_{i}', min_value=l2_dense_min, max_value=l2_dense_max,
                                 step=l2_dense_step)
                    )
                )(x)

                num_dropout_dense = hp.Int(f'num_dropout_dense_{i}', min_value=min_dropout_dense,
                                           max_value=max_dropout_dense, step=dropout_dense_layer_step)
                for _ in range(num_dropout_dense):
                    dropout_rate = hp.Float(f'dropout_rate_dense_{i}', min_value=dropout_dense_min,
                                            max_value=dropout_dense_max, step=dropout_dense_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Output layer
            num_classes = len(np.unique(y_train))  # Assuming y_train is integer encoded
            # Modify your output layer like this:
            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

            # Define a custom clipping function
            def clip_output(x):
                epsilon = 1e-7
                return tf.clip_by_value(x, epsilon, 1 - epsilon)

            # Apply a custom Lambda layer using the function instead of a lambda
            outputs = tf.keras.layers.Lambda(clip_output)(outputs)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=hp.Float('learning_rate', min_value=learning_rate_min,
                                           max_value=learning_rate_max,
                                           step=learning_rate_step), clipnorm=1.0
                ),
                loss='sparse_categorical_crossentropy',
                metrics=[
                    'sparse_categorical_accuracy',
                    'sparse_top_k_categorical_accuracy',
                    'sparse_categorical_crossentropy'
                ]
            )

            return model

        # Initialize the model based on the classification type
        if regression_type_combo == "Random Forest Classifier":
            model = RandomForestClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Gradient Boosting Classifier":
            model = GradientBoostingClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Support Vector Classifier":
            model = SVC()

        elif regression_type_combo == "K-Nearest Neighbors Classifier":
            model = KNeighborsClassifier()

        elif regression_type_combo == "Decision Tree Classifier":
            model = DecisionTreeClassifier()

        elif regression_type_combo == "Gaussian Process Classifier":
            model = GaussianProcessClassifier()

        elif regression_type_combo == "CatBoost Classifier":
            model = CatBoostClassifier(iterations=epochs)

        elif regression_type_combo == "LightGBM Classifier":
            model = LGBMClassifier(n_estimators=100 * epochs)

        elif regression_type_combo == "Naive Bayes Classifier":
            model = GaussianNB()

        elif regression_type_combo == "Extra Trees Classifier":
            model = ExtraTreesClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Bagging Classifier":
            model = BaggingClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Voting Classifier":
            model = VotingClassifier(estimators=[
                ('rf', RandomForestClassifier(n_estimators=epochs)),
                ('gb', GradientBoostingClassifier(n_estimators=epochs)),
                ('svc', SVC(probability=True))
            ], voting='soft')

        elif regression_type_combo == "Keras Classifier":
            # Determine output_dim based on the shape of y_train
            output_dim = len(np.unique(y_train))

            # Check for NaN values
            assert not np.isnan(X_train).any(), 'X_train contains NaN values'
            assert not np.isnan(y_train).any(), 'Y_train contains NaN values'

            # Check for infinite values
            assert not np.isinf(X_train).any(), 'X_train contains infinite values'
            assert not np.isinf(y_train).any(), 'Y_train contains infinite values'

            # Create Keras Regressor model
            model = create_keras_model()

        elif regression_type_combo == "ResNet":
            model = ResNet1D_Model(input_dim=X_train.shape[1], output_dim=output_dim)

        elif regression_type_combo == "TabNet":
            # Create TabNet Regressor model
            model = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.001))

        elif regression_type_combo == "TCN":
            model = TCN_Model(num_inputs=1, num_channels=[25, 50, 100], num_outputs=output_dim)

        elif regression_type_combo == "Hyper Parameter":

            X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
            X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

            # Create a temporary directory for the tuner
            with tempfile.TemporaryDirectory() as temp_dir:
                tuner = BayesianOptimization(
                    hypermodel=build_model,
                    objective='val_sparse_categorical_accuracy',
                    max_trials=max_trials,
                    directory=temp_dir,  # Use the temporary directory
                    project_name='Hyper_Parameter_Tuning'
                )

            # Perform the search
            tuner.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=batch_size)

            # Get the best model
            model = tuner.get_best_models(num_models=1)[0]

        # Assuming y_train and y_val are numpy arrays with labels starting from 1 to 10
        # Convert to tensors with appropriate shapes and types
        X_train_tensor = torch.tensor(X_train.reshape(X_train.shape[0], 1, -1), dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train,
                                      dtype=torch.long).squeeze()  # Ensure zero-based indexing and 1D shape

        X_val_tensor = torch.tensor(X_val.reshape(X_val.shape[0], 1, -1), dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val, dtype=torch.long).squeeze()  # Ensure zero-based indexing and 1D shape

        # Create datasets
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

        # DataLoader and model training setup
        train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)

        if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
            if regression_type_combo == "Keras Classifier":
                # Early stopping to prevent overfitting
                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

                # Set up TensorBoard callback
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                # Fit the model
                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32,
                          callbacks=[early_stopping, metrics_callback, tensorboard_callback])

                # Make predictions
                y_pred = model.predict(X_val)

                # Reshape y_pred if output_dim is 1
                if output_dim == 1:
                    y_pred = y_pred.reshape(-1, 1)

                # Access the best accuracy recorded during training
                best_accuracy = metrics_callback.best_accuracy
                best_top_k = metrics_callback.best_top_k
                best_crossentropy = metrics_callback.best_crossentropy
            elif regression_type_combo == "Hyper Parameter":
                # Early stopping to prevent overfitting
                early_stopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=10,
                                               restore_best_weights=True)

                # Set up TensorBoard callback
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                # Fit the model
                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs,
                          callbacks=[early_stopping, metrics_callback, tensorboard_callback])
                y_pred = model.predict(X_val)

                # Access the best accuracy recorded during training
                best_accuracy = metrics_callback.best_accuracy
                best_top_k = metrics_callback.best_top_k
                best_crossentropy = metrics_callback.best_crossentropy

            elif regression_type_combo == "TabNet":
                y_train = y_train.ravel()  # or y_train.reshape(-1)
                y_val = y_val.ravel()  # or y_val.reshape(-1)

                # Train the model
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric=['accuracy'],  # Use accuracy or other classification metrics
                    max_epochs=epochs,
                    patience=10,
                    batch_size=32,
                    virtual_batch_size=16,
                )

                # Make predictions
                y_pred = model.predict(X_val)

            elif regression_type_combo in ["TCN", "ResNet"]:
                # Assuming train_loader and val_loader are DataLoader instances
                # Model, criterion, optimizer, and scaler are defined in your code
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                model.to(device)

                criterion = nn.CrossEntropyLoss()
                optimizer = optim.Adam(model.parameters(), lr=0.001)
                scaler = GradScaler()

                # TensorBoard setup
                writer = SummaryWriter(log_dir)

                # Log the model graph using a sample batch
                example_input_batch = next(iter(train_loader))[0].to(device)
                writer.add_graph(model, example_input_batch)

                best_val_accuracy = 0
                best_val_loss = float('inf')  # Lower is better for loss

                for epoch in range(epochs):
                    model.train()
                    epoch_loss = 0

                    for X_batch, y_batch in train_loader:
                        X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)
                        optimizer.zero_grad()

                        with autocast():
                            outputs = model(X_batch)
                            loss = criterion(outputs, y_batch)

                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                        epoch_loss += loss.item()

                    # Log the average training loss for this epoch
                    avg_loss = epoch_loss / len(train_loader)
                    writer.add_scalar('Loss/Train', avg_loss, epoch)

                    model.eval()
                    val_loss = 0
                    correct_predictions = 0
                    total_predictions = 0

                    with torch.no_grad():
                        for X_val_batch, y_val_batch in val_loader:
                            X_val_batch, y_val_batch = X_val_batch.to(device, non_blocking=True), y_val_batch.to(device,
                                                                                                                 non_blocking=True)
                            with autocast():
                                val_outputs = model(X_val_batch)
                                val_loss += criterion(val_outputs, y_val_batch).item()

                                # Calculate validation accuracy
                                _, predicted_labels = torch.max(val_outputs, 1)
                                correct_predictions += (predicted_labels == y_val_batch).sum().item()
                                total_predictions += y_val_batch.size(0)

                    avg_val_loss = val_loss / len(val_loader)
                    val_accuracy = correct_predictions / total_predictions

                    # Log validation loss and accuracy
                    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)
                    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

                    # Check if we have the best validation metrics so far
                    if val_accuracy > best_val_accuracy:
                        best_val_accuracy = val_accuracy
                        best_val_loss = avg_val_loss  # Optional, if you want to track corresponding loss

                # Close the TensorBoard writer
                writer.close()

                # Final evaluation
                model.eval()
                with torch.no_grad():
                    y_pred = model(X_val_tensor.to(device)).cpu().numpy()
                    if output_dim == 1:
                        y_pred = np.array(y_pred).squeeze()
        else:
            y_train = y_train.ravel()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            y_pred = np.array(y_pred).squeeze()

        """If y_pred has more than one channel, apply argmax along the last axis."""
        if y_pred.ndim > 1 and y_pred.shape[-1] > 1:
            y_pred = np.argmax(y_pred, axis=-1, keepdims=True)

        if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
            if hasattr(model, 'model'):
                model = model.model  # Extract the actual Keras model
            model.save(file_path)
            return y_pred.astype(dtype), best_accuracy, best_top_k, best_crossentropy

        elif regression_type_combo == "TabNet":
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype)

        elif regression_type_combo in ["TCN", "ResNet"]:
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype), best_val_accuracy, best_val_loss

        return y_pred.astype(dtype), model

    def execute_categorization(self, result_layout, regression_type_combo, selected_targets,
                               selected_features, dimensionality_reduction_type_combo, n_components,
                               selected_parent_key, selected_feature_input_keys, feature_input_names,
                               selected_channel_texts):
        try:

            num_lstm_layers_min = None
            epochs = None
            max_trials = None
            num_lstm_layers_max = None
            num_lstm_layers_step = None
            lstm_units_min = None
            lstm_units_max = None
            lstm_units_step = None
            l2_lstm_min = None
            l2_lstm_max = None
            l2_lstm_step = None
            min_dropout_lstm = None
            max_dropout_lstm = None
            dropout_lstm_layer_step = None
            dropout_lstm_min = None
            dropout_lstm_max = None
            dropout_lstm_step = None
            num_conv_layers_min = None
            num_conv_layers_max = None
            num_conv_layers_step = None
            conv_filters_min = None
            conv_filters_max = None
            conv_filters_step = None
            conv_kernel_size_min = None
            conv_kernel_size_max = None
            conv_kernel_size_step = None
            l2_conv_min = None
            l2_conv_max = None
            l2_conv_step = None
            min_dropout_conv = None
            max_dropout_conv = None
            dropout_conv_layer_step = None
            dropout_conv_min = None
            dropout_conv_max = None
            dropout_conv_step = None
            num_attention_layers_min = None
            num_attention_layers_max = None
            num_attention_layers_step = None
            attention_heads_min = None
            attention_heads_max = None
            attention_heads_step = None
            attention_key_dim_min = None
            attention_key_dim_max = None
            attention_key_dim_step = None
            l2_attention_min = None
            l2_attention_max = None
            l2_attention_step = None
            min_dropout_attention = None
            max_dropout_attention = None
            dropout_attention_layer_step = None
            dropout_attention_min = None
            dropout_attention_max = None
            dropout_attention_step = None
            num_dense_layers_min = None
            num_dense_layers_max = None
            num_dense_layers_step = None
            dense_units_min = None
            dense_units_max = None
            dense_units_step = None
            l2_dense_min = None
            l2_dense_max = None
            l2_dense_step = None
            min_dropout_dense = None
            max_dropout_dense = None
            dropout_dense_layer_step = None
            dropout_dense_min = None
            dropout_dense_max = None
            dropout_dense_step = None
            learning_rate_min = None
            learning_rate_max = None
            learning_rate_step = None
            batch_size = None
            file_path = None
            log_dir = None
            output_dim = None

            def add_model_to_dict(model_dict=None, model_type=None, model=None, features=None, target=None,
                                  file_path=None,
                                  dimensionality_reduction_type_combo=None, n_components=None):
                """
                Adds a trained model, its metadata, and file path to the model dictionary.
                """
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": file_path,
                    "dim_type": dimensionality_reduction_type_combo,
                    "n_components": n_components
                }

                combined_key = f"{model_type}_{dimensionality_reduction_type_combo}"

                if combined_key not in model_dict:
                    model_dict[combined_key] = []

                model_dict[combined_key].append(model_metadata)

            dialog = QDialog(self)
            dialog.setWindowTitle("Train test split")
            layout = QVBoxLayout()

            label = QLabel("Choose the train test split")
            layout.addWidget(label)

            spinBox = QSpinBox()
            spinBox.setMinimum(1)
            spinBox.setMaximum(99)
            spinBox.setValue(5)
            layout.addWidget(spinBox)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Accepted:
                train_test = spinBox.value()
            else:
                return

            # Convert selected features and targets to numpy arrays
            X = np.array(selected_features).T
            y = np.array(selected_targets).T

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Standard Scaler", "Min-Max Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return

            normalization_choice = combo.currentText()

            if normalization_choice == "Min-Max Scaler":
                scaler = MinMaxScaler()
                X = scaler.fit_transform(X)
            elif normalization_choice == "Standard Scaler":
                scaler = StandardScaler()
                X = scaler.fit_transform(X)
            else:
                pass

            n_components = None
            # Create a new dialog to select linear or non-linear regression
            cross_plot_dialog = QDialog(self)
            cross_plot_dialog.setWindowTitle("Dimensionality Reduction Options")

            layout = QVBoxLayout(cross_plot_dialog)

            dimensionality_reduction_type_combo = QComboBox()
            dimensionality_reduction_type_combo.addItems([
                "None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"

            ])
            layout.addWidget(dimensionality_reduction_type_combo)

            select_button = QPushButton("OK")
            select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
            layout.addWidget(select_button)

            cross_plot_dialog.setLayout(layout)
            cross_plot_dialog.adjustSize()
            cross_plot_dialog.setMinimumWidth(250)
            # Disable the '?' help button on the dialog
            cross_plot_dialog.setWindowFlags(
                cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

            result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
            if result == QDialog.DialogCode.Accepted:
                dimensionality_reduction_type_combo = dimensionality_reduction_type_combo.currentText()

                if dimensionality_reduction_type_combo in ["t-SNE", "ICA", "Random Projection", "PCA",
                                                           "Autoencoder"]:

                    dialog = QDialog(self)
                    dialog.setWindowTitle("Principal Components")
                    layout = QVBoxLayout()

                    input_field_label = QLabel()
                    input_field_label.setText("Select the Number of Components:")
                    layout.addWidget(input_field_label)

                    input_field = QLineEdit()
                    # Using QDoubleValidator to allow decimals and integers
                    validator = QDoubleValidator(0, X.shape[1], 2, input_field)
                    validator.setNotation(QDoubleValidator.Notation.StandardNotation)
                    input_field.setValidator(validator)
                    layout.addWidget(input_field)

                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(225)
                    # Disable the '?' help button on the dialog
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        n_components = float(input_field.text())
                        if n_components.is_integer():
                            # Convert to int if the number is an integer
                            n_components = int(n_components)
                    else:
                        return
            else:
                return

            min_label = np.min(y)
            if min_label != 0:
                y = y - min_label

            # Handle single target variable correctly
            if len(y.shape) == 1:
                y = y.reshape(-1, 1)

            if regression_type_combo in ["TCN", "ResNet", "Hyper Parameter"]:
                # Determine output_dim based on the shape of y_train
                output_dim = len(np.unique(y))

            # Split into training and validation sets
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=train_test / 100, random_state=42)

            # Get the number of epochs for certain regressors
            if regression_type_combo in [
                "Random Forest Classifier",
                "Gradient Boosting Classifier",
                "Support Vector Classifier",
                "K-Nearest Neighbors Classifier",
                "Decision Tree Classifier",
                "Gaussian Process Classifier",
                "CatBoost Classifier",
                "LightGBM Classifier",
                "Naive Bayes Classifier",
                "Extra Trees Classifier",
                "Bagging Classifier",
                "Voting Classifier",
                "Keras Classifier",
                "ResNet",
                "TabNet",
                "TCN",
                "Hyper Parameter"
            ]:

                dialog = QDialog(self)
                dialog.setWindowTitle("Epochs")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of epochs")
                layout.addWidget(label)

                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(1000)
                spinBox.setValue(20)
                layout.addWidget(spinBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    epochs = spinBox.value()
                else:
                    return

                # Initialize the model based on the classification type
                if regression_type_combo == "Hyper Parameter":

                    # Create the dialog
                    dialog = QDialog(self)
                    dialog.setWindowTitle("Hyperparameters")
                    layout = QGridLayout()  # Use a grid layout for better organization

                    # Add spin boxes for different hyperparameters
                    def add_spin_boxes(label_text, min_value, max_value, default_value, step=1, is_float=False):
                        label = QLabel(label_text)
                        label.setMinimumWidth(150)  # Set a minimum width for labels
                        min_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                        if is_float:
                            min_spin_box.setDecimals(6)
                        min_spin_box.setMinimum(min_value)
                        min_spin_box.setMaximum(max_value)
                        min_spin_box.setSingleStep(step)
                        min_spin_box.setValue(min_value)

                        max_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            max_spin_box.setDecimals(6)
                        max_spin_box.setMinimum(min_value)
                        max_spin_box.setMaximum(max_value)
                        max_spin_box.setSingleStep(step)
                        max_spin_box.setValue(max_value)

                        step_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            step_spin_box.setDecimals(6)
                        step_spin_box.setMinimum(0.000001 if is_float else 1)
                        step_spin_box.setMaximum(max_value - min_value)
                        step_spin_box.setSingleStep(step)
                        step_spin_box.setValue(step)

                        return label, min_spin_box, max_spin_box, step_spin_box

                    # Layout management function
                    def add_spin_boxes_to_grid(grid, start_row, column, *spinbox_data):
                        for i, (label, min_box, max_box, step_box) in enumerate(spinbox_data):
                            row = start_row + i * 4
                            grid.addWidget(label, row, column, 1, 2)
                            grid.addWidget(QLabel("Min"), row + 1, column)
                            grid.addWidget(min_box, row + 1, column + 1)
                            grid.addWidget(QLabel("Max"), row + 2, column)
                            grid.addWidget(max_box, row + 2, column + 1)
                            grid.addWidget(QLabel("Step"), row + 3, column)
                            grid.addWidget(step_box, row + 3, column + 1)

                    # LSTM layers
                    lstm_spin_boxes = [
                        add_spin_boxes("Number of LSTM Layers", 0, 100, 10),
                        add_spin_boxes("LSTM Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization LSTM", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of LSTM Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate LSTM", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Conv1D layers
                    conv_spin_boxes = [
                        add_spin_boxes("Number of Conv Layers", 0, 100, 10),
                        add_spin_boxes("Conv Filters", 16, 256, 128, step=32),
                        add_spin_boxes("Conv Kernel Size", 1, 5, 1),
                        add_spin_boxes("L2 Regularization Conv", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Conv Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Conv", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # MultiHeadAttention layers
                    attention_spin_boxes = [
                        add_spin_boxes("Number of Attention Layers", 0, 100, 10),
                        add_spin_boxes("Attention Heads", 2, 8, 4, step=2),
                        add_spin_boxes("Attention Key Dim", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Attention", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Attention Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Attention", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Dense layers
                    dense_spin_boxes = [
                        add_spin_boxes("Number of Dense Layers", 0, 100, 10),
                        add_spin_boxes("Dense Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Dense", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Dense Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Dense", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Add the spin boxes to the grid layout
                    add_spin_boxes_to_grid(layout, 0, 0, *lstm_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 2, *conv_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 4, *attention_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 6, *dense_spin_boxes)

                    # Learning rate
                    lr_label, lr_min, lr_max, lr_step = add_spin_boxes("Learning Rate", 1e-4, 1e-2, 1e-3, step=1e-4,
                                                                       is_float=True)
                    layout.addWidget(lr_label, 20, 0, 1, 2)
                    layout.addWidget(QLabel("Min"), 21, 0)
                    layout.addWidget(lr_min, 21, 1)
                    layout.addWidget(QLabel("Max"), 22, 0)
                    layout.addWidget(lr_max, 22, 1)
                    layout.addWidget(QLabel("Step"), 23, 0)
                    layout.addWidget(lr_step, 23, 1)

                    # Max trials
                    label_max_trials = QLabel("Maximum Trials")
                    spinBox_max_trials = QSpinBox(self)
                    spinBox_max_trials.setMinimum(0)
                    spinBox_max_trials.setMaximum(2000)
                    spinBox_max_trials.setValue(10)
                    layout.addWidget(label_max_trials, 24, 0, 1, 2)
                    layout.addWidget(spinBox_max_trials, 25, 0, 1, 2)

                    # Batch size
                    label_batch_size = QLabel("Batch Size")
                    spinBox_batch_size = QSpinBox(self)
                    spinBox_batch_size.setMinimum(32)
                    spinBox_batch_size.setMaximum(2048)
                    spinBox_batch_size.setValue(16)
                    layout.addWidget(label_batch_size, 26, 0, 1, 2)
                    layout.addWidget(spinBox_batch_size, 27, 0, 1, 2)

                    # OK button
                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button, 28, 0, 1, 2)

                    layout.setColumnStretch(1, 1)
                    layout.setColumnStretch(3, 1)
                    layout.setColumnStretch(5, 1)
                    layout.setColumnStretch(7, 1)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(800)  # Increase minimum width
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()

                    # Execute the dialog and get the values
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        # LSTM layer parameters
                        num_lstm_layers_min = lstm_spin_boxes[0][1].value()
                        num_lstm_layers_max = lstm_spin_boxes[0][2].value()
                        num_lstm_layers_step = lstm_spin_boxes[0][3].value()
                        lstm_units_min = lstm_spin_boxes[1][1].value()
                        lstm_units_max = lstm_spin_boxes[1][2].value()
                        lstm_units_step = lstm_spin_boxes[1][3].value()
                        l2_lstm_min = lstm_spin_boxes[2][1].value()
                        l2_lstm_max = lstm_spin_boxes[2][2].value()
                        l2_lstm_step = lstm_spin_boxes[2][3].value()
                        dropout_lstm_min = lstm_spin_boxes[4][1].value()
                        dropout_lstm_max = lstm_spin_boxes[4][2].value()
                        dropout_lstm_step = lstm_spin_boxes[4][3].value()
                        min_dropout_lstm = lstm_spin_boxes[3][1].value()
                        max_dropout_lstm = lstm_spin_boxes[3][2].value()
                        dropout_lstm_layer_step = lstm_spin_boxes[3][3].value()

                        # Conv layer parameters
                        num_conv_layers_min = conv_spin_boxes[0][1].value()
                        num_conv_layers_max = conv_spin_boxes[0][2].value()
                        num_conv_layers_step = conv_spin_boxes[0][3].value()
                        conv_filters_min = conv_spin_boxes[1][1].value()
                        conv_filters_max = conv_spin_boxes[1][2].value()
                        conv_filters_step = conv_spin_boxes[1][3].value()
                        conv_kernel_size_min = conv_spin_boxes[2][1].value()
                        conv_kernel_size_max = conv_spin_boxes[2][2].value()
                        conv_kernel_size_step = conv_spin_boxes[2][3].value()
                        l2_conv_min = conv_spin_boxes[3][1].value()
                        l2_conv_max = conv_spin_boxes[3][2].value()
                        l2_conv_step = conv_spin_boxes[3][3].value()
                        dropout_conv_min = conv_spin_boxes[5][1].value()
                        dropout_conv_max = conv_spin_boxes[5][2].value()
                        dropout_conv_step = conv_spin_boxes[5][3].value()
                        min_dropout_conv = conv_spin_boxes[4][1].value()
                        max_dropout_conv = conv_spin_boxes[4][2].value()
                        dropout_conv_layer_step = conv_spin_boxes[4][3].value()

                        # Attention layer parameters
                        num_attention_layers_min = attention_spin_boxes[0][1].value()
                        num_attention_layers_max = attention_spin_boxes[0][2].value()
                        num_attention_layers_step = attention_spin_boxes[0][3].value()
                        attention_heads_min = attention_spin_boxes[1][1].value()
                        attention_heads_max = attention_spin_boxes[1][2].value()
                        attention_heads_step = attention_spin_boxes[1][3].value()
                        attention_key_dim_min = attention_spin_boxes[2][1].value()
                        attention_key_dim_max = attention_spin_boxes[2][2].value()
                        attention_key_dim_step = attention_spin_boxes[2][3].value()
                        l2_attention_min = attention_spin_boxes[3][1].value()
                        l2_attention_max = attention_spin_boxes[3][2].value()
                        l2_attention_step = attention_spin_boxes[3][3].value()
                        dropout_attention_min = attention_spin_boxes[5][1].value()
                        dropout_attention_max = attention_spin_boxes[5][2].value()
                        dropout_attention_step = attention_spin_boxes[5][3].value()
                        min_dropout_attention = attention_spin_boxes[4][1].value()
                        max_dropout_attention = attention_spin_boxes[4][2].value()
                        dropout_attention_layer_step = attention_spin_boxes[4][3].value()

                        # Dense layer parameters
                        num_dense_layers_min = dense_spin_boxes[0][1].value()
                        num_dense_layers_max = dense_spin_boxes[0][2].value()
                        num_dense_layers_step = dense_spin_boxes[0][3].value()
                        dense_units_min = dense_spin_boxes[1][1].value()
                        dense_units_max = dense_spin_boxes[1][2].value()
                        dense_units_step = dense_spin_boxes[1][3].value()
                        l2_dense_min = dense_spin_boxes[2][1].value()
                        l2_dense_max = dense_spin_boxes[2][2].value()
                        l2_dense_step = dense_spin_boxes[2][3].value()
                        min_dropout_dense = dense_spin_boxes[3][1].value()
                        max_dropout_dense = dense_spin_boxes[3][2].value()
                        dropout_dense_layer_step = dense_spin_boxes[3][3].value()
                        dropout_dense_min = dense_spin_boxes[4][1].value()
                        dropout_dense_max = dense_spin_boxes[4][2].value()
                        dropout_dense_step = dense_spin_boxes[4][3].value()

                        # Other parameters
                        learning_rate_min = lr_min.value()
                        learning_rate_max = lr_max.value()
                        learning_rate_step = lr_step.value()
                        max_trials = spinBox_max_trials.value()
                        batch_size = spinBox_batch_size.value()
                    else:
                        return

            if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
                if regression_type_combo == "Keras Classifier":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "keras_classifier",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo == "Hyper Parameter":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "hyper_parameter_classifier",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo in ["TCN", "ResNet"]:
                    # TensorBoard setup
                    log_dir = f"logs/Pytorch/{regression_type_combo}_classifier/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"

            if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                # Check if it's a KerasRegressor and extract the underlying model
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model.h5"

            elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model.pth"

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir_dr = os.path.join("logs", "Tensorflow", "autoencoder_DR_classification_petro",
                                       datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir_dr = None

            result = self.task_runner.run_task(
                TensorVisualizer.run_petro_categorization_task, X, y,
                train_test, num_lstm_layers_min, regression_type_combo, epochs,
                max_trials, num_lstm_layers_max, num_lstm_layers_step,
                lstm_units_min,
                lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                l2_lstm_step, min_dropout_lstm, max_dropout_lstm,
                dropout_lstm_layer_step,
                dropout_lstm_min, dropout_lstm_max, dropout_lstm_step,
                num_conv_layers_min,
                num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                conv_filters_max, conv_filters_step, conv_kernel_size_min,
                conv_kernel_size_max, conv_kernel_size_step, l2_conv_min,
                l2_conv_max,
                l2_conv_step, min_dropout_conv, max_dropout_conv,
                dropout_conv_layer_step,
                dropout_conv_min, dropout_conv_max, dropout_conv_step,
                num_attention_layers_min, num_attention_layers_max,
                num_attention_layers_step,
                attention_heads_min, attention_heads_max, attention_heads_step,
                attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                l2_attention_min, l2_attention_max, l2_attention_step,
                min_dropout_attention, max_dropout_attention,
                dropout_attention_layer_step,
                dropout_attention_min, dropout_attention_max, dropout_attention_step,
                num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                dropout_dense_step, learning_rate_min, learning_rate_max,
                learning_rate_step, log_dir, batch_size, file_path, output_dim,
                dimensionality_reduction_type_combo, n_components, log_dir_dr)

            if result is not None:

                if regression_type_combo in ["Keras Classifier", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                        y_pred, best_accuracy, best_top_k, best_crossentropy = result
                    elif regression_type_combo in ["TCN", "ResNet"]:
                        y_pred, val_accuracy, avg_val_loss = result
                    elif regression_type_combo == "TabNet":
                        y_pred = result
                else:
                    y_pred, model = result

                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir_dr)

                if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
                    if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                        # Access the best accuracy recorded during training
                        self.best_accuracy = best_accuracy
                        self.best_top_k = best_top_k
                        self.best_crossentropy = best_crossentropy
                        self.show_tensorboard(log_dir=log_dir)

                    elif regression_type_combo in ["TCN", "ResNet"]:
                        self.best_val_accuracy = val_accuracy
                        self.best_val_loss = avg_val_loss  # Optional, if you want to track corresponding loss
                        self.show_tensorboard(log_dir=log_dir)

                if regression_type_combo in ["Keras Classifier", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.petro_models, model_type=regression_type_combo, model=None,
                                      features=feature_input_names, target=selected_channel_texts, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                else:  # Assume Sklearn or similar

                    # Ask the user if they want to save the model
                    reply = QMessageBox.question(self, 'Save Model', 'Do you want to save the model?',
                                                 QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                                                 QMessageBox.StandardButton.No)

                    if reply == QMessageBox.StandardButton.Yes:
                        file_dialog = QFileDialog()
                        file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                        file_path = f"{file_directory}/{regression_type_combo}model.pkl"
                        joblib.dump(model, file_path)
                    else:
                        file_path = None
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.petro_models, model_type=regression_type_combo, model=model,
                                      features=feature_input_names, target=selected_channel_texts, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                self.plot_petro_categorical_results(y_val, y_pred, regression_type_combo, result_layout, X, train_test,
                                                    selected_parent_key,
                                                    selected_feature_input_keys, selected_channel_texts)
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_petro_categorical_results(self, y_actual, y_pred, regression_type, result_layout, X_clean_copy, train_test,
                                       selected_parent_key, selected_feature_input_keys, selected_channel_texts):
        """
        Plot the predicted values against the actual values for regression analysis.
        """

        if y_actual.ndim > 1:
            y_actual = y_actual.ravel()

        if y_pred.ndim > 1 and y_pred.shape[1] == 1:
            y_pred = y_pred.ravel()

        elif y_pred.ndim > 1 and y_pred.shape[1] > 1:
            y_pred = np.argmax(y_pred, axis=1)

        def create_selection_dialog(title, items):
            dialog = QDialog(self)
            dialog.setWindowTitle(title)

            layout = QVBoxLayout(dialog)
            list_widget = QListWidget()
            list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)

            for item in items:
                list_widget.addItem(item)

            layout.addWidget(list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(dialog.accept)
            layout.addWidget(select_button)

            dialog.setLayout(layout)
            if dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return

            selected_items = list_widget.selectedItems()
            if len(selected_items) == 0:
                QMessageBox.warning(self, "Warning", f"Please select an item from the {title} list.")
                return None

            return selected_items[0].text()

        # Select target variable
        selected_target_name = create_selection_dialog("Select Target Variable", selected_channel_texts)
        if selected_target_name is None:
            return

        # Map the selected target name back to its index
        selected_target_idx = selected_channel_texts.index(selected_target_name)

        y_pred_selected = y_pred[:, selected_target_idx] if y_pred.ndim > 1 else y_pred
        y_actual_selected = y_actual[:, selected_target_idx] if y_actual.ndim > 1 else y_actual

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.petro_color_mapping, source='matplotlib')

        # Clear the previous plot and reset the view
        self.petro_widget.clear()
        self.petro_widget.autoRange()

        # Calculate confusion matrix
        conf_matrix = confusion_matrix(y_actual_selected, y_pred_selected)

        # Get the actual range of the confusion matrix
        conf_min, conf_max = conf_matrix.min(), conf_matrix.max()

        # Create an ImageItem for the confusion matrix
        img = pg.ImageItem(conf_matrix.T)

        # Set the colormap to the ImageItem
        levels = (conf_min, conf_max)  # Use the actual range of the raw values
        img.setLookupTable(cmap.getLookupTable(nPts=256))  # 256 points for the color map
        img.setLevels(levels)  # Set levels to match the actual data range

        # Add the image to the plot
        self.petro_widget.addItem(img)

        # Set axis labels
        self.petro_widget.setLabel('bottom', 'Predicted Class')
        self.petro_widget.setLabel('left', 'Actual Class')

        # Adjust the view to align tiles correctly with axes
        img.setRect(QRectF(0, 0, conf_matrix.shape[1], conf_matrix.shape[0]))

        # Add text labels for confusion matrix values
        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                value = conf_matrix[i, j]
                text = pg.TextItem(str(value), anchor=(0.5, 0.5))
                text.setPos(j + 0.5, i + 0.5)  # Center the text in each tile
                self.petro_widget.addItem(text)

        # Create or update the ColorBarItem for the color bar
        if hasattr(self, 'petro_color_bar') and self.petro_color_bar is not None:
            self.petro_color_bar.setLevels(levels)  # Update levels to match the actual range
            self.petro_color_bar.setColorMap(cmap)
        else:
            color_bar = pg.ColorBarItem(values=levels)
            color_bar.setColorMap(cmap)
            self.petro_color_bar = color_bar
            self.petro_widget.getPlotItem().layout.addItem(self.petro_color_bar, 2, 2)

        self.petro_widget.autoRange()

        if regression_type in ["Keras Classifier", "Hyper Parameter"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual_selected, y_pred_selected):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Sparse Categorical Accuracy: {self.best_accuracy:.2f}\n"
            result_text += f"Sparse Top K Categorical Accuracy: {self.best_top_k:.2f}\n"
            result_text += f"Sparse Categorical Crossentropy: {self.best_crossentropy :.2f}\n"

        elif regression_type in ["ResNet", "TCN"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual_selected, y_pred_selected):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Sparse Categorical Accuracy: {self.best_val_accuracy:.2f}\n"
            result_text += f"Sparse Categorical Crossentropy: {self.best_val_loss:.2f}\n"

        elif regression_type in ["TabNet"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual_selected, y_pred_selected):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
        else:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual_selected, y_pred_selected):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual_selected, y_pred_selected, average='weighted', zero_division=0):.2f}\n"

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def process_and_predict_petro_regression(self):
        try:

            def add_new_model_to_dict():
                """Add a new model with metadata to the dictionary."""

                # Step 1: Browse for the model file
                model_file_path, _ = QFileDialog.getOpenFileName(self, 'Select Model File', '',
                                                                 'Model Files (*.pkl *.h5 *.pth);;All Files (*)')
                if not model_file_path:
                    return  # Cancel if no file selected

                # Step 2: Ask for model type, including an "Else" option
                if model_file_path.endswith(".pkl"):
                    model_type_name = "Machine Learning"
                elif model_file_path.endswith(".h5"):
                    model_type_name = "TF Neural Network"
                elif model_file_path.endswith(".pth"):
                    model_type_name = "torch Neural Network"
                else:
                    QMessageBox.critical(self, "Error", f"Unsupported model format")
                    return  # Cancel if no input

                # Step 4: Determine whether to load the model
                if model_file_path.endswith(".pkl"):
                    try:
                        model = joblib.load(model_file_path)
                    except Exception as e:
                        QMessageBox.critical(self, "Error", f"Failed to load model: {str(e)}")
                        return
                else:
                    model = None  # Neural network models remain uninitialized

                # Step 5: Get features
                # Get children of the selected parent key
                child_keys = []
                parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                    selected_parent_key].get('source')
                for key, value in self.metadata.items():
                    name_or_source = value.get('name') or value.get('source')
                    template = value.get('template').strip()
                    if name_or_source == parent_name_or_source and template != "Seismic" and not template.startswith(
                            'Upscaled'):
                        child_keys.append(key)

                # Display child keys of the selected parent key for feature input selection in a dialog
                feature_input_list_widget_dialog = QDialog(self)
                feature_input_list_widget_dialog.setWindowTitle("Select Feature Inputs")

                layout = QVBoxLayout(feature_input_list_widget_dialog)
                feature_input_list_widget = QListWidget()
                feature_input_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
                for key in child_keys:
                    child_name = self.metadata[key].get('template').strip()
                    feature_input_list_widget.addItem(child_name)
                layout.addWidget(feature_input_list_widget)

                ok_button = QPushButton("OK")
                ok_button.clicked.connect(feature_input_list_widget_dialog.accept)
                layout.addWidget(ok_button)

                feature_input_list_widget_dialog.setLayout(layout)

                if child_keys:
                    if feature_input_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                        return

                chosen_feature_input_keys = [selected_parent_key]

                selected_child_input_keys = []
                for item in feature_input_list_widget.selectedItems():
                    for key in child_keys:
                        if self.metadata[key].get('template').strip() == item.text():
                            selected_child_input_keys.append(key)

                chosen_feature_input_keys.extend(selected_child_input_keys)
                features = chosen_feature_input_keys

                # Step 6: Get target variable
                target, ok = QInputDialog.getText(
                    self, "Target Variable", "Enter the target variable:"
                )
                if not ok or not target.strip():
                    return
                target = target.strip()

                # Step 7: Ask for dimensionality reduction type
                dim_types = ["None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"]
                dim_type, ok = QInputDialog.getItem(
                    self, "Dimensionality Reduction", "Select type:", dim_types, editable=False
                )
                if not ok:
                    return
                dim_type = dim_type if dim_type != "None" else None

                # Step 8: If dimensionality reduction is selected, ask for number of components
                n_components = None
                if dim_type:
                    n_components, ok = QInputDialog.getInt(
                        self, "Number of Components",
                        f"Enter the number of components (max {len(features)}):",
                        min=1, max=len(features)
                    )
                    if not ok:
                        return

                # Step 9: Add model to the dictionary
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": model_file_path,
                    "dim_type": dim_type,
                    "n_components": n_components,
                }
                combined_key = f"{model_type_name}_{dim_type if dim_type else 'None'}"

                if combined_key not in self.petro_models:
                    self.petro_models[combined_key] = []

                self.petro_models[combined_key].append(model_metadata)

                QMessageBox.information(self, "Success", "Model successfully added to the dictionary!")

            def select_feature_inputs(selected_parent_key):
                # Get children of the selected parent key
                child_keys = []
                parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                    selected_parent_key].get('source')
                for key, value in self.metadata.items():
                    name_or_source = value.get('name') or value.get('source')
                    template = value.get('template').strip()
                    if name_or_source == parent_name_or_source and template != "Seismic" and not template.startswith(
                            'Upscaled'):
                        child_keys.append(key)

                # Display child keys of the selected parent key for feature input selection in a dialog
                feature_input_list_widget_dialog = QDialog(self)
                feature_input_list_widget_dialog.setWindowTitle("Select Feature Inputs")

                layout = QVBoxLayout(feature_input_list_widget_dialog)
                feature_input_list_widget = QListWidget()
                feature_input_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
                for key in child_keys:
                    child_name = self.metadata[key].get('template').strip()
                    feature_input_list_widget.addItem(child_name)
                layout.addWidget(feature_input_list_widget)

                ok_button = QPushButton("OK")
                ok_button.clicked.connect(feature_input_list_widget_dialog.accept)
                layout.addWidget(ok_button)

                feature_input_list_widget_dialog.setLayout(layout)

                if child_keys:
                    if feature_input_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                        return

                chosen_feature_input_keys = [selected_parent_key]

                selected_child_input_keys = []
                for item in feature_input_list_widget.selectedItems():
                    for key in child_keys:
                        if self.metadata[key].get('template').strip() == item.text():
                            selected_child_input_keys.append(key)

                chosen_feature_input_keys.extend(selected_child_input_keys)

                return chosen_feature_input_keys

            # Step 1: Extract keys from self.tensor_dict that correspond to NumPy tensors
            tensor_keys = [key for key, value in self.tensor_dict.items() if isinstance(value, np.ndarray)]

            # Step 2: Get parent keys from the QTreeWidget
            parent_keys = []
            root = self.treeWidget.invisibleRootItem()
            child_count = root.childCount()

            for i in range(child_count):
                parent_item = root.child(i)
                parent_key = parent_item.text(0)
                if parent_key in tensor_keys:
                    parent_keys.append(parent_key)

            # Display parent keys for selection in a dialog
            parent_list_widget_dialog = QDialog(self)
            parent_list_widget_dialog.setWindowTitle("Select Parent")

            layout = QVBoxLayout(parent_list_widget_dialog)
            parent_list_widget = QListWidget()
            parent_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for key in parent_keys:
                parent_list_widget.addItem(key)
            layout.addWidget(parent_list_widget)

            ok_button = QPushButton("OK")
            ok_button.clicked.connect(parent_list_widget_dialog.accept)
            layout.addWidget(ok_button)

            add_new_model_to_dict_button = QPushButton("Import Model")
            add_new_model_to_dict_button.clicked.connect(add_new_model_to_dict)
            layout.addWidget(add_new_model_to_dict_button)

            parent_list_widget_dialog.setLayout(layout)
            if parent_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_parent_key = parent_list_widget.currentItem().text()
            selected_feature_input_keys = select_feature_inputs(selected_parent_key)

            # Step 3: Find Target Variables Based on Selected Features in the Model Dictionary
            potential_target_variables = set()
            for model_type, models in self.petro_models.items():
                for model_info in models:
                    if set(selected_feature_input_keys) == set(model_info['features']):
                        potential_target_variables.add(tuple(model_info['target']))  # Convert list to tuple

            if not potential_target_variables:
                QMessageBox.warning(self, "No Matching Models",
                                    "No models are available for the selected feature inputs.")
                return

            # Step 5: Model Selection Based on Features
            available_models = []
            for model_type, models in self.petro_models.items():
                for model_info in models:
                    if set(selected_feature_input_keys) == set(model_info['features']):
                        if isinstance(model_info['target'], str):
                            # Keep it as a string if it's already a string
                            target_variable = model_info['target']
                        else:
                            # Convert to tuple only if it's not a string
                            target_variable = tuple(model_info['target'])

                        available_models.append((model_type, model_info, target_variable))

            if not available_models:
                QMessageBox.warning(self, "No Matching Model",
                                    "No trained models available for the selected features.")
                return

            model_selection_dialog = QDialog(self)
            model_selection_dialog.setWindowTitle("Select Model for Prediction")
            layout = QVBoxLayout(model_selection_dialog)

            model_list_widget = QListWidget()
            model_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for model_type, model_info, target_variable in available_models:
                # Check if target_variable is a list
                if isinstance(target_variable, str):
                    target = target_variable
                else:
                    target = ' '.join(target_variable)
                # Add both model type and target variable to the list
                model_list_widget.addItem(f"{model_type} model (Target: {target})")

            layout.addWidget(model_list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(model_selection_dialog.accept)
            layout.addWidget(select_button)

            model_selection_dialog.setLayout(layout)
            if model_selection_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_model_items = model_list_widget.selectedItems()
            if len(selected_model_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select a model.")
                return

            # Extract the selected model information
            selected_model_index = model_list_widget.currentRow()
            selected_model_type, selected_model_info, selected_target_variable = available_models[selected_model_index]

            # Get the model object
            model = selected_model_info['model']
            file_path = selected_model_info['file_path']
            targets = selected_model_info['target']
            dimensionality_reduction_type_combo = selected_model_info['dim_type']
            n_components = selected_model_info['n_components']

            # Step 6: Data Preprocessing
            feature_inputs = []

            # Collect all tensors for the selected keys
            for key in selected_feature_input_keys:
                feature_inputs.append(np.copy(self.tensor_dict[key]))

            # Check if the list is empty
            if not feature_inputs:
                QMessageBox.warning(self, "Warning", "No tensors available for the selected feature inputs.")
                return

            # Extract the shape of one tensor as a reference (excluding the channel dimension)
            reference_shape = feature_inputs[0].shape[:3]  # Get the Depth, Height, Width

            # Flatten each tensor
            flattened_feature_inputs = [tensor.flatten() for tensor in feature_inputs]

            # Stack flattened tensors into a 2D array
            X = np.stack(flattened_feature_inputs, axis=1)

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Standard Scaler", "Min-Max Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Rejected:
                return

            normalization_choice = combo.currentText()

            selected_model_type = selected_model_type.split('_')[0]

            if selected_model_type in ["ResNet", "TCN", "Keras Regressor", "Hyper Parameter", "TabNet",
                                       "Keras Classifier", "TF Neural Network", "torch Neural Network"]:
                model = None
                if selected_model_type in ["ResNet", "TCN", "TabNet", "torch Neural Network"]:
                    # Set up the QInputDialog
                    dialog = QInputDialog(self)
                    dialog.setWindowTitle("Batch Size Input")
                    dialog.setLabelText("Please enter the batch size:")
                    dialog.setIntRange(1, 100000)  # Set the range of acceptable values
                    if selected_model_type in ["TCN", "TabNet"]:
                        dialog.setIntValue(100000)  # Set the initial value
                    else:
                        dialog.setIntValue(10000)  # Set the initial value
                    # Execute the dialog and get the batch size
                    if dialog.exec() == QInputDialog.DialogCode.Accepted:
                        batch_size = dialog.intValue()
                    else:
                        return
                else:
                    batch_size = None
            else:
                batch_size = None

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir = os.path.join("logs", "Tensorflow", "autoencoder_DR_predict_petro",
                                       datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir = None

            y_pred = self.task_runner.run_task(TensorVisualizer.run_petro_inference_regression, X,
                                               selected_model_type, file_path, batch_size, model, dimensionality_reduction_type_combo, n_components, normalization_choice, log_dir)

            if y_pred is not None:
                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir)

                # Determine the number of channels
                if y_pred.ndim == 1:
                    num_channels = 1
                else:
                    num_channels = y_pred.shape[1]

                # Calculate the total number of elements in the reference shape
                total_elements = np.prod(reference_shape)

                # Ensure the number of samples in y_pred matches the total elements in the reference shape
                if y_pred.shape[0] != total_elements:
                    raise ValueError(
                        "The number of samples in y_pred does not match the total elements in the reference shape.")

                # Reshape y_pred to the desired shape
                new_shape = (*reference_shape, num_channels)
                reshaped_y_pred = y_pred.reshape(new_shape)

                self.tensor_data = reshaped_y_pred
                # Store the modified tensor with an informative key
                if isinstance(targets, str):
                    target_items = targets
                else:
                    target_items = ' '.join(targets)
                self.metadata[f"{selected_parent_key}_Predicted {target_items} {selected_model_type}"] = self.metadata[selected_parent_key].copy()
                self.metadata[f"{selected_parent_key}_Predicted {target_items} {selected_model_type}"]['template'] = f"Predicted {target_items} {selected_model_type}"
                self.add_tensor(f"{selected_parent_key}_Predicted {target_items} {selected_model_type}", self.tensor_data)
                QMessageBox.information(self, "Prediction Complete", "The prediction process has finished successfully.")

        except Exception as e:
            traceback.print_exc()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def run_petro_inference_regression(X, selected_model_type, file_path, batch_size, model, dimensionality_reduction_type_combo, n_components, normalization_choice, log_dir):

        def load_model_by_type(file_path, selected_model_type):
            """
            Loads the model from the specified file path based on its type.

            :param file_path: Path to the saved model file.
            :param selected_model_type: Type of the model (e.g., "Keras", "PyTorch", "Sklearn").
            :return: The loaded model object.
            """
            if selected_model_type in ["Keras Regressor", "Hyper Parameter", "Keras Classifier",
                                       "TF Neural Network"]:
                # Load the Keras model
                return load_model(file_path, compile=False)
            elif selected_model_type in ["ResNet", "TCN", "TabNet", "torch Neural Network"]:
                # Load the whole PyTorch model
                return torch.load(file_path)

        # Set up logging
        TensorVisualizer.setup_logging('Petro_Regression_Inference')

        dtype = X.dtype

        if normalization_choice == "Min-Max Scaler":
            scaler = MinMaxScaler()
            X_clean = scaler.fit_transform(X)
        elif normalization_choice == "Standard Scaler":
            scaler = StandardScaler()
            X_clean = scaler.fit_transform(X)
        else:
            X_clean = X

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X_clean)

            X_clean = pca.transform(X_clean)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X_clean = tsne.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X_clean)
            X_clean = ica.transform(X_clean)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X_clean = grp.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X_clean.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X_clean, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val), callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X_clean = encoder.predict(X_clean)

        else:
            pass

        # Predict using the fitted model
        if selected_model_type in ["Keras Regressor", "Keras Classifier"]:
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)
            y_pred = model.predict(X_clean)

        elif selected_model_type in ["Hyper Parameter", "TF Neural Network"]:
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)
            if X_clean.shape[1] > 1:
                X_clean = np.reshape(X_clean, (-1, 1, 3))

            y_pred = model.predict(X_clean)

        elif selected_model_type == "TabNet":
            # Make predictions
            model = load_model_by_type(file_path, selected_model_type)

            y_pred_list = []
            for i in range(0, len(X_clean), batch_size):
                batch_X = X_clean[i:i + batch_size]
                y_pred_batch = model.predict(batch_X)
                y_pred_list.append(y_pred_batch)

            # Concatenate all batch predictions
            y_pred = np.concatenate(y_pred_list, axis=0)

        elif selected_model_type in ["TCN", "ResNet", "torch Neural Network"]:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            # Load and prepare your model
            model = load_model_by_type(file_path, selected_model_type)
            model.to(device)
            model.eval()

            # Convert X_clean from a NumPy array to a PyTorch tensor
            X_val_tensor = torch.tensor(X_clean.reshape(X_clean.shape[0], 1, -1), dtype=torch.float32)

            # Create a DataLoader to handle batches of data
            val_dataset = TensorDataset(X_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            y_pred_list = []

            # Disable gradient calculation for inference
            with torch.no_grad():
                for batch in val_loader:
                    batch_X = batch[0].to(device)
                    y_pred_batch = model(batch_X).cpu().numpy()
                    y_pred_list.append(y_pred_batch)

            # Concatenate all batch predictions
            y_pred = np.concatenate(y_pred_list, axis=0)
        else:
            y_pred = model.predict(X_clean)

        return y_pred.astype(dtype)

    @staticmethod
    def run_petro_regression_task(X, y, train_test, num_lstm_layers_min, regression_type_combo, epochs,
                                  max_trials, num_lstm_layers_max, num_lstm_layers_step, lstm_units_min,
                                  lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                  l2_lstm_step, min_dropout_lstm, max_dropout_lstm, dropout_lstm_layer_step,
                                  dropout_lstm_min, dropout_lstm_max, dropout_lstm_step, num_conv_layers_min,
                                  num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                  conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                  conv_kernel_size_max, conv_kernel_size_step, l2_conv_min, l2_conv_max,
                                  l2_conv_step, min_dropout_conv, max_dropout_conv, dropout_conv_layer_step,
                                  dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                  num_attention_layers_min, num_attention_layers_max, num_attention_layers_step,
                                  attention_heads_min, attention_heads_max, attention_heads_step,
                                  attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                                  l2_attention_min, l2_attention_max, l2_attention_step,
                                  min_dropout_attention, max_dropout_attention, dropout_attention_layer_step,
                                  dropout_attention_min, dropout_attention_max, dropout_attention_step,
                                  num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                                  dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                  l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                                  dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                  dropout_dense_step, learning_rate_min, learning_rate_max,
                                  learning_rate_step, Loss_type_combo, log_dir, batch_size, file_path, log_dir_dr,
                                  dimensionality_reduction_type_combo, n_components):

        # Set up logging
        TensorVisualizer.setup_logging('Petro_Regression')

        dtype = y.dtype

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X)

            X = pca.transform(X)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X = tsne.fit_transform(X)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X)
            X = ica.transform(X)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X = grp.fit_transform(X)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Set up TensorBoard callback
            tensorboard_callback = TensorBoard(log_dir=log_dir_dr, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val),
                            callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X = encoder.predict(X)

        # Split into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=train_test / 100, random_state=42)

        # Determine output_dim based on the shape of y_train
        output_dim = y_train.shape[1] if len(y_train.shape) > 1 else 1

        # Function to create the Keras model
        def create_keras_model():
            model = Sequential()
            model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
            model.add(Dense(32, activation='relu'))
            model.add(Dense(output_dim, activation='linear'))
            model.compile(loss='mse', optimizer='adam', metrics=['mae'])
            return model

        # Define the hypermodel function using the Functional API
        def build_model(hp):
            input_shape = (X_train.shape[1], X_train.shape[2])
            inputs = tf.keras.Input(shape=input_shape)
            x = inputs

            # Add LSTM layers with Dropout and L2 regularization
            num_lstm_layers = hp.Int('num_lstm_layers', min_value=num_lstm_layers_min,
                                     max_value=num_lstm_layers_max,
                                     step=num_lstm_layers_step)

            for i in range(num_lstm_layers):
                x = tf.keras.layers.LSTM(
                    units=hp.Int(f'lstm_units_{i}', min_value=lstm_units_min, max_value=lstm_units_max,
                                 step=lstm_units_step),
                    return_sequences=True,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_lstm_{i}', min_value=l2_lstm_min, max_value=l2_lstm_max, step=l2_lstm_step)
                    )
                )(x)

                num_dropout_lstm = hp.Int(f'num_dropout_lstm_{i}', min_value=min_dropout_lstm,
                                          max_value=max_dropout_lstm, step=dropout_lstm_layer_step)
                for _ in range(num_dropout_lstm):
                    dropout_rate = hp.Float(f'dropout_rate_lstm_{i}', min_value=dropout_lstm_min,
                                            max_value=dropout_lstm_max, step=dropout_lstm_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add Conv1D layers with Dropout and L2 regularization
            num_conv_layers = hp.Int('num_conv_layers', min_value=num_conv_layers_min,
                                     max_value=num_conv_layers_max,
                                     step=num_conv_layers_step)

            for i in range(num_conv_layers):
                x = tf.keras.layers.Conv1D(
                    filters=hp.Int(f'conv_filters_{i}', min_value=conv_filters_min, max_value=conv_filters_max,
                                   step=conv_filters_step),
                    kernel_size=hp.Int(f'conv_kernel_size_{i}', min_value=conv_kernel_size_min,
                                       max_value=conv_kernel_size_max, step=conv_kernel_size_step),
                    activation='relu',
                    padding='same',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_conv_{i}', min_value=l2_conv_min, max_value=l2_conv_max, step=l2_conv_step)
                    )
                )(x)

                num_dropout_conv = hp.Int(f'num_dropout_conv_{i}', min_value=min_dropout_conv,
                                          max_value=max_dropout_conv, step=dropout_conv_layer_step)
                for _ in range(num_dropout_conv):
                    dropout_rate = hp.Float(f'dropout_rate_conv_{i}', min_value=dropout_conv_min,
                                            max_value=dropout_conv_max, step=dropout_conv_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add MultiHeadAttention layers with Dropout and L2 regularization
            num_attention_layers = hp.Int('num_attention_layers', min_value=num_attention_layers_min,
                                          max_value=num_attention_layers_max, step=num_attention_layers_step)

            for i in range(num_attention_layers):
                num_heads = hp.Int(f'num_attention_heads_{i}', min_value=attention_heads_min,
                                   max_value=attention_heads_max, step=attention_heads_step)
                key_dim = hp.Int(f'attention_key_dim_{i}', min_value=attention_key_dim_min,
                                 max_value=attention_key_dim_max, step=attention_key_dim_step)

                attention_layer = tf.keras.layers.MultiHeadAttention(
                    num_heads=num_heads,
                    key_dim=key_dim,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_attention_{i}', min_value=l2_attention_min, max_value=l2_attention_max,
                                 step=l2_attention_step)
                    )
                )
                attention_output = attention_layer(query=x, value=x)
                x = tf.keras.layers.Add()([x, attention_output])  # Residual connection

                num_dropout_attention = hp.Int(f'num_dropout_attention_{i}', min_value=min_dropout_attention,
                                               max_value=max_dropout_attention, step=dropout_attention_layer_step)
                for _ in range(num_dropout_attention):
                    dropout_rate = hp.Float(f'dropout_rate_attention_{i}', min_value=dropout_attention_min,
                                            max_value=dropout_attention_max, step=dropout_attention_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Flatten the output for Dense layers
            x = tf.keras.layers.Flatten()(x)

            # Add Dense layers with Dropout and L2 regularization
            num_dense_layers = hp.Int('num_dense_layers', min_value=num_dense_layers_min,
                                      max_value=num_dense_layers_max, step=num_dense_layers_step)

            for i in range(num_dense_layers):
                x = tf.keras.layers.Dense(
                    units=hp.Int(f'dense_units_{i}', min_value=dense_units_min, max_value=dense_units_max,
                                 step=dense_units_step),
                    activation='relu',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_dense_{i}', min_value=l2_dense_min, max_value=l2_dense_max,
                                 step=l2_dense_step)
                    )
                )(x)

                num_dropout_dense = hp.Int(f'num_dropout_dense_{i}', min_value=min_dropout_dense,
                                           max_value=max_dropout_dense, step=dropout_dense_layer_step)
                for _ in range(num_dropout_dense):
                    dropout_rate = hp.Float(f'dropout_rate_dense_{i}', min_value=dropout_dense_min,
                                            max_value=dropout_dense_max, step=dropout_dense_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Output layer
            output_units = y_train.shape[1] if len(y_train.shape) > 1 else 1
            outputs = tf.keras.layers.Dense(output_units)(x)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=hp.Float('learning_rate', min_value=learning_rate_min,
                                           max_value=learning_rate_max,
                                           step=learning_rate_step)
                ),
                loss=Loss_type_combo
            )

            return model

        # Initialize the model based on the regression type
        if regression_type_combo == "Ridge Regression":
            model = Ridge()

        elif regression_type_combo == "Lasso Regression":
            model = Lasso()

        elif regression_type_combo == "Bayesian Ridge Regression":

            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = BayesianRidge()
            else:
                model = MultiOutputRegressor(BayesianRidge())

        elif regression_type_combo == "Random Forest Regressor":
            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = RandomForestRegressor(n_estimators=epochs)  # Using epochs as number of estimators
            else:
                model = RandomForestRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Gradient Boosting Regressor":

            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = GradientBoostingRegressor(n_estimators=epochs)  # Using epochs as number of estimators
            else:
                model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=epochs))

        elif regression_type_combo == "AdaBoost Regressor":

            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = AdaBoostRegressor(n_estimators=epochs)  # Using epochs as number of estimators
            else:
                model = MultiOutputRegressor(AdaBoostRegressor(n_estimators=epochs))

        elif regression_type_combo == "Support Vector Regressor":

            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = SVR()
            else:
                model = MultiOutputRegressor(SVR())

        elif regression_type_combo == "K-Nearest Neighbors Regressor":
            model = KNeighborsRegressor()

        elif regression_type_combo == "Decision Tree Regressor":
            model = DecisionTreeRegressor()

        elif regression_type_combo == "Gaussian Process Regressor":
            model = GaussianProcessRegressor()

        elif regression_type_combo == "XGBoost Regressor":
            model = XGBRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "CatBoost Regressor":
            if output_dim < 2:
                model = CatBoostRegressor(iterations=epochs)
            else:
                model = CatBoostRegressor(iterations=epochs,
                                          loss_function='MultiRMSE')

        elif regression_type_combo == "LightGBM Regressor":

            if output_dim < 2:
                y_train = y_train.ravel()
                y_val = y_val.ravel()
                model = LGBMRegressor(n_estimators=100 * epochs)
            else:
                model = MultiOutputRegressor(LGBMRegressor(n_estimators=100 * epochs))

        elif regression_type_combo == "Keras Regressor":
            # Create Keras Regressor model
            model = create_keras_model()

        elif regression_type_combo == "ResNet":
            model = ResNet1D_Model(input_dim=X_train.shape[1], output_dim=output_dim)

        elif regression_type_combo == "TabNet":
            # Create TabNet Regressor model
            model = TabNetRegressor(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.001))

        elif regression_type_combo == "TCN":
            num_outputs = y_train.shape[1]
            model = TCN_Model(num_inputs=1, num_channels=[25, 50, 100], num_outputs=num_outputs)

        elif regression_type_combo == "Hyper Parameter":

            X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
            X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

            # Create a temporary directory for the tuner
            with tempfile.TemporaryDirectory() as temp_dir:
                tuner = BayesianOptimization(
                    hypermodel=build_model,
                    objective='val_loss',
                    max_trials=max_trials,
                    directory=temp_dir,  # Use the temporary directory
                    project_name='Hyper_Parameter_Tuning'
                )

            # Perform the search
            tuner.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=batch_size)

            # Get the best model
            model = tuner.get_best_models(num_models=1)[0]

        # Convert to tensors
        X_train_tensor = torch.tensor(X_train.reshape(X_train.shape[0], 1, -1), dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
        X_val_tensor = torch.tensor(X_val.reshape(X_val.shape[0], 1, -1), dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)

        if regression_type_combo in ["Keras Regressor", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
            if regression_type_combo == "Keras Regressor":
                # Early stopping to prevent overfitting
                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                # Fit the model
                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32,
                          callbacks=[early_stopping, tensorboard_callback])

                # Make predictions
                y_pred = model.predict(X_val)

                # Reshape y_pred if output_dim is 1
                if output_dim == 1:
                    y_pred = y_pred.reshape(-1, 1)

            elif regression_type_combo == "Hyper Parameter":
                # Set up TensorBoard callback
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[tensorboard_callback])
                y_pred = model.predict(X_val)
                # Reshape y_pred if output_dim is 1
                if output_dim == 1:
                    y_pred = y_pred.reshape(-1, 1)

            elif regression_type_combo == "TabNet":
                # Train the model
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric=['rmse'],
                    max_epochs=epochs,
                    patience=10,
                    batch_size=32,
                    virtual_batch_size=16
                )

                # Make predictions
                y_pred = model.predict(X_val)

                # Reshape y_pred if output_dim is 1
                if output_dim == 1:
                    y_pred = y_pred.reshape(-1, 1)

            elif regression_type_combo in ["TCN", "ResNet"]:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                model.to(device)

                criterion = nn.MSELoss()
                optimizer = optim.Adam(model.parameters(), lr=0.001)
                scaler = GradScaler()

                # TensorBoard setup
                writer = SummaryWriter(log_dir)

                # Log the model graph using a sample batch
                example_input_batch = next(iter(train_loader))[0].to(device)
                writer.add_graph(model, example_input_batch)

                # Training loop
                for epoch in range(epochs):
                    model.train()
                    epoch_loss = 0

                    for X_batch, y_batch in train_loader:
                        X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)
                        optimizer.zero_grad()

                        with autocast():
                            outputs = model(X_batch)
                            loss = criterion(outputs, y_batch)

                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                        epoch_loss += loss.item()

                    model.eval()
                    val_loss = 0
                    with torch.no_grad():
                        for X_val_batch, y_val_batch in val_loader:
                            X_val_batch, y_val_batch = X_val_batch.to(device, non_blocking=True), y_val_batch.to(device,
                                                                                                                 non_blocking=True)
                            with autocast():
                                val_outputs = model(X_val_batch)
                                val_loss += criterion(val_outputs, y_val_batch).item()

                    avg_loss = epoch_loss / len(train_loader)
                    writer.add_scalar('Loss/Train', avg_loss, epoch)
                    avg_val_loss = val_loss / len(val_loader)
                    # Log validation loss and accuracy
                    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)
                    print(
                        f"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

                writer.close()

                model.eval()
                with torch.no_grad():
                    y_pred = model(X_val_tensor.to(device)).cpu().numpy()
                    if output_dim == 1:
                        y_pred = np.array(y_pred).squeeze()
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            if output_dim == 1:
                y_pred = np.array(y_pred).squeeze()

        if regression_type_combo in ["Keras Regressor", "Hyper Parameter"]:
            if hasattr(model, 'model'):
                model = model.model  # Extract the actual Keras model
            model.save(file_path)
            return y_pred.astype(dtype)

        elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype)

        return y_pred.astype(dtype), model

    def execute_regression(self, result_layout, regression_type_combo, selected_targets,
                           selected_features, dimensionality_reduction_type_combo, n_components, selected_parent_key,
                           selected_feature_input_keys, feature_input_names, selected_channel_texts):

        try:

            num_lstm_layers_min = None
            epochs = None
            max_trials = None
            num_lstm_layers_max = None
            num_lstm_layers_step = None
            lstm_units_min = None
            lstm_units_max = None
            lstm_units_step = None
            l2_lstm_min = None
            l2_lstm_max = None
            l2_lstm_step = None
            min_dropout_lstm = None
            max_dropout_lstm = None
            dropout_lstm_layer_step = None
            dropout_lstm_min = None
            dropout_lstm_max = None
            dropout_lstm_step = None
            num_conv_layers_min = None
            num_conv_layers_max = None
            num_conv_layers_step = None
            conv_filters_min = None
            conv_filters_max = None
            conv_filters_step = None
            conv_kernel_size_min = None
            conv_kernel_size_max = None
            conv_kernel_size_step = None
            l2_conv_min = None
            l2_conv_max = None
            l2_conv_step = None
            min_dropout_conv = None
            max_dropout_conv = None
            dropout_conv_layer_step = None
            dropout_conv_min = None
            dropout_conv_max = None
            dropout_conv_step = None
            num_attention_layers_min = None
            num_attention_layers_max = None
            num_attention_layers_step = None
            attention_heads_min = None
            attention_heads_max = None
            attention_heads_step = None
            attention_key_dim_min = None
            attention_key_dim_max = None
            attention_key_dim_step = None
            l2_attention_min = None
            l2_attention_max = None
            l2_attention_step = None
            min_dropout_attention = None
            max_dropout_attention = None
            dropout_attention_layer_step = None
            dropout_attention_min = None
            dropout_attention_max = None
            dropout_attention_step = None
            num_dense_layers_min = None
            num_dense_layers_max = None
            num_dense_layers_step = None
            dense_units_min = None
            dense_units_max = None
            dense_units_step = None
            l2_dense_min = None
            l2_dense_max = None
            l2_dense_step = None
            min_dropout_dense = None
            max_dropout_dense = None
            dropout_dense_layer_step = None
            dropout_dense_min = None
            dropout_dense_max = None
            dropout_dense_step = None
            learning_rate_min = None
            learning_rate_max = None
            learning_rate_step = None
            Loss_type_combo = None
            batch_size = None
            file_path = None
            log_dir = None

            def add_model_to_dict(model_dict=None, model_type=None, model=None, features=None, target=None,
                                  file_path=None,
                                  dimensionality_reduction_type_combo=None, n_components=None):
                """
                Adds a trained model, its metadata, and file path to the model dictionary.
                """
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": file_path,
                    "dim_type": dimensionality_reduction_type_combo,
                    "n_components": n_components
                }

                combined_key = f"{model_type}_{dimensionality_reduction_type_combo}"

                if combined_key not in model_dict:
                    model_dict[combined_key] = []

                model_dict[combined_key].append(model_metadata)

            dialog = QDialog(self)
            dialog.setWindowTitle("Train test split")
            layout = QVBoxLayout()

            label = QLabel("Choose the train test split")
            layout.addWidget(label)

            spinBox = QSpinBox()
            spinBox.setMinimum(1)
            spinBox.setMaximum(99)
            spinBox.setValue(20)
            layout.addWidget(spinBox)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Accepted:
                train_test = spinBox.value()
            else:
                return

            # Convert selected features and targets to numpy arrays
            X = np.array(selected_features).T
            y = np.array(selected_targets).T

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Standard Scaler", "Min-Max Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return

            normalization_choice = combo.currentText()

            if normalization_choice == "Min-Max Scaler":
                scaler = MinMaxScaler()
                X = scaler.fit_transform(X)
            elif normalization_choice == "Standard Scaler":
                scaler = StandardScaler()
                X = scaler.fit_transform(X)
            else:
                pass

            n_components = None
            # Create a new dialog to select linear or non-linear regression
            cross_plot_dialog = QDialog(self)
            cross_plot_dialog.setWindowTitle("Dimensionality Reduction Options")

            layout = QVBoxLayout(cross_plot_dialog)

            dimensionality_reduction_type_combo = QComboBox()
            dimensionality_reduction_type_combo.addItems([
                "None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"

            ])
            layout.addWidget(dimensionality_reduction_type_combo)

            select_button = QPushButton("OK")
            select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
            layout.addWidget(select_button)

            cross_plot_dialog.setLayout(layout)
            cross_plot_dialog.adjustSize()
            cross_plot_dialog.setMinimumWidth(250)
            # Disable the '?' help button on the dialog
            cross_plot_dialog.setWindowFlags(
                cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

            result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
            if result == QDialog.DialogCode.Accepted:
                dimensionality_reduction_type_combo = dimensionality_reduction_type_combo.currentText()

                if dimensionality_reduction_type_combo in ["t-SNE", "ICA", "Random Projection", "PCA",
                                                           "Autoencoder"]:

                    dialog = QDialog(self)
                    dialog.setWindowTitle("Principal Components")
                    layout = QVBoxLayout()

                    input_field_label = QLabel()
                    input_field_label.setText("Select the Number of Components:")
                    layout.addWidget(input_field_label)

                    input_field = QLineEdit()
                    # Using QDoubleValidator to allow decimals and integers
                    validator = QDoubleValidator(0, X.shape[1], 2, input_field)
                    validator.setNotation(QDoubleValidator.Notation.StandardNotation)
                    input_field.setValidator(validator)
                    layout.addWidget(input_field)

                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(225)
                    # Disable the '?' help button on the dialog
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        n_components = float(input_field.text())
                        if n_components.is_integer():
                            # Convert to int if the number is an integer
                            n_components = int(n_components)
                    else:
                        return
            else:
                return

            # Handle single target variable correctly
            if len(y.shape) == 1:
                y = y.reshape(-1, 1)

            # Split into training and validation sets
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=train_test / 100, random_state=42)

            # Get the number of epochs for certain regressors
            if regression_type_combo in ["Random Forest Regressor", "Gradient Boosting Regressor", "AdaBoost Regressor",
                                         "XGBoost Regressor", "CatBoost Regressor", "Keras Regressor", "ResNet",
                                         "TabNet",
                                         "TCN", "LightGBM Regressor", "Hyper Parameter"]:

                dialog = QDialog(self)
                dialog.setWindowTitle("Epochs")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of epochs")
                layout.addWidget(label)

                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(1000)
                spinBox.setValue(20)
                layout.addWidget(spinBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    epochs = spinBox.value()
                else:
                    return

                if regression_type_combo == "Hyper Parameter":

                    # Create a new dialog to select linear or non-linear regression
                    cross_plot_dialog = QDialog(self)
                    cross_plot_dialog.setWindowTitle("Loss Options")

                    layout = QVBoxLayout(cross_plot_dialog)

                    Loss_type_combo = QComboBox(self)
                    Loss_type_combo.addItems([
                        "mean_squared_error", "mean_absolute_error", "mean_absolute_percentage_error",
                        "mean_squared_logarithmic_error", "huber", "log_cosh", "cosine_similarity", "poisson"

                    ])
                    layout.addWidget(Loss_type_combo)

                    select_button = QPushButton("OK")
                    select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
                    layout.addWidget(select_button)

                    cross_plot_dialog.setLayout(layout)
                    cross_plot_dialog.adjustSize()
                    cross_plot_dialog.setMinimumWidth(250)
                    # Disable the '?' help button on the dialog
                    cross_plot_dialog.setWindowFlags(
                        cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                    result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
                    if result == QDialog.DialogCode.Accepted:
                        Loss_type_combo = Loss_type_combo.currentText()
                    else:
                        return

                    # Create the dialog
                    dialog = QDialog(self)
                    dialog.setWindowTitle("Hyperparameters")
                    layout = QGridLayout()  # Use a grid layout for better organization

                    # Add spin boxes for different hyperparameters
                    def add_spin_boxes(label_text, min_value, max_value, default_value, step=1, is_float=False):
                        label = QLabel(label_text)
                        label.setMinimumWidth(150)  # Set a minimum width for labels
                        min_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                        if is_float:
                            min_spin_box.setDecimals(6)
                        min_spin_box.setMinimum(min_value)
                        min_spin_box.setMaximum(max_value)
                        min_spin_box.setSingleStep(step)
                        min_spin_box.setValue(min_value)

                        max_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            max_spin_box.setDecimals(6)
                        max_spin_box.setMinimum(min_value)
                        max_spin_box.setMaximum(max_value)
                        max_spin_box.setSingleStep(step)
                        max_spin_box.setValue(max_value)

                        step_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            step_spin_box.setDecimals(6)
                        step_spin_box.setMinimum(0.000001 if is_float else 1)
                        step_spin_box.setMaximum(max_value - min_value)
                        step_spin_box.setSingleStep(step)
                        step_spin_box.setValue(step)

                        return label, min_spin_box, max_spin_box, step_spin_box

                    # Layout management function
                    def add_spin_boxes_to_grid(grid, start_row, column, *spinbox_data):
                        for i, (label, min_box, max_box, step_box) in enumerate(spinbox_data):
                            row = start_row + i * 4
                            grid.addWidget(label, row, column, 1, 2)
                            grid.addWidget(QLabel("Min"), row + 1, column)
                            grid.addWidget(min_box, row + 1, column + 1)
                            grid.addWidget(QLabel("Max"), row + 2, column)
                            grid.addWidget(max_box, row + 2, column + 1)
                            grid.addWidget(QLabel("Step"), row + 3, column)
                            grid.addWidget(step_box, row + 3, column + 1)

                    # LSTM layers
                    lstm_spin_boxes = [
                        add_spin_boxes("Number of LSTM Layers", 0, 100, 10),
                        add_spin_boxes("LSTM Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization LSTM", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of LSTM Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate LSTM", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Conv1D layers
                    conv_spin_boxes = [
                        add_spin_boxes("Number of Conv Layers", 0, 100, 10),
                        add_spin_boxes("Conv Filters", 16, 256, 128, step=32),
                        add_spin_boxes("Conv Kernel Size", 1, 5, 1),
                        add_spin_boxes("L2 Regularization Conv", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Conv Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Conv", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # MultiHeadAttention layers
                    attention_spin_boxes = [
                        add_spin_boxes("Number of Attention Layers", 0, 100, 10),
                        add_spin_boxes("Attention Heads", 2, 8, 4, step=2),
                        add_spin_boxes("Attention Key Dim", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Attention", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Attention Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Attention", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Dense layers
                    dense_spin_boxes = [
                        add_spin_boxes("Number of Dense Layers", 0, 100, 10),
                        add_spin_boxes("Dense Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Dense", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Dense Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Dense", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Add the spin boxes to the grid layout
                    add_spin_boxes_to_grid(layout, 0, 0, *lstm_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 2, *conv_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 4, *attention_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 6, *dense_spin_boxes)

                    # Learning rate
                    lr_label, lr_min, lr_max, lr_step = add_spin_boxes("Learning Rate", 1e-4, 1e-2, 1e-3, step=1e-4,
                                                                       is_float=True)
                    layout.addWidget(lr_label, 20, 0, 1, 2)
                    layout.addWidget(QLabel("Min"), 21, 0)
                    layout.addWidget(lr_min, 21, 1)
                    layout.addWidget(QLabel("Max"), 22, 0)
                    layout.addWidget(lr_max, 22, 1)
                    layout.addWidget(QLabel("Step"), 23, 0)
                    layout.addWidget(lr_step, 23, 1)

                    # Max trials
                    label_max_trials = QLabel("Maximum Trials")
                    spinBox_max_trials = QSpinBox(self)
                    spinBox_max_trials.setMinimum(0)
                    spinBox_max_trials.setMaximum(2000)
                    spinBox_max_trials.setValue(10)
                    layout.addWidget(label_max_trials, 24, 0, 1, 2)
                    layout.addWidget(spinBox_max_trials, 25, 0, 1, 2)

                    # Batch size
                    label_batch_size = QLabel("Batch Size")
                    spinBox_batch_size = QSpinBox(self)
                    spinBox_batch_size.setMinimum(32)
                    spinBox_batch_size.setMaximum(2048)
                    spinBox_batch_size.setValue(16)
                    layout.addWidget(label_batch_size, 26, 0, 1, 2)
                    layout.addWidget(spinBox_batch_size, 27, 0, 1, 2)

                    # OK button
                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button, 28, 0, 1, 2)

                    layout.setColumnStretch(1, 1)
                    layout.setColumnStretch(3, 1)
                    layout.setColumnStretch(5, 1)
                    layout.setColumnStretch(7, 1)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(800)  # Increase minimum width
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()

                    # Execute the dialog and get the values
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        # LSTM layer parameters
                        num_lstm_layers_min = lstm_spin_boxes[0][1].value()
                        num_lstm_layers_max = lstm_spin_boxes[0][2].value()
                        num_lstm_layers_step = lstm_spin_boxes[0][3].value()
                        lstm_units_min = lstm_spin_boxes[1][1].value()
                        lstm_units_max = lstm_spin_boxes[1][2].value()
                        lstm_units_step = lstm_spin_boxes[1][3].value()
                        l2_lstm_min = lstm_spin_boxes[2][1].value()
                        l2_lstm_max = lstm_spin_boxes[2][2].value()
                        l2_lstm_step = lstm_spin_boxes[2][3].value()
                        dropout_lstm_min = lstm_spin_boxes[4][1].value()
                        dropout_lstm_max = lstm_spin_boxes[4][2].value()
                        dropout_lstm_step = lstm_spin_boxes[4][3].value()
                        min_dropout_lstm = lstm_spin_boxes[3][1].value()
                        max_dropout_lstm = lstm_spin_boxes[3][2].value()
                        dropout_lstm_layer_step = lstm_spin_boxes[3][3].value()

                        # Conv layer parameters
                        num_conv_layers_min = conv_spin_boxes[0][1].value()
                        num_conv_layers_max = conv_spin_boxes[0][2].value()
                        num_conv_layers_step = conv_spin_boxes[0][3].value()
                        conv_filters_min = conv_spin_boxes[1][1].value()
                        conv_filters_max = conv_spin_boxes[1][2].value()
                        conv_filters_step = conv_spin_boxes[1][3].value()
                        conv_kernel_size_min = conv_spin_boxes[2][1].value()
                        conv_kernel_size_max = conv_spin_boxes[2][2].value()
                        conv_kernel_size_step = conv_spin_boxes[2][3].value()
                        l2_conv_min = conv_spin_boxes[3][1].value()
                        l2_conv_max = conv_spin_boxes[3][2].value()
                        l2_conv_step = conv_spin_boxes[3][3].value()
                        dropout_conv_min = conv_spin_boxes[5][1].value()
                        dropout_conv_max = conv_spin_boxes[5][2].value()
                        dropout_conv_step = conv_spin_boxes[5][3].value()
                        min_dropout_conv = conv_spin_boxes[4][1].value()
                        max_dropout_conv = conv_spin_boxes[4][2].value()
                        dropout_conv_layer_step = conv_spin_boxes[4][3].value()

                        # Attention layer parameters
                        num_attention_layers_min = attention_spin_boxes[0][1].value()
                        num_attention_layers_max = attention_spin_boxes[0][2].value()
                        num_attention_layers_step = attention_spin_boxes[0][3].value()
                        attention_heads_min = attention_spin_boxes[1][1].value()
                        attention_heads_max = attention_spin_boxes[1][2].value()
                        attention_heads_step = attention_spin_boxes[1][3].value()
                        attention_key_dim_min = attention_spin_boxes[2][1].value()
                        attention_key_dim_max = attention_spin_boxes[2][2].value()
                        attention_key_dim_step = attention_spin_boxes[2][3].value()
                        l2_attention_min = attention_spin_boxes[3][1].value()
                        l2_attention_max = attention_spin_boxes[3][2].value()
                        l2_attention_step = attention_spin_boxes[3][3].value()
                        dropout_attention_min = attention_spin_boxes[5][1].value()
                        dropout_attention_max = attention_spin_boxes[5][2].value()
                        dropout_attention_step = attention_spin_boxes[5][3].value()
                        min_dropout_attention = attention_spin_boxes[4][1].value()
                        max_dropout_attention = attention_spin_boxes[4][2].value()
                        dropout_attention_layer_step = attention_spin_boxes[4][3].value()

                        # Dense layer parameters
                        num_dense_layers_min = dense_spin_boxes[0][1].value()
                        num_dense_layers_max = dense_spin_boxes[0][2].value()
                        num_dense_layers_step = dense_spin_boxes[0][3].value()
                        dense_units_min = dense_spin_boxes[1][1].value()
                        dense_units_max = dense_spin_boxes[1][2].value()
                        dense_units_step = dense_spin_boxes[1][3].value()
                        l2_dense_min = dense_spin_boxes[2][1].value()
                        l2_dense_max = dense_spin_boxes[2][2].value()
                        l2_dense_step = dense_spin_boxes[2][3].value()
                        min_dropout_dense = dense_spin_boxes[3][1].value()
                        max_dropout_dense = dense_spin_boxes[3][2].value()
                        dropout_dense_layer_step = dense_spin_boxes[3][3].value()
                        dropout_dense_min = dense_spin_boxes[4][1].value()
                        dropout_dense_max = dense_spin_boxes[4][2].value()
                        dropout_dense_step = dense_spin_boxes[4][3].value()

                        # Other parameters
                        learning_rate_min = lr_min.value()
                        learning_rate_max = lr_max.value()
                        learning_rate_step = lr_step.value()
                        max_trials = spinBox_max_trials.value()
                        batch_size = spinBox_batch_size.value()
                    else:
                        return

            if regression_type_combo in ["Keras Regressor", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
                if regression_type_combo == "Keras Regressor":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "keras_regressor",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo == "Hyper Parameter":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "hyper_parameter_regressor",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo in ["TCN", "ResNet"]:
                    # TensorBoard setup
                    log_dir = f"logs/Pytorch/{regression_type_combo}_regressor/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"

            if regression_type_combo in ["Keras Regressor", "Hyper Parameter"]:
                # Check if it's a KerasRegressor and extract the underlying model
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model.h5"

            elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model.pth"

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir_dr = os.path.join("logs", "Tensorflow", "autoencoder_DR_regression_petro",
                                          datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir_dr = None

            result = self.task_runner.run_task(TensorVisualizer.run_petro_regression_task, X, y,
                                                       train_test, num_lstm_layers_min, regression_type_combo, epochs,
                                                       max_trials, num_lstm_layers_max, num_lstm_layers_step,
                                                       lstm_units_min,
                                                       lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                                       l2_lstm_step, min_dropout_lstm, max_dropout_lstm,
                                                       dropout_lstm_layer_step,
                                                       dropout_lstm_min, dropout_lstm_max, dropout_lstm_step,
                                                       num_conv_layers_min,
                                                       num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                                       conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                                       conv_kernel_size_max, conv_kernel_size_step, l2_conv_min,
                                                       l2_conv_max,
                                                       l2_conv_step, min_dropout_conv, max_dropout_conv,
                                                       dropout_conv_layer_step,
                                                       dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                                       num_attention_layers_min, num_attention_layers_max,
                                                       num_attention_layers_step,
                                                       attention_heads_min, attention_heads_max, attention_heads_step,
                                                       attention_key_dim_min, attention_key_dim_max,
                                                       attention_key_dim_step,
                                                       l2_attention_min, l2_attention_max, l2_attention_step,
                                                       min_dropout_attention, max_dropout_attention,
                                                       dropout_attention_layer_step,
                                                       dropout_attention_min, dropout_attention_max,
                                                       dropout_attention_step,
                                                       num_dense_layers_min, num_dense_layers_max,
                                                       num_dense_layers_step,
                                                       dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                                       l2_dense_max, l2_dense_step, min_dropout_dense,
                                                       max_dropout_dense,
                                                       dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                                       dropout_dense_step, learning_rate_min, learning_rate_max,
                                                       learning_rate_step, Loss_type_combo, log_dir, batch_size,
                                                       file_path,
                                                       log_dir_dr, dimensionality_reduction_type_combo, n_components)

            if result is not None:

                if regression_type_combo in ["Keras Regressor", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    y_pred = result
                else:
                    y_pred, model = result

                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir_dr)

                if regression_type_combo in ["Keras Regressor", "ResNet", "TCN", "Hyper Parameter"]:
                    self.show_tensorboard(log_dir=log_dir)

                if regression_type_combo in ["Keras Regressor", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.petro_models, model_type=regression_type_combo, model=None,
                                      features=feature_input_names, target=selected_channel_texts, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                else:  # Assume Sklearn or similar
                    # Ask the user if they want to save the model
                    reply = QMessageBox.question(self, 'Save Model', 'Do you want to save the model?',
                                                 QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                                                 QMessageBox.StandardButton.No)

                    if reply == QMessageBox.StandardButton.Yes:
                        file_dialog = QFileDialog()
                        file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                        file_path = f"{file_directory}/{regression_type_combo}model.pkl"
                        joblib.dump(model, file_path)
                    else:
                        file_path = None
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.petro_models, model_type=regression_type_combo, model=model,
                                      features=feature_input_names, target=selected_channel_texts, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                self.plot_petro_results(y_val, y_pred, regression_type_combo, result_layout, X, train_test,
                                        selected_parent_key,
                                        selected_feature_input_keys, selected_channel_texts)
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_petro_results(self, y_actual, y_pred, regression_type, result_layout, X_clean_copy, train_test,
                           selected_parent_key, selected_feature_input_keys, selected_channel_texts):
        """
        Plot the predicted values against the actual values for regression analysis.
        """

        def create_selection_dialog(title, items):
            dialog = QDialog(self)
            dialog.setWindowTitle(title)

            layout = QVBoxLayout(dialog)
            list_widget = QListWidget()
            list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)

            for item in items:
                list_widget.addItem(item)

            layout.addWidget(list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(dialog.accept)
            layout.addWidget(select_button)

            dialog.setLayout(layout)
            if dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return  # Use exec() to block until the dialog is closed

            selected_items = list_widget.selectedItems()
            if len(selected_items) == 0:
                QMessageBox.warning(self, "Warning", f"Please select an item from the {title} list.")
                return None

            return selected_items[0].text()

        # Select target variable
        selected_target_name = create_selection_dialog("Select Target Variable", selected_channel_texts)
        if selected_target_name is None:
            return

        # Map the selected target name back to its index
        selected_target_idx = selected_channel_texts.index(selected_target_name)

        y_pred_selected = y_pred[:, selected_target_idx] if y_pred.ndim > 1 else y_pred
        y_actual_selected = y_actual[:, selected_target_idx] if y_actual.ndim > 1 else y_actual

        # Prepare feature input names

        selected_feature_input_keys = []
        for key in selected_feature_input_keys:
            key_name = self.metadata[key].get('template')
            selected_feature_input_keys.append(key_name)

        feature_input_names = [selected_parent_key] + selected_feature_input_keys

        # Select feature input for color coding
        selected_feature_name = create_selection_dialog("Select Feature Input for Color Coding", feature_input_names)
        if selected_feature_name is None:
            return

        # Map the selected feature name back to its index
        selected_feature_idx = feature_input_names.index(selected_feature_name)

        # Select feature input for color coding
        num_features = X_clean_copy.shape[1] if len(X_clean_copy.shape) > 1 else 1

        X_clean = X_clean_copy[:, selected_feature_idx].reshape(-1, 1) if num_features > 1 else X_clean_copy

        # Convert to numpy arrays
        X_clean = np.asarray(X_clean, dtype=np.float32)

        # Split into training and validation sets (80% train, 20% validation)
        X_train, X_val = train_test_split(X_clean, test_size=train_test / 100, random_state=42)

        # Normalize c_data for color mapping
        norm_c_data = (X_val - X_val.min()) / (X_val.max() - X_val.min())

        self.petro_color_data = norm_c_data.ravel()

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.petro_color_mapping, source='matplotlib')
        colors = cmap.map(self.petro_color_data, mode='qcolor')

        # Clear the previous plot and reset the view
        self.petro_widget.clear()
        self.petro_widget.autoRange()

        # Create scatter plot item for actual vs. predicted values
        scatter = pg.ScatterPlotItem(x=y_actual_selected, y=y_pred_selected, pen=None, brush=colors)
        self.petro_widget.addItem(scatter)

        # Optionally, you can add a line y=x to indicate perfect predictions
        diag_line = pg.PlotCurveItem(x=[min(y_actual_selected), max(y_actual_selected)],
                                     y=[min(y_actual_selected), max(y_actual_selected)],
                                     pen=pg.mkPen('r', style=Qt.PenStyle.DashLine))
        self.petro_widget.addItem(diag_line)

        # Find the unit for the selected target name
        target_unit = ""
        for key, units in self.units_dict.items():
            if selected_target_name in units:
                target_unit = units[selected_target_name]
                break

        # Set axis labels with units
        x_unit_label = f"Actual {selected_target_name} ({target_unit})"
        y_unit_label = f"Predicted {selected_target_name} ({target_unit})"

        self.petro_widget.setLabel('bottom', x_unit_label)  # x-axis label with unit
        self.petro_widget.setLabel('left', y_unit_label)  # y-axis label with unit

        # Define the units for feature inputs
        feature_units = {
            "Inst. Phase": "degrees",
            "Inst. Frequency": "Hz",
            "Inst. Bandwidth": "Hz",
            "Dom. Frequency": "Hz",
            "Cos Phase": "unitless",
            "Envelope": "unitless",
            "Sweetness": "unitless",
            "App. Polarity": "unitless",
            "RMS Amplitude": "amp"
        }

        if selected_feature_name in feature_units:
            # Get the unit for the selected feature input
            feature_unit = feature_units.get(selected_feature_name, "unknown")
            if feature_unit == "unitless":
                color_bar_name = f"{selected_feature_name}"
            else:
                color_bar_name = f"{selected_feature_name} ({feature_unit})"
        else:
            feature_unit = "amp"
            selected_feature_name_plot = "Seismic"
            color_bar_name = f"{selected_feature_name_plot} ({feature_unit})"

        # Create or update the ColorBarItem for the color bar
        if hasattr(self, 'petro_color_bar') and self.petro_color_bar is not None:
            self.petro_color_bar.setLevels((X_val.min(), X_val.max()))
            self.petro_widget.getPlotItem().layout.removeItem(self.color_bar_label_item)
            self.petro_color_bar.label = color_bar_name
            self.petro_color_bar.setColorMap(cmap)
        else:
            color_bar = pg.ColorBarItem(values=(X_val.min(), X_val.max()))
            color_bar.setColorMap(cmap)
            color_bar.label = color_bar_name
            self.petro_color_bar = color_bar
            self.petro_widget.getPlotItem().layout.addItem(self.petro_color_bar, 2, 2)

        # Add vertical label for the color bar with units
        self.color_bar_label_item = pg.LabelItem(text=color_bar_name, angle=90,
                                                 color='k', size='10pt')
        self.petro_widget.getPlotItem().layout.addItem(self.color_bar_label_item, 2, 2, 1, 1)

        if selected_feature_idx == 0:
            self.petro_widget.getPlotItem().layout.setContentsMargins(0, 0, 40,
                                                                      0)  # Adjust the values as needed
        else:
            self.petro_widget.getPlotItem().layout.setContentsMargins(0, 0, 30,
                                                                      0)  # Adjust the values as needed

        self.petro_widget.autoRange()

        def robust_mape(y_true, y_pred, epsilon=1e-10, max_error=None):
            """
            Calculate the Mean Absolute Percentage Error (MAPE) with robust handling of edge cases.

            Parameters:
            - y_true: array-like, true values.
            - y_pred: array-like, predicted values.
            - epsilon: small value to replace zeros or very small values in y_true to prevent division by zero (default: 1e-10).
            - max_error: optional, maximum percentage error to clip extreme values (e.g., 1000 for capping at 1000%).

            Returns:
            - A string with the computed MAPE as a percentage or an error message if calculation is not possible.
            """
            try:
                # Convert inputs to numpy arrays for element-wise operations
                y_true = np.array(y_true)
                y_pred = np.array(y_pred)

                # Check for NaNs or Infs in the inputs
                if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)):
                    return "Error: Input arrays contain NaNs."
                if np.any(np.isinf(y_true)) or np.any(np.isinf(y_pred)):
                    return "Error: Input arrays contain Infs."

                # Replace zeros or very small values in y_true with epsilon to prevent division by zero
                y_true_safe = np.where(np.abs(y_true) < epsilon, epsilon, y_true)

                # Compute the absolute percentage errors
                percentage_errors = np.abs((y_true - y_pred) / y_true_safe) * 100

                # Optionally clip extreme percentage errors if max_error is provided
                if max_error is not None:
                    percentage_errors = np.clip(percentage_errors, 0, max_error)

                # Calculate the mean of the absolute percentage errors
                mape = np.mean(percentage_errors)

                return f"MAPE: {mape:.2f}%"

            except Exception as e:
                QMessageBox.critical(self, "Error", f"Error: {str(e)}")

        # Optionally, display statistics in the result layout
        result_text = f"{regression_type} Results:\n"
        result_text += f"Correlation Coefficient: {pearsonr(y_actual_selected, y_pred_selected)[0]:.2f}\n"
        result_text += f"R-squared Score: {r2_score(y_actual_selected, y_pred_selected):.2f}\n"
        result_text += f"MSE: {mean_squared_error(y_actual_selected, y_pred_selected):.6f}\n"
        result_text += f"RMSE: {root_mean_squared_error(y_actual_selected, y_pred_selected):.6f}\n"
        result_text += f"MAE: {mean_absolute_error(y_actual_selected, y_pred_selected):.6f}\n"
        result_text += robust_mape(y_actual_selected, y_pred_selected, epsilon=1e-10, max_error=1000)

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def prepare_for_training(self, result_layout, analysis_type):

        def filter_and_correct_data(feature_inputs, target_variables, placeholder_value=-999.25):
            """
            Filters out placeholder values from the target variables and removes the corresponding data points
            from both target variables and feature inputs.

            Parameters:
            - feature_inputs: list of np.ndarray
                List containing the feature input tensors.
            - target_variables: list of np.ndarray
                List containing the target variable tensors.
            - placeholder_value: float
                The placeholder value to be filtered out from the target variables (default is -999.25).

            Returns:
            - filtered_feature_inputs: list of np.ndarray
                List containing the filtered feature input tensors.
            - filtered_target_variables: list of np.ndarray
                List containing the filtered target variable tensors.
            """

            # Flatten all the tensors
            flattened_feature_inputs = [tensor.flatten() for tensor in feature_inputs]
            flattened_target_variables = [tensor.flatten() for tensor in target_variables]

            # Initialize mask for valid data points
            valid_mask = np.ones(flattened_target_variables[0].shape, dtype=bool)

            # Identify and create mask for placeholder values in each target variable
            for target in flattened_target_variables:
                valid_mask &= (target != placeholder_value)

            # Apply mask to filter out invalid data points from both feature inputs and target variables
            filtered_feature_inputs = [tensor[valid_mask] for tensor in flattened_feature_inputs]
            filtered_target_variables = [tensor[valid_mask] for tensor in flattened_target_variables]

            # Calculate min, max, and shape for filtered feature inputs
            min_feature_inputs = [np.min(tensor) for tensor in filtered_feature_inputs]
            max_feature_inputs = [np.max(tensor) for tensor in filtered_feature_inputs]
            shape_feature_inputs = [tensor.shape for tensor in filtered_feature_inputs]

            # Calculate min, max, and shape for filtered target variables
            min_target_variables = [np.min(tensor) for tensor in filtered_target_variables]
            max_target_variables = [np.max(tensor) for tensor in filtered_target_variables]
            shape_target_variables = [tensor.shape for tensor in filtered_target_variables]

            # Return the calculated values along with the filtered data
            return (filtered_feature_inputs, filtered_target_variables,
                    min_feature_inputs, max_feature_inputs, shape_feature_inputs,
                    min_target_variables, max_target_variables, shape_target_variables)

        def confirm_selection(selected_parent_key, selected_feature_input_keys, selected_child_key, selected_channels,
                              selected_channel_texts):
            # Step 5: Set feature input and target variables
            # Create copies of the data
            feature_inputs = [np.copy(self.tensor_dict[selected_parent_key])] + \
                             [np.copy(self.tensor_dict[key]) for key in selected_feature_input_keys]

            child_tensor = np.copy(self.tensor_dict[selected_child_key])

            target_variables = [child_tensor[..., ch] for ch in selected_channels]

            # Print confirmation
            feature_input_names = [selected_parent_key] + selected_feature_input_keys

            # Set the selected tensors for the next step (e.g., neural network training)
            self.feature_inputs = feature_inputs
            self.target_variables = target_variables

            filtered_data = filter_and_correct_data(feature_inputs, target_variables)

            dimensionality_reduction_type_combo = None

            n_components = None

            # Create a new dialog to select linear or non-linear regression
            cross_plot_dialog = QDialog(self)
            if analysis_type == "Classification":
                cross_plot_dialog.setWindowTitle("Classification Options")
            else:
                cross_plot_dialog.setWindowTitle("Regression Options")

            layout = QVBoxLayout(cross_plot_dialog)

            regression_type_combo = QComboBox()
            if analysis_type == "Regression":
                regression_type_combo.addItems([
                    "Ridge Regression",
                    "Lasso Regression",
                    "Bayesian Ridge Regression",
                    "Random Forest Regressor",
                    "Support Vector Regressor",
                    "K-Nearest Neighbors Regressor",
                    "Decision Tree Regressor",
                    "Gaussian Process Regressor",
                    "XGBoost Regressor",
                    "LightGBM Regressor",
                    "CatBoost Regressor",
                    "Gradient Boosting Regressor",
                    "AdaBoost Regressor",
                    "Keras Regressor",
                    "ResNet",
                    "TabNet",
                    "TCN",
                    "Hyper Parameter"
                ])
            else:
                regression_type_combo.addItems([
                    "Random Forest Classifier",
                    "Gradient Boosting Classifier",
                    "Support Vector Classifier",
                    "K-Nearest Neighbors Classifier",
                    "Decision Tree Classifier",
                    "Gaussian Process Classifier",
                    "CatBoost Classifier",
                    "LightGBM Classifier",
                    "Naive Bayes Classifier",
                    "Extra Trees Classifier",
                    "Bagging Classifier",
                    "Voting Classifier",
                    "Keras Classifier",
                    "ResNet",
                    "TabNet",
                    "TCN",
                    "Hyper Parameter"
                ])
            layout.addWidget(regression_type_combo)

            select_button = QPushButton("OK")
            select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
            layout.addWidget(select_button)

            cross_plot_dialog.setLayout(layout)
            cross_plot_dialog.adjustSize()
            cross_plot_dialog.setMinimumWidth(225)
            # Disable the '?' help button on the dialog
            cross_plot_dialog.setWindowFlags(
                cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

            result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
            if result == QDialog.DialogCode.Accepted:
                regression_type_combo = regression_type_combo.currentText()
            else:
                return

            if analysis_type == "Regression":
                self.execute_regression(result_layout, regression_type_combo,
                                        filtered_data[1],
                                        filtered_data[0], dimensionality_reduction_type_combo, n_components,
                                        selected_parent_key, selected_feature_input_keys, feature_input_names,
                                        selected_channel_texts)
            else:
                self.execute_categorization(result_layout, regression_type_combo,
                                            filtered_data[1],
                                            filtered_data[0], dimensionality_reduction_type_combo, n_components,
                                            selected_parent_key, selected_feature_input_keys, feature_input_names,
                                            selected_channel_texts)

        def select_child(selected_parent_key, selected_feature_input_keys, selected_child_key):
            # Step 4: Display channels of the selected child tensor in a dialog
            child_tensor = np.copy(self.tensor_dict[selected_child_key])
            num_channels = child_tensor.shape[-1]

            # Find the position of 'Upscaled'
            upscaled_pos = selected_child_key.find('Upscaled')

            # Get the substring after 'Upscaled'
            after_upscaled = selected_child_key[upscaled_pos + len('Upscaled'):].strip()

            # Split by spaces and filter out empty strings
            selected_target_names = [word for word in after_upscaled.split() if word]

            # Assume we have a dictionary that maps channel indices to names
            channel_names_dict = {j: channel_name for j, channel_name in enumerate(selected_target_names)}

            channel_list_widget_dialog = QDialog(self)
            channel_list_widget_dialog.setWindowTitle("Select Channels")

            layout = QVBoxLayout(channel_list_widget_dialog)
            channel_list_widget = QListWidget()
            channel_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)

            # Display channel names instead of "Channel 1", "Channel 2", etc.
            for ix in range(num_channels):
                channel_name = channel_names_dict.get(ix, f"Channel {ix + 1}")
                channel_list_widget.addItem(channel_name)

            layout.addWidget(channel_list_widget)

            ok_button = QPushButton("OK")
            ok_button.clicked.connect(channel_list_widget_dialog.accept)
            layout.addWidget(ok_button)

            channel_list_widget_dialog.setLayout(layout)
            if channel_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return

            # Get the text of the selected items
            selected_channel_texts = [item.text() for item in channel_list_widget.selectedItems()]

            # Map selected names back to their respective indices
            selected_channels = [list(channel_names_dict.values()).index(item.text()) for item in
                                 channel_list_widget.selectedItems()]
            confirm_selection(selected_parent_key, selected_feature_input_keys, selected_child_key, selected_channels,
                              selected_channel_texts)

        def select_feature_inputs(selected_parent_key):
            # Get children of the selected parent key
            child_keys = []
            parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                selected_parent_key].get('source')
            for key, value in self.metadata.items():
                name_or_source = value.get('name') or value.get('source')
                template = value.get('template').strip()
                if name_or_source == parent_name_or_source and template != "Seismic" and not template.startswith('Upscaled'):
                    child_keys.append(key)

            # Display child keys of the selected parent key for feature input selection in a dialog
            feature_input_list_widget_dialog = QDialog(self)
            feature_input_list_widget_dialog.setWindowTitle("Select Feature Inputs")

            layout = QVBoxLayout(feature_input_list_widget_dialog)
            feature_input_list_widget = QListWidget()
            feature_input_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
            for key in child_keys:
                child_name = self.metadata[key].get('template').strip()
                feature_input_list_widget.addItem(child_name)
            layout.addWidget(feature_input_list_widget)

            ok_button = QPushButton("OK")
            ok_button.clicked.connect(feature_input_list_widget_dialog.accept)
            layout.addWidget(ok_button)

            feature_input_list_widget_dialog.setLayout(layout)

            if child_keys:
                if feature_input_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                    QApplication.restoreOverrideCursor()
                    return

            selected_feature_input_keys = []
            for item in feature_input_list_widget.selectedItems():
                for key in child_keys:
                    if self.metadata[key].get('template').strip() == item.text():
                        selected_feature_input_keys.append(key)

            select_target_child(selected_parent_key, selected_feature_input_keys)

        def select_target_child(selected_parent_key, selected_feature_input_keys):
            # Get children of the selected parent key
            child_keys = []
            parent_name_or_source = self.metadata[selected_parent_key].get('name') or self.metadata[
                selected_parent_key].get('source')
            for key, value in self.metadata.items():
                name_or_source = value.get('name') or value.get('source')
                template = value.get('template').strip()
                if name_or_source == parent_name_or_source and template.startswith('Upscaled') and key not in selected_feature_input_keys:
                    child_keys.append(key)

            # Display child keys of the selected parent key for feature input selection in a dialog
            child_list_widget_dialog = QDialog(self)
            child_list_widget_dialog.setWindowTitle("Select Target Variable")

            layout = QVBoxLayout(child_list_widget_dialog)
            child_list_widget = QListWidget()
            child_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for key in child_keys:
                child_name = self.metadata[key].get('template').strip()
                child_list_widget.addItem(child_name)
            layout.addWidget(child_list_widget)

            ok_button = QPushButton("OK")
            ok_button.clicked.connect(child_list_widget_dialog.accept)
            layout.addWidget(ok_button)

            child_list_widget_dialog.setLayout(layout)

            if child_keys:
                if child_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
                    QApplication.restoreOverrideCursor()
                    return

            selected_child_name = child_list_widget.currentItem().text()

            for key in child_keys:
                if self.metadata[key].get('template').strip() == selected_child_name:
                    selected_child_key = key
                    select_child(selected_parent_key, selected_feature_input_keys, selected_child_key)
                    break

        # Step 1: Extract keys from self.tensor_dict that correspond to NumPy tensors
        tensor_keys = [key for key, value in self.tensor_dict.items() if isinstance(value, np.ndarray)]

        # Step 2: Get parent keys from the QTreeWidget
        parent_keys = []
        root = self.treeWidget.invisibleRootItem()
        child_count = root.childCount()

        for i in range(child_count):
            parent_item = root.child(i)
            parent_key = parent_item.text(0)
            if parent_key in tensor_keys:
                parent_keys.append(parent_key)

        # Display parent keys for selection in a dialog
        parent_list_widget_dialog = QDialog(self)
        parent_list_widget_dialog.setWindowTitle("Select Parent")

        layout = QVBoxLayout(parent_list_widget_dialog)
        parent_list_widget = QListWidget()
        parent_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
        for key in parent_keys:
            parent_list_widget.addItem(key)
        layout.addWidget(parent_list_widget)

        ok_button = QPushButton("OK")
        ok_button.clicked.connect(parent_list_widget_dialog.accept)
        layout.addWidget(ok_button)

        parent_list_widget_dialog.setLayout(layout)
        if parent_list_widget_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        selected_parent_key = parent_list_widget.currentItem().text()
        select_feature_inputs(selected_parent_key)

    def parse_deviation_file(self):
        dialog = QFileDialog()
        options = dialog.options()
        file_path, _ = QFileDialog.getOpenFileName(self, "Open Deviation File", "", "All Files (*)", options=options)

        if not file_path:
            return

        with open(file_path, 'r') as file:
            lines = file.readlines()

        # Metadata extraction
        metadata = {}
        for line in lines:
            if line.startswith('# WELL NAME:'):
                metadata['Deviation Well Name'] = line.split(':')[1].strip()
            elif line.startswith('# WELL HEAD X-COORDINATE:'):
                metadata['Well Head X Coordinate'] = float(re.findall(r"[-+]?\d*\.\d+|\d+", line.split(':')[1])[0])
            elif line.startswith('# WELL HEAD Y-COORDINATE:'):
                metadata['Well Head Y Coordinate'] = float(re.findall(r"[-+]?\d*\.\d+|\d+", line.split(':')[1])[0])
            elif line.startswith('# WELL DATUM'):
                metadata['Well Datum'] = float(re.findall(r"[-+]?\d*\.\d+|\d+", line.split(':')[1])[0])
            elif line.startswith('# WELL TYPE:'):
                metadata['Well Type'] = line.split(':')[1].strip()
            elif line.startswith('# DEFINITIVE SURVEY:'):
                metadata['Definitive Survey'] = line.split(':')[1].strip()
            elif 'COORDINATE SYSTEM' in line:
                metadata['Coordinate System'] = line.split('COORDINATE SYSTEM', 1)[1].strip()
            elif 'MD AND TVD ARE REFERENCED (' in line:
                match = re.search(r'=\s*([\d.]+)\)\s*AT\s*(.+)', line)
                if match:
                    metadata['MD and TVD Reference'] = f"{match.group(1)}, {match.group(2)}"
            elif 'ANGLES ARE GIVEN IN' in line:
                metadata['Angle Unit'] = line.split('IN', 1)[1].strip().split()[0]  # Extracts only "DEGREES"
            elif 'AZIM_TN:' in line:
                metadata['Azimuth TN Reference'] = line.split('in', 1)[1].strip()
            elif 'AZIM_GN:' in line:
                metadata['Azimuth GN Reference'] = line.split('in', 1)[1].strip()
            elif 'DX DY ARE GIVEN IN' in line:
                match = re.search(r'IN\s+(\w+\s+\w+)\s+IN\s+(\w+)-UNITS', line)  # Extracts "GRID NORTH" and "m"
                if match:
                    metadata['DX DY Reference'] = match.group(1)
                    metadata['DX DY Unit'] = match.group(2)
            elif 'DEPTH' in line and 'GIVEN IN' in line:
                match = re.search(r'IN\s+(\w+)-', line)
                if match:
                    metadata['Depth Unit'] = match.group(1)

        # Data table extraction
        data_table = []
        headers = []
        in_table = False
        for line in lines:
            if line.startswith(
                    '#==============================================================================================================================================='):
                in_table = True
                continue
            if in_table:
                if line.startswith('#'):
                    continue
                if not headers:
                    headers = line.split()
                else:
                    values = line.split()
                    data_entry = {headers[i]: float(values[i]) for i in range(len(values))}
                    data_table.append(data_entry)

        metadata['Deviation Data'] = pd.DataFrame(data_table)
        self.well_deviation_dict[self.file_name] = metadata
        QMessageBox.information(self, "Loading Completed", "The Deviation File has been loaded successfully.")

    def save_to_las(self):
        """
        Save data to LAS file format combining metadata, curve information, and data.
        """

        # Open a QFileDialog for the user to enter the CSV file path
        output_path, _ = QFileDialog.getSaveFileName(self, 'Save LAS', '', 'LAS Files (*.las)')

        if output_path:
            # Initialize the LAS file content
            las_content = []

            # Get the specific dictionaries for this file using self.filename as key
            well_info = self.well_header_info_library[self.file_name]
            units_info = self.units_dict[self.file_name]

            # Version section
            las_content.extend([
                "# LAS format log file from SeismicFlow",
                "# Project units are specified as depth units",
                "#==================================================================",
                "~Version information",
                "VERS.   2.0:",
                "WRAP.   NO:",
                "#==================================================================",
            ])

            # Well section
            las_content.append("~Well")

            # Function to extract values dynamically
            def extract_well_info(well_info):
                STRT_value, STOP_value, STEP_value = '', '', ''
                unit = 'm'  # Default unit if not found

                for key in well_info.keys():
                    match = re.match(r'^(STRT|STOP|STEP) \((.*?)\)$', key)
                    if match:
                        prefix, found_unit = match.groups()
                        unit = found_unit  # Use the unit found in the key
                        if prefix == 'STRT':
                            STRT_value = well_info.get(key, '')
                        elif prefix == 'STOP':
                            STOP_value = well_info.get(key, '')
                        elif prefix == 'STEP':
                            STEP_value = well_info.get(key, '')

                return STRT_value, STOP_value, STEP_value, unit

            # Extract values
            STRT_value, STOP_value, STEP_value, stunit = extract_well_info(well_info)
            # Handle NULL separately as it has no unit in parentheses
            null_value = well_info.get('NULL', '-999.250000')
            # Handle COMP - check if exists in dictionary
            comp_value = well_info.get('COMP', '')
            # Handle WELL with the well name
            well_name = well_info.get('Well Name', '')
            # Handle FLD - check if exists in dictionary
            fld_value = well_info.get('FLD', '')
            # Handle LOC - check if exists in dictionary
            loc_value = well_info.get('LOC', '')
            # Handle SRVC - check if exists in dictionary
            srvc_value = well_info.get('SRVC', '')
            # Handle DATE with special format
            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            # Handle PROV - check if exists in dictionary
            prov_value = well_info.get('PROV', '')
            # Handle UWI - check if exists in dictionary
            uwi_value = well_info.get('UWI', '')
            # Handle API - check if exists in dictionary
            api_value = well_info.get('API', '')

            dialog = QDialog(self)
            dialog.setWindowTitle("Edit Well Information")
            layout = QVBoxLayout()

            # Data for the table
            entries = [
                ("UNIT", stunit),
                ("STRT", STRT_value),
                ("STOP", STOP_value),
                ("STEP", STEP_value),
                ("NULL", null_value),
                ("COMP", comp_value),
                ("WELL", well_name),
                ("FLD", fld_value),
                ("LOC", loc_value),
                ("SRVC", srvc_value),
                ("DATE", current_datetime),
                ("PROV", prov_value),
                ("UWI", uwi_value),
                ("API", api_value),
            ]

            # Create a table with two columns: Entry Name and Value
            table = QTableWidget(dialog)
            table.setColumnCount(2)
            table.setRowCount(14)
            table.setHorizontalHeaderLabels(["Entry", "Value"])
            table.horizontalHeader().setStretchLastSection(True)

            # Populate the table
            for row, (key, value) in enumerate(entries):
                entry_item = QTableWidgetItem(key)
                entry_item.setFlags(entry_item.flags() ^ Qt.ItemFlag.ItemIsEditable)  # Make entry names non-editable
                table.setItem(row, 0, entry_item)
                value_item = QTableWidgetItem(str(value))
                table.setItem(row, 1, value_item)

            layout.addWidget(table)

            # OK and Cancel buttons
            button_box = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
            layout.addWidget(button_box)

            dialog.setLayout(layout)

            # Connect the buttons
            button_box.accepted.connect(dialog.accept)
            button_box.rejected.connect(dialog.reject)

            # Execute the dialog and handle the result
            if dialog.exec() == QDialog.DialogCode.Rejected:
                return

            # Update only changed values from the dialog
            for row, (key, original_value) in enumerate(entries):
                new_value = table.item(row, 1).text()
                if new_value != str(original_value):  # Update only if the value has changed
                    if key == "UNIT":
                        stunit = new_value
                    elif key == "STRT":
                        STRT_value = new_value
                    elif key == "STOP":
                        STOP_value = new_value
                    elif key == "STEP":
                        STEP_value = new_value
                    elif key == "NULL":
                        null_value = new_value
                    elif key == "COMP":
                        comp_value = new_value
                    elif key == "WELL":
                        well_name = new_value
                    elif key == "FLD":
                        fld_value = new_value
                    elif key == "LOC":
                        loc_value = new_value
                    elif key == "SRVC":
                        srvc_value = new_value
                    elif key == "DATE":
                        current_datetime = new_value
                    elif key == "PROV":
                        prov_value = new_value
                    elif key == "UWI":
                        uwi_value = new_value
                    elif key == "API":
                        api_value = new_value

            # Append content
            las_content.append(f"STRT .{stunit}      {STRT_value} :")
            las_content.append(f"STOP .{stunit}      {STOP_value} :")
            las_content.append(f"STEP .{stunit}      {STEP_value} :")
            las_content.append(f"NULL .        {null_value} :")
            las_content.append(f"COMP.  {comp_value}   : COMPANY")
            las_content.append(f"WELL.  {well_name}   : WELL")
            las_content.append(f"FLD.  {fld_value}   : FIELD")
            las_content.append(f"LOC.  {loc_value}   : LOCATION")
            las_content.append(f"SRVC.  {srvc_value}   : SERVICE COMPANY")
            las_content.append(f"DATE.  {current_datetime}   : Log Export Date {{yyyy-MM-dd HH:mm:ss}}")
            las_content.append(f"PROV.  {prov_value}   : PROVINCE")
            las_content.append(f"UWI.  {uwi_value}   : UNIQUE WELL ID")
            las_content.append(f"API.  {api_value}   : API NUMBER")

            # Dialog for editing curve units
            unit_dialog = QDialog(self)
            unit_dialog.setWindowTitle("Edit Units")
            unit_layout = QVBoxLayout()

            # Table for curve names and units
            unit_table = QTableWidget(unit_dialog)
            unit_table.setColumnCount(2)
            unit_table.setRowCount(len(units_info))
            unit_table.setHorizontalHeaderLabels(["Log", "Unit"])
            unit_table.horizontalHeader().setStretchLastSection(True)

            # Populate the table with existing units
            for row, (col_name, unit) in enumerate(units_info.items()):
                curve_item = QTableWidgetItem(col_name)
                curve_item.setFlags(curve_item.flags() ^ Qt.ItemFlag.ItemIsEditable)  # Make curve names non-editable
                unit_table.setItem(row, 0, curve_item)

                unit_item = QTableWidgetItem(unit)
                unit_table.setItem(row, 1, unit_item)

            unit_layout.addWidget(unit_table)

            # OK and Cancel buttons
            unit_button_box = QDialogButtonBox(
                QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
            unit_layout.addWidget(unit_button_box)

            unit_dialog.setLayout(unit_layout)

            # Connect buttons
            unit_button_box.accepted.connect(unit_dialog.accept)
            unit_button_box.rejected.connect(unit_dialog.reject)

            # Execute the dialog and handle the result
            if unit_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            # Update only changed units
            for row in range(unit_table.rowCount()):
                curve_name = unit_table.item(row, 0).text()
                new_unit = unit_table.item(row, 1).text()
                if new_unit != units_info[curve_name]:
                    units_info[curve_name] = new_unit

            # Append curve section with updated units
            las_content.extend([
                "#==================================================================",
                "~Curve"
            ])

            for col_name, unit in units_info.items():
                formatted_unit = f"{unit:<20}" if unit.startswith('.') else f".{unit:<19}"
                las_content.append(f"{col_name} {formatted_unit}: {col_name}")

            # Parameter section
            las_content.extend([
                "~Parameter",
                "#==================================================================",
            ])

            # ASCII section - data from pandas DataFrame
            las_content.append("~Ascii")

            # Format data rows
            formatted_data = self.tensor_data.fillna(float(null_value)).to_string(
                header=False,
                index=False,
                float_format=lambda x: f"{x:>12.7f}",
                col_space=12
            )

            # Add formatted data
            las_content.append(formatted_data)

            # Write to file
            with open(output_path, 'w') as f:
                f.write('\n'.join(las_content))

    @staticmethod
    def save_tensor_compress_task(tensor, file_path, metadata):
        np.savez_compressed(file_path, tensor=tensor, metadata=metadata)
        return True

    def save_tensor(self):
        try:
            if isinstance(self.tensor_data, np.ndarray):

                # Get the file path from the user using a file dialog window
                file_path, _ = QFileDialog.getSaveFileName(self, "Save File", f"{self.file_name}.npz",
                                                           "NumPy Files (*.npz)")

                # Check if the user selected a file path
                if file_path:

                    dialog = QDialog(self)
                    dialog.setWindowTitle("Save Options")
                    layout = QVBoxLayout()

                    label = QLabel("Select Save Option:")
                    layout.addWidget(label)

                    combo = QComboBox()
                    combo.addItems(['Standard', 'Compressed'])
                    layout.addWidget(combo)

                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(225)
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()

                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        QApplication.setOverrideCursor(self.custom_cursor)
                        compress = combo.currentText() == 'Compressed'

                        if compress:
                            # Compute total sample count
                            num_samples = np.int64(self.tensor_data.nbytes)

                            if num_samples <= self.multiprocessing_threshold * 0.85 * 4:
                                result = TensorVisualizer.save_tensor_compress_task(self.tensor_data, file_path,
                                                                                    self.metadata[self.file_name])
                            else:
                                QApplication.restoreOverrideCursor()
                                result = self.task_runner.run_task(TensorVisualizer.save_tensor_compress_task,
                                                                   self.tensor_data, file_path,
                                                                   self.metadata[self.file_name])

                            if result is not None:
                                self.loaded_file_paths[self.file_name] = file_path
                                self.update_recent_files_submenu()  # Update the recent files submenu
                                QApplication.restoreOverrideCursor()
                                QMessageBox.information(self, "Save Confirmation",
                                                        f"Tensor saved successfully as {combo.currentText().lower()}")

                        else:
                            np.savez(file_path, tensor=self.tensor_data, metadata=self.metadata[self.file_name])
                            self.loaded_file_paths[self.file_name] = file_path
                            self.update_recent_files_submenu()  # Update the recent files submenu
                            QApplication.restoreOverrideCursor()
                            QMessageBox.information(self, "Save Confirmation",
                                                    f"Tensor saved successfully as {combo.currentText().lower()}")

            else:
                # Open a QFileDialog for the user to enter the CSV file path
                csv_file_path, _ = QFileDialog.getSaveFileName(self, 'Save CSV', '', 'CSV Files (*.csv)')

                if csv_file_path:
                    # Save the DataFrame to the specified path
                    self.tensor_data.to_csv(csv_file_path, index=False)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def analysis(self):
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            dialog = QDialog(self)
            dialog.resize(1000, 600)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint | Qt.WindowType.WindowMinMaxButtonsHint)
            dialog.setWindowIcon(QIcon('icon.png'))  # Set the icon for the dialog
            dialog.setWindowTitle(f"Information - {self.file_name}")
            # Create a splitter for the left and right sections
            splitter = QSplitter(Qt.Orientation.Horizontal, dialog)

            # Left layout for statistics
            self.statsArea = QScrollArea(splitter)
            self.statsArea.setWidgetResizable(True)
            self.statsLayout = QVBoxLayout()
            statsWidget = QWidget(self)
            statsWidget.setLayout(self.statsLayout)
            self.statsArea.setWidget(statsWidget)

            # Right layout for histograms
            self.histogramArea = QScrollArea(splitter)
            self.histogramArea.setWidgetResizable(True)
            self.histogramLayout = QVBoxLayout()
            histogramWidget = QWidget(self)
            histogramWidget.setLayout(self.histogramLayout)
            self.histogramArea.setWidget(histogramWidget)

            # Add the splitter to the dialog's layout
            layout = QVBoxLayout(dialog)
            layout.addWidget(splitter)

            # Optionally set initial splitter sizes
            splitter.setSizes([300, 700])  # Adjust sizes as needed

            # Analyze the tensor and update stats and histograms
            self.analyze_tensor(self.tensor_data, self.statsLayout, self.histogramLayout)

            dialog.show()
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def analyze_tensor(self, tensor, statsLayout, histogramLayout):
        try:
            def add_editable_range_field(layout, label_text, values, key, metadata_dict):
                layout.addWidget(QLabel(f"<b>{label_text}</b>"))
                labels = ['X' if key in
                                 {'origin', 'inline_end', 'xline_end'} else 'Min',
                          'Y' if key in {'origin', 'inline_end', 'xline_end'} else 'Max',
                          'Delta'][:len(values)]  # Handle 2 or 3 elements

                # Create line edits for each value (min, max, delta)
                for i, (sub_label_text, value) in enumerate(zip(labels, values)):
                    sub_label = QLabel(f"{sub_label_text}:")
                    sub_line_edit = QLineEdit(str(value))
                    sub_line_edit.setReadOnly(False)
                    sub_line_edit.original_value = str(value)

                    # Factory function to avoid late-binding issues
                    def create_edit_handler(sub_line_edit, index):
                        def on_edit_finished():
                            new_value = sub_line_edit.text()
                            if new_value != sub_line_edit.original_value:
                                confirm = QMessageBox.question(
                                    self,
                                    "Confirm Change",
                                    f"Do you want to update {sub_label_text}?\n\nOld Value: {sub_line_edit.original_value}\nNew Value: {new_value}",
                                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                                )
                                if confirm == QMessageBox.StandardButton.Yes:
                                    updated_values = list(metadata_dict.get(key, (None,) * len(values)))  # Copy existing values
                                    if new_value.strip() == "":  # If the input is empty
                                        updated_values[index] = None  # Set to None for empty
                                    else:
                                        updated_values[index] = float(new_value)  # Update the correct element
                                    metadata_dict[key] = tuple(updated_values)
                                    sub_line_edit.original_value = new_value
                                else:
                                    sub_line_edit.setText(sub_line_edit.original_value)

                        return on_edit_finished

                    sub_line_edit.editingFinished.connect(create_edit_handler(sub_line_edit, i))

                    layout.addWidget(sub_label)
                    layout.addWidget(sub_line_edit)

            def add_editable_field(layout, label_text, value, dictionary_key, metadata_dict):
                layout.addWidget(QLabel(f"<b>{label_text}</b>"))
                line_edit = QLineEdit(str(value))
                line_edit.setReadOnly(False)
                line_edit.original_value = str(value)  # Store original value
                line_edit.key = dictionary_key  # Attach the key for later updates

                def on_edit_finished():
                    new_value = line_edit.text()
                    if new_value != line_edit.original_value:
                        # Confirmation dialog
                        confirm = QMessageBox.question(
                            self,
                            "Confirm Change",
                            f"Do you want to update '{label_text}'?\n\nOld Value: {line_edit.original_value}\nNew Value: {new_value}",
                            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                        )
                        if confirm == QMessageBox.StandardButton.Yes:
                            metadata_dict[line_edit.key] = new_value  # Update if confirmed
                            line_edit.original_value = new_value  # Update stored original value
                        else:
                            line_edit.setText(line_edit.original_value)  # Revert to original value

                line_edit.editingFinished.connect(on_edit_finished)
                layout.addWidget(line_edit)

            # Add a title label
            statsLayout.addSpacing(5)
            title = QLabel(f"Tensor Properties")
            title.setStyleSheet("font-weight: bold; font-size: 15px;")
            statsLayout.addWidget(title)
            statsLayout.addWidget(QLabel(f"Data size (MB): {tensor.nbytes / (1024 * 1024):.2f}"))
            statsLayout.addWidget(QLabel(f"Data Type: {tensor.dtype}"))

            if np.issubdtype(tensor.dtype, np.floating):
                if tensor.dtype not in (np.float16, np.float32):
                    tensor = tensor.astype(np.float32)  # Convert only if not already float16/float32
            elif np.issubdtype(tensor.dtype, np.bool_):
                tensor = tensor.astype(np.int16)  # Convert bool → int32 (no need to check if already int32)
            elif np.issubdtype(tensor.dtype, np.integer):
                if tensor.dtype in (np.int16, np.int32):
                    pass
                else:
                    tensor = tensor.astype(np.int32)  # Convert other integer types to int32

            statsLayout.addWidget(QLabel(f"Data shape: {tensor.shape}"))
            statsLayout.addWidget(
                QLabel(f"Number of samples in total : {tensor.shape[0] * tensor.shape[1] * tensor.shape[2]}"))
            statsLayout.addWidget(QLabel(f"Number of samples per trace : {tensor.shape[0]}"))
            statsLayout.addWidget(QLabel(f"Number of inlines : {tensor.shape[1]}"))
            statsLayout.addWidget(QLabel(f"Number of crosslines : {tensor.shape[2]}"))

            # Extract the min and max values from the filename
            metadata = self.metadata[self.file_name]

            if all(key in metadata for key in ("origin", "xline_end", "inline_end")):

                inline_dim = tensor.shape[1]
                crossline_dim = tensor.shape[2]

                # Extract coordinates
                origin = metadata['origin']
                inline_end = metadata['inline_end']
                xline_end = metadata['xline_end']

                # Extract coordinates
                ox, oy = origin
                ix, iy = inline_end
                xx, xy = xline_end

                # Calculate vectors for inline and crossline
                inline_vector = np.array([ix - ox, iy - oy])
                xline_vector = np.array([xx - ox, xy - oy])

                if metadata.get('crs'):
                    source_crs = extract_epsg(metadata['crs'])
                    if source_crs:
                        # Calculate the fourth corner of the rectangle
                        # The fourth point is at origin + (xline_end - origin) + (inline_end - origin)
                        fourth_corner = (
                            origin[0] + (xline_end[0] - origin[0]) + (inline_end[0] - origin[0]),
                            origin[1] + (xline_end[1] - origin[1]) + (inline_end[1] - origin[1])
                        )

                        # Define the four corners in your source CRS
                        corners = [
                            origin,
                            inline_end,
                            fourth_corner,
                            xline_end
                        ]

                        # Create transformer to convert from source CRS to WGS84 (EPSG:4326)
                        transformer = Transformer.from_crs(source_crs, 'EPSG:4326', always_xy=True)

                        # Transform corners to lat/lon coordinates
                        latlon_corners = []
                        for x, y in corners:
                            lon, lat = transformer.transform(x, y)
                            latlon_corners.append((lon, lat))

                        # Create a Geod object based on the WGS84 ellipsoid for geodesic calculations
                        geod = Geod(ellps='WGS84')

                        # Calculate geodesic inline length (distance between origin and inline_end)
                        forward_azimuth, back_azimuth, inline_length = geod.inv(
                            latlon_corners[0][0], latlon_corners[0][1],
                            latlon_corners[1][0], latlon_corners[1][1]
                        )

                        # Calculate geodesic xline length (distance between origin and xline_end)
                        forward_azimuth, back_azimuth, xline_length = geod.inv(
                            latlon_corners[0][0], latlon_corners[0][1],
                            latlon_corners[3][0], latlon_corners[3][1]
                        )

                        # Calculate step sizes using geodesic distances
                        inline_step_size = inline_length / (inline_dim - 1) if inline_dim > 1 else 0
                        xline_step_size = xline_length / (crossline_dim - 1) if crossline_dim > 1 else 0

                        # Calculate the geodesic area
                        lons = [p[0] for p in latlon_corners] + [latlon_corners[0][0]]  # Close the polygon
                        lats = [p[1] for p in latlon_corners] + [latlon_corners[0][1]]  # Close the polygon

                        # Calculate the perimeter
                        perimeter = 0
                        for i in range(len(latlon_corners)):
                            j = (i + 1) % len(latlon_corners)
                            forward_azimuth, back_azimuth, distance = geod.inv(
                                latlon_corners[i][0], latlon_corners[i][1],
                                latlon_corners[j][0], latlon_corners[j][1]
                            )
                            perimeter += distance

                        # Calculate area using geod.polygon_area_perimeter
                        area_geodesic, perimeter_geodesic = geod.polygon_area_perimeter(lons, lats)

                        # Area comes back as negative for clockwise polygons, so take absolute value
                        area_geodesic = abs(area_geodesic)

                        # Set final values
                        area = area_geodesic
                        perimeter = perimeter_geodesic

                    else:
                        # Calculate the lengths of the axes in meters
                        inline_length = np.linalg.norm(inline_vector)
                        xline_length = np.linalg.norm(xline_vector)

                        # Calculate step sizes (meters per pixel)
                        # Note: For N pixels, there are N-1 steps
                        inline_step_size = inline_length / (inline_dim - 1) if inline_dim > 1 else 0
                        xline_step_size = xline_length / (crossline_dim - 1) if crossline_dim > 1 else 0

                        # Calculate area of the rectangle
                        area = inline_length * xline_length

                        # Calculate perimeter of the rectangle
                        perimeter = 2 * (inline_length + xline_length)
                else:

                    # Calculate the lengths of the axes in meters
                    inline_length = np.linalg.norm(inline_vector)
                    xline_length = np.linalg.norm(xline_vector)

                    # Calculate step sizes (meters per pixel)
                    # Note: For N pixels, there are N-1 steps
                    inline_step_size = inline_length / (inline_dim - 1) if inline_dim > 1 else 0
                    xline_step_size = xline_length / (crossline_dim - 1) if crossline_dim > 1 else 0

                    # Calculate area of the rectangle
                    area = inline_length * xline_length

                    # Calculate perimeter of the rectangle
                    perimeter = 2 * (inline_length + xline_length)

                # Calculate rotation (azimuth)
                # Reference direction is North (positive y-axis)
                north = np.array([0, 1])

                # Normalize vectors
                inline_unit = inline_vector / np.linalg.norm(inline_vector) if np.linalg.norm(
                    inline_vector) > 0 else np.array([0, 0])
                xline_unit = xline_vector / np.linalg.norm(xline_vector) if np.linalg.norm(
                    xline_vector) > 0 else np.array([0, 0])

                # Calculate angles using dot product (in radians)
                inline_angle_rad = np.arccos(np.clip(np.dot(north, inline_unit), -1.0, 1.0))
                xline_angle_rad = np.arccos(np.clip(np.dot(north, xline_unit), -1.0, 1.0))

                # Adjust for clockwise measurement (azimuth convention)
                if inline_unit[0] < 0:
                    inline_angle_rad = 2 * np.pi - inline_angle_rad

                if xline_unit[0] < 0:
                    xline_angle_rad = 2 * np.pi - xline_angle_rad

                # Convert to degrees
                inline_azimuth = np.degrees(inline_angle_rad)
                xline_azimuth = np.degrees(xline_angle_rad)

                statsLayout.addWidget(QLabel(f"Crossline length (km): {inline_length / 1000:.2f}"))
                statsLayout.addWidget(QLabel(f"Inline length (km): {xline_length / 1000:.2f}"))
                statsLayout.addWidget(QLabel(f"Inline step size (m): {inline_step_size:.2f}"))
                statsLayout.addWidget(QLabel(f"Crossline step size (m): {xline_step_size:.2f}"))
                statsLayout.addWidget(QLabel(f"Crossline azimuth (°): {inline_azimuth:.2f}°"))
                statsLayout.addWidget(QLabel(f"Inline azimuth (°): {xline_azimuth:.2f}°"))
                statsLayout.addWidget(QLabel(f"Total area (km²): {area / 1000000:.2f}"))
                statsLayout.addWidget(QLabel(f"Perimeter (km): {perimeter / 1000:.2f}"))

            # Add a title label
            statsLayout.addSpacing(5)
            title = QLabel(f"Survey Metadata")
            title.setStyleSheet("font-weight: bold; font-size: 15px;")
            statsLayout.addWidget(title)

            # Display additional metadata fields using .get() to avoid KeyError
            add_editable_field(statsLayout, "Source:", metadata.get('source'), 'source', metadata)
            add_editable_field(statsLayout, "Name:", metadata.get('name'), 'name', metadata)
            add_editable_field(statsLayout, "Survey Type:", metadata.get('survey_type'), 'survey_type', metadata)
            add_editable_field(statsLayout, "Template:", metadata.get('template'), 'template', metadata)
            add_editable_field(statsLayout, "Trace Format:", metadata.get('trace_format'), 'trace_format', metadata)
            add_editable_field(statsLayout, "Sampling interval:", metadata.get('sampling_interval_ms', None),
                               'sampling_interval_ms', metadata)

            min_val, max_val, delta = metadata.get('time_range', (None, None, None))
            add_editable_range_field(statsLayout, "Time Range:", (min_val, max_val, delta), 'time_range', metadata)

            inline_start, inline_end = metadata.get('inline_range', (None, None))
            add_editable_range_field(statsLayout, "Inline Range:", (inline_start, inline_end), 'inline_range',
                                     metadata)

            xline_start, xline_end = metadata.get('xline_range', (None, None))
            add_editable_range_field(statsLayout, "Crossline Range:", (xline_start, xline_end), 'xline_range',
                                     metadata)

            add_editable_field(statsLayout, "CRS:", metadata.get('crs', 'Undefined'), 'crs', metadata)
            add_editable_field(statsLayout, "Coordinate Scale Factor:", metadata.get('coord_scale_factor'),
                               'coord_scale_factor', metadata)

            origin_min, origin_max = metadata.get('origin', (None, None))
            add_editable_range_field(statsLayout, "Origin Coordinates:",
                                     (origin_min, origin_max), 'origin', metadata)

            xline_end_min, xline_end_max = metadata.get('xline_end', (None, None))
            add_editable_range_field(statsLayout, "Crossline End Coordinates:",
                                     (xline_end_min, xline_end_max), 'xline_end', metadata)

            inline_end_min, inline_end_max = metadata.get('inline_end', (None, None))
            add_editable_range_field(statsLayout, "Inline End Coordinates:",
                                     (inline_end_min, inline_end_max), 'inline_end', metadata)

            # Ranges with Delta Values
            for key, label in [
                ('x_range', "X Range"), ('y_range', "Y Range"),
                ('lat_range', "Latitude Range"), ('long_range', "Longitude Range")

            ]:
                min_val, max_val, delta = metadata.get(key, (None, None, None))
                add_editable_range_field(statsLayout, f"{label}:", (min_val, max_val, delta), key, metadata)

            # Add a title label
            statsLayout.addSpacing(5)
            title = QLabel(f"Text Header")
            title.setStyleSheet("font-weight: bold; font-size: 15px;")
            statsLayout.addWidget(title)
            ebcdic_text = "\n".join(line.strip() for line in metadata.get('ebcdic_header_metadata', []))
            ebcdic_edit = ConfirmableTextEdit(ebcdic_text, metadata, self)
            statsLayout.addWidget(ebcdic_edit)

            channel_dimension = np.argmin(tensor.shape)

            # Define a context menu handler outside the loop
            def show_context_menu_info(event, widget):
                global_pos = widget.mapToGlobal(event)

                # Create a QMenu
                context_menu = QMenu(widget)

                # Add a "Save" option
                save_action = context_menu.addAction("Save Plot")

                # Connect actions to functionality
                save_action.triggered.connect(lambda: save_plot_info(widget))

                # Show the context menu at the cursor position
                context_menu.exec(global_pos)

            # Define the save functionality outside the loop
            def save_plot_info(widget):
                self.export_dialog = exportDialog.ExportDialog(widget.scene())
                if not self.isDarkTheme:
                    stylesheet = """
                                    QWidget {{
                                        background-color: {light_color_2};
                                        color: {light_color_3};
                                    }}
                                    QListWidget {{
                                        background-color: {light_color_5}; /* Light grey background */
                                        color: {light_color_3}; /* Dark grey text */
                                    }}
                                    QListWidget::item:selected {{
                                        background-color: {light_color_2}; /* Slightly darker grey for selected item */
                                        color: {light_color_3}; /* Dark grey text for consistency */
                                    }}
                                    QTreeWidget {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                    }}
                                    QTreeWidget::item {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                    }}
                                    QTreeWidget::item:selected {{
                                        background-color: {light_color_2}; /* Selection background */
                                        color: {light_color_3}; /* Selection text color */
                                    }}
                                    QTreeWidget QMenu {{
                                        background-color: {light_color_5}; /* Menu background */
                                        color: {light_color_3}; /* Menu text */
                                    }}
                                    QTreeWidget QMenu::item:selected {{
                                        background-color: {light_color_2}; /* Selection background */
                                        color: {light_color_3}; /* Selection text color */
                                    }}
                                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                        selection-background-color: {light_color_2}; /* Selection background */
                                        selection-color: {light_color_3}; /* Selection text color */
                                    }}
                                    QLineEdit {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                        border-radius: 2px; /* Corner rounding */
                                        height: 20px; /* Height */
                                        selection-background-color: {light_color_2}; /* Selection background */
                                        selection-color: {light_color_3}; /* Selection text color */
                                    }}
                                    QLineEdit:focus {{
                                        border: 1px solid {light_color_8}; /* Focus border */
                                        border-radius: 2px; /* Corner rounding */
                                    }}
                                    QComboBox {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                        border-radius: 2px; /* Corner rounding */
                                        height: 20px; /* Height */
                                        selection-background-color: {light_color_2}; /* Selection background */
                                        selection-color: {light_color_3}; /* Selection text color */
                                    }}
                                    QComboBox:focus {{
                                        border: 1px solid {light_color_8}; /* Focus border */
                                        border-radius: 2px; /* Corner rounding */
                                    }}
                                    QComboBox QAbstractItemView {{
                                        background-color: {light_color_5}; /* Background color */
                                        color: {light_color_3}; /* Text color */
                                        selection-background-color: {light_color_2}; /* Selection background */
                                        selection-color: {light_color_3}; /* Selection text color */
                                    }}
                                    QDialog {{
                                        background-color: {light_color_1}; /* Background color */
                                        color: {light_color_3}; /* Selection text color */
                                    }}
                                    QAbstractItemView {{
                                        selection-background-color: {light_color_2}; /* Selection background */
                                        selection-color: {light_color_3}; /* Selection text color */
                                    }}
                                    QScrollArea {{
                                        background-color: {light_color_5}; /* Scroll area background */
                                        color: {light_color_3}; /* Scroll area text */
                                    }}
                                    QScrollArea QWidget {{
                                        background-color: {light_color_5}; /* Content background */
                                        color: {light_color_3}; /* Content text */
                                    }}
                                    QScrollBar:vertical {{
                                        background-color: {light_color_5}; /* Scrollbar background */
                                    }}
                                    QScrollBar:horizontal {{
                                        background-color: {light_color_5}; /* Scrollbar background */
                                    }}
                                    """.format(
                        light_color_1=self.light_color_1,
                        light_color_2=self.light_color_2,
                        light_color_3=self.light_color_3,
                        light_color_4=self.light_color_4,
                        light_color_5=self.light_color_5,
                        light_color_6=self.light_color_6,
                        light_color_7=self.light_color_7,
                        light_color_8=self.light_color_8,
                        light_color_9=self.light_color_9,
                        light_color_10=self.light_color_10,
                    )
                else:
                    stylesheet = """
                                    QWidget {{
                                        background-color: {darkColor1};
                                        color: {darkColor8};
                                    }}
                                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                                        background-color: {darkColor1};
                                        color: {darkColor5};
                                        selection-background-color: {darkColor2};
                                        selection-color: {darkColor4};
                                    }}
                                    QLineEdit {{
                                        background-color: {darkColor3};
                                        color: {darkColor5};
                                        border-radius: 2px;
                                        height: 20px;
                                        selection-background-color: {darkColor2};
                                        selection-color: {darkColor4};
                                    }}
                                    QLineEdit:focus {{ 
                                        border: 1px solid {darkColor8}; 
                                        border-radius: 2px; /* Rounded corners for the progress bar */
                                    }}
                                    QComboBox {{
                                        background-color: {darkColor3};
                                        color: {darkColor5};
                                        border-radius: 2px;
                                        height: 20px;
                                        selection-background-color: {darkColor2};
                                        selection-color: {darkColor4};
                                    }}
                                    QComboBox:focus {{ 
                                        border: 1px solid {darkColor8}; 
                                        border-radius: 2px; /* Rounded corners for the progress bar */
                                    }}
                                    QComboBox QAbstractItemView {{
                                        background-color: {darkColor3}; /* Same as the combo box background */
                                        color: {darkColor5}; /* Text color */
                                        selection-background-color: {darkColor9}; /* Background color when an item is selected */
                                        selection-color: {darkColor5}; /* Text color when an item is selected */
                                    }}
                                    QTreeWidget {{
                                        background-color: {darkColor2};
                                        color: {darkColor5};
                                    }}
                                    QTreeWidget::item {{
                                        background-color: {darkColor2};
                                        color: {darkColor5};
                                    }}
                                    QTreeWidget::item:selected {{
                                        background-color: {darkColor3};
                                        color: {darkColor5};
                                    }}
                                    QTreeWidget QMenu {{
                                        background-color: {darkColor2};
                                        color: {darkColor5};
                                    }}
                                    QTreeWidget QMenu::item:selected {{
                                        background-color: {darkColor3};
                                        color: {darkColor5};
                                    }}
                                    QDialog {{
                                        background-color: {darkColor1};
                                    }}
                                    QAbstractItemView {{
                                        selection-background-color: {darkColor3};
                                        selection-color: {darkColor5};
                                    }}
                                    QScrollArea {{
                                        background-color: {darkColor2}; /* Dark grey background for the scroll area */
                                        color: {darkColor5}; /* White text for better readability */
                                    }}
                                    QScrollArea QWidget {{
                                        background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                                        color: {darkColor5}; /* White text for better readability */
                                    }}
                                    QScrollBar:vertical {{
                                        background-color: {darkColor2};
                                    }}
                                    QScrollBar:horizontal {{
                                        background-color: {darkColor2};
                                    }}
                                    """.format(
                        darkColor1=self.darkColor1,
                        darkColor2=self.darkColor2,
                        darkColor3=self.darkColor3,
                        darkColor4=self.darkColor4,
                        darkColor5=self.darkColor5,
                        darkColor6=self.darkColor6,
                        darkColor7=self.darkColor7,
                        darkColor8=self.darkColor8,
                        darkColor9=self.darkColor9,
                        darkColor10=self.darkColor10,
                        darkColor11=self.darkColor11,
                        darkColor12=self.darkColor12,
                        darkColor13=self.darkColor13,
                    )
                self.export_dialog.setStyleSheet(stylesheet)
                self.export_dialog.show(widget.plotItem)

            # Check if any channel stats exist in metadata
            has_channel_data = any(
                key.startswith(f"{self.file_name}channel_") for key in metadata.keys()
            )

            if not has_channel_data:
                QApplication.restoreOverrideCursor()
                reply = QMessageBox.question(
                    self,
                    "Statistics Calculation",
                    "Statistics calculations may take a while. Do you want to proceed?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                    QMessageBox.StandardButton.No
                )
                QApplication.setOverrideCursor(self.custom_cursor)
            else:
                reply = QMessageBox.StandardButton.Yes

            num_channels = tensor.shape[channel_dimension]
            for i in range(num_channels):
                channel_data = tensor.take(i, axis=channel_dimension)
                # Check if statistics for this channel already exist in metadata
                channel_key = f"{self.file_name}channel_{i}_stats"
                if channel_key not in metadata:
                    if reply == QMessageBox.StandardButton.Yes:
                        # Compute statistics only if not already cached

                        gpu_statistics = False

                        if gpu_available:
                            # Check available GPU memory before allocating
                            free_memory, total_memory = cp.cuda.Device().mem_info
                            required_memory = channel_data.nbytes

                            # Safety margin (e.g., 80% of free memory)
                            safety_margin = 0.8
                            if required_memory > free_memory * safety_margin:
                                cp.get_default_memory_pool().free_all_blocks()

                                gpu_statistics = False
                            else:
                                gpu_statistics = True

                        if gpu_statistics:

                            # Compute total sample count
                            num_samples = np.prod(channel_data.shape, dtype=np.int64)

                            if num_samples <= self.multiprocessing_threshold:
                                result = TensorVisualizer.statistical_analysis_gpu_task(channel_data)
                            else:
                                QApplication.restoreOverrideCursor()
                                result = self.task_runner.run_task(TensorVisualizer.statistical_analysis_gpu_task,
                                                                   channel_data)
                                QApplication.processEvents()
                                QApplication.setOverrideCursor(self.custom_cursor)

                            if result is not None:
                                mean, std_dev, min_val, max_val, zero_count, positive_count, negative_count, unique_numbers, max_pos, min_pos = result

                                # Cache the computed statistics in metadata
                                metadata[channel_key] = {
                                    "mean": mean,
                                    "std_dev": std_dev,
                                    "min_val": min_val,
                                    "max_val": max_val,
                                    "zero_count": zero_count,
                                    "positive_count": positive_count,
                                    "negative_count": negative_count,
                                    "unique_numbers": unique_numbers,
                                    "max_pos": max_pos,
                                    "min_pos": min_pos,
                                }

                                # Retrieve cached statistics
                                stats = metadata[channel_key]
                                # Add a title label for the channel
                                statsLayout.addSpacing(5)
                                title = QLabel(f"Statistics{' for Channel ' + str(i + 1) if num_channels > 1 else ''}:")
                                title.setStyleSheet("font-weight: bold; font-size: 15px;")
                                statsLayout.addWidget(title)
                                statsLayout.addWidget(QLabel(f"Mean: {stats['mean']}"))
                                statsLayout.addWidget(QLabel(f"Standard Deviation: {stats['std_dev']}"))
                                statsLayout.addWidget(
                                    QLabel(f"Minimum Value: {stats['min_val']} at position {stats['min_pos']}"))
                                statsLayout.addWidget(
                                    QLabel(f"Maximum Value: {stats['max_val']} at position {stats['max_pos']}"))
                                statsLayout.addWidget(QLabel(f"Number of Zeros: {stats['zero_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Positive Values: {stats['positive_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Negative Values: {stats['negative_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Unique Numbers: {stats['unique_numbers']}"))

                        else:

                            # Compute total sample count
                            num_samples = np.prod(channel_data.shape, dtype=np.int64)

                            if num_samples <= self.multiprocessing_threshold:
                                result = TensorVisualizer.statistical_analysis_cpu_task(channel_data)
                            else:
                                QApplication.restoreOverrideCursor()
                                result = self.task_runner.run_task(TensorVisualizer.statistical_analysis_cpu_task,
                                                                   channel_data)
                                QApplication.processEvents()
                                QApplication.setOverrideCursor(self.custom_cursor)

                            if result is not None:
                                mean, std_dev, min_val, max_val, zero_count, positive_count, negative_count, unique_numbers, max_pos, min_pos = result

                                # Cache the computed statistics in metadata
                                metadata[channel_key] = {
                                    "mean": mean,
                                    "std_dev": std_dev,
                                    "min_val": min_val,
                                    "max_val": max_val,
                                    "zero_count": zero_count,
                                    "positive_count": positive_count,
                                    "negative_count": negative_count,
                                    "unique_numbers": unique_numbers,
                                    "max_pos": max_pos,
                                    "min_pos": min_pos,
                                }

                                # Retrieve cached statistics
                                stats = metadata[channel_key]
                                # Add a title label for the channel
                                statsLayout.addSpacing(5)
                                title = QLabel(f"Statistics{' for Channel ' + str(i + 1) if num_channels > 1 else ''}:")
                                title.setStyleSheet("font-weight: bold; font-size: 15px;")
                                statsLayout.addWidget(title)
                                statsLayout.addWidget(QLabel(f"Mean: {stats['mean']}"))
                                statsLayout.addWidget(QLabel(f"Standard Deviation: {stats['std_dev']}"))
                                statsLayout.addWidget(
                                    QLabel(f"Minimum Value: {stats['min_val']} at position {stats['min_pos']}"))
                                statsLayout.addWidget(
                                    QLabel(f"Maximum Value: {stats['max_val']} at position {stats['max_pos']}"))
                                statsLayout.addWidget(QLabel(f"Number of Zeros: {stats['zero_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Positive Values: {stats['positive_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Negative Values: {stats['negative_count']}"))
                                statsLayout.addWidget(QLabel(f"Number of Unique Numbers: {stats['unique_numbers']}"))

                else:
                    # Retrieve cached statistics
                    stats = metadata[channel_key]
                    # Add a title label for the channel
                    statsLayout.addSpacing(5)
                    title = QLabel(f"Statistics{' for Channel ' + str(i + 1) if num_channels > 1 else ''}:")
                    title.setStyleSheet("font-weight: bold; font-size: 15px;")
                    statsLayout.addWidget(title)
                    statsLayout.addWidget(QLabel(f"Mean: {stats['mean']}"))
                    statsLayout.addWidget(QLabel(f"Standard Deviation: {stats['std_dev']}"))
                    statsLayout.addWidget(
                        QLabel(f"Minimum Value: {stats['min_val']} at position {stats['min_pos']}"))
                    statsLayout.addWidget(
                        QLabel(f"Maximum Value: {stats['max_val']} at position {stats['max_pos']}"))
                    statsLayout.addWidget(QLabel(f"Number of Zeros: {stats['zero_count']}"))
                    statsLayout.addWidget(QLabel(f"Number of Positive Values: {stats['positive_count']}"))
                    statsLayout.addWidget(QLabel(f"Number of Negative Values: {stats['negative_count']}"))
                    statsLayout.addWidget(QLabel(f"Number of Unique Numbers: {stats['unique_numbers']}"))

                # Create a new histogram view for this channel
                histogramView = RoundedCanvas(self)
                histogramView.setMinimumSize(400, 400)  # Set a fixed size for each plot
                histogramView.showAxis('bottom')
                histogramView.showAxis('left')
                title = self.file_name.replace("_", " ")
                histogramView.setLabel('bottom', f"{title} Channel {i + 1}")
                histogramView.setLabel('left', 'Count')

                hist_data, bins = np.histogram(channel_data, bins='auto')
                histogramView.plot(bins, hist_data, stepMode=True, fillLevel=0, brush=self.graph_brush)

                # Set up context menu for this instance
                histogramView.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                histogramView.customContextMenuRequested.connect(
                    lambda event, widget=histogramView: show_context_menu_info(event, widget)
                )

                # Add histogram view to the histogram layout
                histogramLayout.addWidget(histogramView)

            QApplication.restoreOverrideCursor()
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def statistical_analysis_gpu_task(channel_data):
        # Copy the channel_data and convert it to a CuPy array for GPU processing
        channel_data_cp = cp.array(channel_data)

        # Perform calculations using CuPy
        mean = cp.mean(channel_data_cp).get()  # .get() transfers the result back to a NumPy scalar
        std_dev = cp.std(channel_data_cp).get()
        min_val = cp.min(channel_data_cp).get()
        max_val = cp.max(channel_data_cp).get()
        zero_count = (cp.size(channel_data_cp) - cp.count_nonzero(channel_data_cp)).get()
        positive_count = cp.sum(channel_data_cp > 0).get()
        negative_count = cp.sum(channel_data_cp < 0).get()
        unique_numbers = cp.unique(channel_data_cp).size  # Size is already a CPU operation

        max_pos = tuple(map(int, cp.unravel_index(cp.argmax(channel_data_cp), channel_data_cp.shape)))
        min_pos = tuple(map(int, cp.unravel_index(cp.argmin(channel_data_cp), channel_data_cp.shape)))

        return mean, std_dev, min_val, max_val, zero_count, positive_count, negative_count, unique_numbers, max_pos, min_pos

    @staticmethod
    def statistical_analysis_cpu_task(channel_data):
        # Fallback code for any error
        mean = np.mean(channel_data)
        std_dev = np.std(channel_data)
        min_val = np.min(channel_data)
        max_val = np.max(channel_data)
        zero_count = np.size(channel_data) - np.count_nonzero(channel_data)
        positive_count = np.sum(channel_data > 0)
        negative_count = np.sum(channel_data < 0)
        unique_numbers = np.unique(channel_data).size

        max_pos = np.unravel_index(np.argmax(channel_data), channel_data.shape)
        min_pos = np.unravel_index(np.argmin(channel_data), channel_data.shape)

        return mean, std_dev, min_val, max_val, zero_count, positive_count, negative_count, unique_numbers, max_pos, min_pos

    def adjust_grid_color(self, last_selected_color):
        # Convert the last selected color to a QColor object
        background_color = QColor(last_selected_color)

        # Calculate the luminance of the background color
        luminance = (
                            0.299 * background_color.red() + 0.587 * background_color.green() + 0.114 * background_color.blue()) / 255

        # If the background color is light, set the grid color to black with transparency, otherwise set it to white with transparency
        grid_color = QColor(0, 0, 0, 64) if luminance > 0.5 else QColor(255, 255, 255, 64)

        # Return the grid color as a string in the format '#RRGGBBAA'
        return grid_color

    def populateList(self):
        self.treeWidget.clear()
        # Create a mapping of parent data keys to their tree items
        parent_items = {}

        # First pass: Create parent nodes for keys without underscores
        for key in self.tensor_dict.keys():

            if isinstance(self.tensor_dict[key], np.ndarray):
                data_type = 'np'
            else:
                data_type = 'pd'

            if data_type == 'np':
                if self.metadata[key].get('template') == 'Seismic':
                    parent_item = QTreeWidgetItem(self.treeWidget, [key])
                    parent_items[key] = parent_item
            else:
                if self.well_header_info_library[key].get('Template') == 'Original':
                    parent_item = QTreeWidgetItem(self.treeWidget, [key])
                    parent_items[key] = parent_item

        # Second pass: Process all keys
        for key in self.tensor_dict.keys():

            if isinstance(self.tensor_dict[key], np.ndarray):
                data_type = 'np'
            else:
                data_type = 'pd'

            if data_type == 'np':
                # Step 1: Get 'name' or 'source' for the provided key
                target_value = self.metadata[key].get('name') if self.metadata[key].get('name') else self.metadata[
                    key].get(
                    'source')

                # Step 2: Check other keys in metadata for matching 'name' or 'source' and ensure 'template' is 'Seismic'
                data_name = next(
                    (k for k, v in self.metadata.items()
                     if (v.get('name') == target_value or v.get('source') == target_value)
                     and v.get('template') == 'Seismic'),
                    None
                )

                attribute_name = self.metadata[key].get('template') if not self.metadata[key].get(
                    'template') == 'Seismic' else None
            else:
                # Step 1: Get 'name' or 'source' for the provided key
                target_value = self.well_header_info_library[key].get('Well Name')

                # Step 2: Check other keys in metadata for matching 'name' or 'source' and ensure 'template' is 'Seismic'
                data_name = next(
                    (k for k, v in self.well_header_info_library.items()
                     if v.get('Well Name') == target_value and v.get('Template') == 'Original'),
                    None
                )

                attribute_name = self.well_header_info_library[key].get('Template') if not \
                self.well_header_info_library[key].get(
                    'Template') == 'Original' else None

            # If the base name already exists as a parent, add as a child
            if data_name in parent_items:
                parent_item = parent_items[data_name]
                if attribute_name:  # Only create child if there's an attribute name
                    QTreeWidgetItem(parent_item, [attribute_name])
            else:
                # If base name doesn't exist, create as parent with full name
                parent_item = QTreeWidgetItem(self.treeWidget, [key])
                parent_items[key] = parent_item

        # Expand all items to make them visible
        self.treeWidget.expandAll()
        # Connect the item selection change to the handleSelectionChange function
        self.treeWidget.itemSelectionChanged.connect(self.handleSelectionChange)

    def handleSelectionChange(self):
        selected_items = self.treeWidget.selectedItems()
        if not selected_items:
            return

        # Assuming single selection, we use the first selected item
        selected_item = selected_items[0]
        key = self.getKeyFromItem(selected_item)
        if key:
            self.selectTensor(key)

    def getKeyFromItem(self, item):
        # Determine if the item is a parent or child and return the appropriate key
        parent_item = item.parent()
        if parent_item is None:
            # This is a parent item
            key = item.text(0).strip()
        else:
            try:
                # Get the parent key and the template value to match
                parent_key = parent_item.text(0).strip()
                target_template = item.text(0).strip()

                # Get the value of 'name' or 'source' for the parent item
                parent_name_or_source = self.metadata[parent_key].get('name') or self.metadata[parent_key].get('source')

                # Search for a key that meets both conditions
                matching_key = None
                for key, value in self.metadata.items():
                    # Check if 'name' or 'source' matches the parent's 'name' or 'source'
                    name_or_source = value.get('name') or value.get('source')
                    if name_or_source == parent_name_or_source and value.get('template') == target_template:
                        matching_key = key
                        break  # Stop once we find the first matching key

                key = matching_key

            except Exception as e:

                # Get the parent key and the template value to match
                parent_key = parent_item.text(0).strip()
                target_template = item.text(0).strip()

                # Get the value of 'name' for the parent item
                parent_name_or_source = self.well_header_info_library[parent_key].get('Well Name')

                # Search for a key that meets both conditions
                matching_key = None
                for key, value in self.well_header_info_library.items():
                    # Check if 'name' or 'source' matches the parent's 'name'
                    name_or_source = value.get('Well Name')
                    if name_or_source == parent_name_or_source and value.get('Template') == target_template:
                        matching_key = key
                        break  # Stop once we find the first matching key

                key = matching_key

        return key

    def selectTensor(self, key):
        self.time_slice_button.show()
        self.cross_line_button.show()
        self.inline_button.show()
        self.plot_button.show()
        if isinstance(self.tensor_dict[key], np.ndarray):
            self.color_mapp = self.metadata[key].get('template')
        # Set the selected tensor as dataTensor
        self.tensor_data = self.tensor_dict[key]
        self.file_name = key
        # Add the loaded file path to the dictionary
        self.three_d()

        if self.tensor_data.shape[-1] > 1:
            self.set_channel_slider_range(self.tensor_data.shape[-1])
            self.channel_index_label.show()
            self.channel_index_entry.show()
            self.Channel_slider.show()
        else:
            self.channel_index_label.hide()
            self.channel_index_entry.hide()
            self.Channel_slider.hide()

        current_index = self.tab_widget.currentIndex()
        current_widget = self.tab_widget.widget(current_index)
        if current_widget.layout():
            canvas = current_widget.layout().itemAt(0).widget()

            if isinstance(canvas, CustomVTKWidget):
                # If the CustomVTKWidget is shown, hide the sampling interval UI elements
                if isinstance(self.tensor_data, np.ndarray):
                    self.Three_D_button.show()
                self.plot_button.clicked.disconnect()  # Disconnect previous signal
                self.plot_button.clicked.connect(self.plot_data3d)
            elif isinstance(canvas, RoundedCanvas):
                # If the RoundedCanvas is shown
                self.plot_button.clicked.disconnect()  # Disconnect previous signal
                self.plot_button.clicked.connect(self.plot_tensor)

        if not isinstance(self.tensor_data, np.ndarray):
            self.time_slice_button.hide()
            self.cross_line_button.hide()
            self.inline_button.hide()
            self.channel_index_entry.hide()
            self.channel_index_label.hide()
            self.Channel_slider.hide()
            self.Three_D_button.hide()
            self.plot_button.clicked.disconnect()  # Disconnect previous signal
            self.plot_button.clicked.connect(self.well_log_viewer)

        if isinstance(self.tensor_data, np.ndarray):
            shape = self.tensor_data.shape
            if len(shape) >= 3:  # Ensure it has at least three dimensions
                if shape[0] == 1:  # First dimension
                    self.cross_line_button.hide()
                    self.inline_button.hide()
                if shape[1] == 1:  # Second dimension
                    self.time_slice_button.hide()
                    self.cross_line_button.hide()
                if shape[2] == 1:  # Third dimension
                    self.time_slice_button.hide()
                    self.inline_button.hide()

    def add_tensor(self, key, tensor):
        # Add the tensor to the dictionary with a unique key
        self.tensor_dict[key] = tensor

        if isinstance(tensor, np.ndarray):
            data_type = 'np'
        else:
            data_type = 'pd'

        self.populateList()

        if data_type == 'np':
            # Step 1: Get 'name' or 'source' for the provided key
            target_value = self.metadata[key].get('name') if self.metadata[key].get('name') else self.metadata[key].get(
                'source')

            # Step 2: Check other keys in metadata for matching 'name' or 'source' and ensure 'template' is 'Seismic'
            data_name = next(
                (k for k, v in self.metadata.items()
                 if (v.get('name') == target_value or v.get('source') == target_value)
                 and v.get('template') == 'Seismic'),
                None
            )

            attribute_name = self.metadata[key].get('template') if not self.metadata[key].get(
                'template') == 'Seismic' else None
        else:
            # Step 1: Get 'name' or 'source' for the provided key
            target_value = self.well_header_info_library[key].get('Well Name')

            # Step 2: Check other keys in metadata for matching 'name' or 'source' and ensure 'template' is 'Seismic'
            data_name = next(
                (k for k, v in self.well_header_info_library.items()
                 if v.get('Well Name') == target_value and v.get('Template') == 'Original'),
                None
            )

            attribute_name = self.well_header_info_library[key].get('Template') if not self.well_header_info_library[key].get(
                'Template') == 'Original' else None

        # Find the parent item for the data
        parent_items = self.treeWidget.findItems(data_name, Qt.MatchFlag.MatchExactly)
        parent_item = parent_items[0] if parent_items else None

        if parent_item:
            if attribute_name:
                # We are adding an attribute, so select the appropriate child item
                for i in range(parent_item.childCount()):
                    child_item = parent_item.child(i)
                    if child_item.text(0).strip() == attribute_name:
                        # Select the child item
                        self.treeWidget.setCurrentItem(child_item)
                        self.selectTensor(key)  # Manually invoke to ensure the tensor is set
                        break
            else:
                # We are adding parent data, select the parent item
                self.treeWidget.setCurrentItem(parent_item)
                self.selectTensor(data_name)  # Manually invoke to ensure the tensor is set
        else:
            full_key_items = self.treeWidget.findItems(key, Qt.MatchFlag.MatchExactly)
            if full_key_items:
                self.treeWidget.setCurrentItem(full_key_items[0])
            self.selectTensor(key)

    def load_text_file(self):
        # Open a file dialog to select a text file
        dialog = QFileDialog()
        options = dialog.options()
        file_name, _ = QFileDialog.getOpenFileName(self, "Open Check Shot File", "", "All Files (*)", options=options)

        if file_name:
            QApplication.setOverrideCursor(self.custom_cursor)
            # Read and parse the file to extract data into a DataFrame
            checkshot_data = self.parse_text_file(file_name)

            # Ensure all checkshot data columns are positive and replace -999 with -999.25
            checkshot_data = checkshot_data.applymap(
                lambda x: -999.25 if x == -999 else (abs(x) if isinstance(x, (int, float)) else x)
            )

            # Extract the display name (name without an extension)
            display_name = os.path.splitext(os.path.basename(file_name))[0]

            # Store the checkshot data in a separate dictionary
            self.checkshot_dict[display_name] = checkshot_data
            self.loaded_file_paths[display_name] = file_name

            for well_name, well_log_df in self.tensor_dict.items():
                well_info = self.well_header_info_library.get(well_name)

                # Skip the loop iteration if well_info is None (i.e., well_name is missing)
                if well_info is None:
                    continue

                # Process the well name if found, stripping and converting to lowercase
                well_name_clean = well_info.get('Well Name').strip().lower()

                # Remove quotes and strip whitespace from checkshot well names
                check_well_names = checkshot_data['Well'].str.strip('"').str.strip().str.lower().str.replace(r'\s*-\s*',
                                                                                                             '-',
                                                                                                             regex=True)  # Remove all spaces around the hyphen

                if well_name_clean in check_well_names.values:
                    matched_rows = checkshot_data[check_well_names == well_name_clean]

                    def identify_depth_column():
                        """Identify the depth column, typically the first column, and return its name."""
                        possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                        for name in possible_names:
                            if name in well_log_df.columns:
                                return name
                        return well_log_df.columns[0]  # If no common name matches, assume the first column is depth

                    depth = identify_depth_column()

                    well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                    # Create a rounded copy of the depth column
                    rounded_depth = well_log_df[depth].round(2)

                    # Set the index of the well log DataFrame to the rounded depth for faster lookup
                    well_log_df.set_index(rounded_depth, inplace=True)

                    # Only add new columns to the well log DataFrame if they don't already exist
                    new_cols = [col for col in matched_rows.columns if col not in well_log_df.columns and col != 'Well']
                    for col in new_cols:
                        well_log_df[col] = -999.25

                    # Set the index of the matched rows DataFrame to 'MD' for fast lookup
                    matched_rows.set_index(matched_rows['MD'].round(2), inplace=True)

                    # Update the well log DataFrame using vectorized operations
                    well_log_df.update(matched_rows[new_cols])

                    # Reset index after operations
                    well_log_df.reset_index(drop=True, inplace=True)

                    # Update the dictionary with the modified DataFrame
                    self.tensor_dict[well_name] = well_log_df

            self.update_recent_files_submenu()
            QApplication.restoreOverrideCursor()
            QMessageBox.information(self, "Loading Completed", "The Loading process has finished successfully.")

    def parse_text_file(self, file_path):
        with open(file_path, 'r') as file:
            lines = file.readlines()

            # Initialize the list to hold the data
            data = []
            column_names = []
            in_header = False
            header_ended = False

            for line in lines:
                stripped_line = line.strip()

                # Detect the start of the header
                if stripped_line == "BEGIN HEADER":
                    in_header = True
                    continue

                # Detect the end of the header
                elif stripped_line == "END HEADER":
                    header_ended = True
                    in_header = False
                    continue

                # Read column names within the header
                if in_header:
                    column_names.append(stripped_line)

                # Collect data after the header
                elif header_ended and stripped_line:
                    # Use regex to extract each field correctly
                    parts = re.findall(r'\".*?\"|\S+', stripped_line)
                    if len(parts) == len(column_names):
                        data.append(parts)

        # Create DataFrame with the extracted data
        df = pd.DataFrame(data, columns=column_names)

        # Convert numeric columns to floats
        for col in df.columns:
            if col != "Well":
                df[col] = pd.to_numeric(df[col], errors='coerce')

        return df

    def load_las_file(self):
        try:
            # Open a file dialog to select an LAS file
            dialog = QFileDialog()
            options = dialog.options()
            file_name, _ = QFileDialog.getOpenFileName(self, "Open Well Log File", "", "All Files (*)", options=options)
            # Get the base name without the extension
            display_name = os.path.splitext(os.path.basename(file_name))[0]

            if file_name:
                QApplication.setOverrideCursor(self.custom_cursor)

                # Determine the file format and load the data
                if file_name.endswith(".csv"):
                    well_log_df = pd.read_csv(file_name)
                    # Replace missing values with -999.25
                    well_log_df.fillna(-999.25, inplace=True)
                    self.units_dict[display_name] = {'DEPT': '.m', 'DEPTH': '.m', 'CALI': '.in', 'DRHO': '.g/cm3', 'DT': '.us/ft', 'GR': '.gAPI', 'RD': '.ohm.m', 'RHOB': '.g/cm3', 'RS': '.sm3/sm3', 'One-waytime1': '.ms', 'Rhom': '.g/cm3', 'Vp': '.m/s', 'Rho': '.g/cm3', 'Vpm': '.m/s', 'PHIE': '.m3/m3', 'PHIT': '.m3/m3', 'SWT': '._', 'SWE': '._', 'VOL_ANHYDR': '._', 'VOL_CALCITE': '._', 'VOL_DOLOM': '._', 'VOL_KAOLIN': '._', 'VOL_QUARTZ': '._', 'VOL_UGAS': '._', 'VOL_UOIL': '._', 'VOL_UWAT': '._', 'VOL_WCS': '._', 'VOL_SIDER': '._'}
                    if display_name not in self.well_header_info_library:
                        self.well_header_info_library[display_name] = {}
                    self.well_header_info_library[display_name]['Well Name'] = display_name
                    self.well_header_info_library[display_name]['Template'] = "Original"
                elif file_name.endswith((".xls", ".xlsx")):
                    # Read all sheet names
                    all_sheets = pd.ExcelFile(file_name).sheet_names  # Get a list of sheet names
                    QApplication.restoreOverrideCursor()
                    # Ask the user to select a sheet
                    selected_sheet, ok = QInputDialog.getItem(
                        self,
                        "Select Sheet",
                        "Choose a sheet to load:",
                        all_sheets,
                        editable=False  # Prevent user from typing custom sheet names
                    )
                    QApplication.setOverrideCursor(self.custom_cursor)
                    if not ok:  # If the user clicked OK and selected a sheet
                        QApplication.restoreOverrideCursor()
                        return

                    well_log_df = pd.read_excel(file_name, sheet_name=selected_sheet)
                    # Replace missing values with -999.25
                    well_log_df.fillna(-999.25, inplace=True)
                    self.units_dict[display_name] = {'DEPT': '.m', 'DEPTH': '.m', 'CALI': '.in', 'DRHO': '.g/cm3', 'DT': '.us/ft', 'GR': '.gAPI', 'RD': '.ohm.m', 'RHOB': '.g/cm3', 'RS': '.sm3/sm3', 'One-waytime1': '.ms', 'Rhom': '.g/cm3', 'Vp': '.m/s', 'Rho': '.g/cm3', 'Vpm': '.m/s', 'PHIE': '.m3/m3', 'PHIT': '.m3/m3', 'SWT': '._', 'SWE': '._', 'VOL_ANHYDR': '._', 'VOL_CALCITE': '._', 'VOL_DOLOM': '._', 'VOL_KAOLIN': '._', 'VOL_QUARTZ': '._', 'VOL_UGAS': '._', 'VOL_UOIL': '._', 'VOL_UWAT': '._', 'VOL_WCS': '._', 'VOL_SIDER': '._'}
                    if display_name not in self.well_header_info_library:
                        self.well_header_info_library[display_name] = {}
                    self.well_header_info_library[display_name]['Well Name'] = display_name
                    self.well_header_info_library[display_name]['Template'] = "Original"
                else:
                    # Parse the LAS file
                    well_log_df = self.parse_las_file(file_name)

                # Store the data in the tensor dictionary with the display name as the key
                self.tensor_dict[display_name] = well_log_df
                self.loaded_file_paths[display_name] = file_name

                # Check if there is any checkshot data that matches this well log
                for checkshot_name, checkshot_data in self.checkshot_dict.items():
                    # Transform checkshot data: make all values positive and replace -999 with -999.25
                    checkshot_data = checkshot_data.applymap(
                        lambda x: -999.25 if x == -999 else (abs(x) if isinstance(x, (int, float)) else x)
                    )

                    checkshot_well_names = checkshot_data['Well'].str.strip('"').str.strip().str.lower().str.replace(
                        r'\s*-\s*', '-', regex=True)  # Remove all spaces around the hyphen
                    display_name_clean = self.well_header_info_library[display_name].get('Well Name').strip().lower()

                    if display_name_clean in checkshot_well_names.values:
                        matched_rows = checkshot_data[checkshot_well_names == display_name_clean]

                        def identify_depth_column():
                            """Identify the depth column, typically the first column, and return its name."""
                            possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                            for name in possible_names:
                                if name in well_log_df.columns:
                                    return name
                            return well_log_df.columns[0]  # Assume the first column is depth if no common name matches

                        depth = identify_depth_column()

                        well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                        # Create a rounded copy of the depth column
                        rounded_depth = well_log_df[depth].round(2)

                        # Set the index of the well log DataFrame to the rounded depth for faster lookup
                        well_log_df.set_index(rounded_depth, inplace=True)

                        # Only add new columns to the well log DataFrame if they don't already exist
                        new_cols = [col for col in matched_rows.columns if col not in well_log_df.columns and col != 'Well']
                        for col in new_cols:
                            well_log_df[col] = -999.25

                        # Set the index of the matched rows DataFrame to 'MD' for fast lookup
                        matched_rows.set_index(matched_rows['MD'].round(2), inplace=True)

                        # Update the well log DataFrame using vectorized operations
                        well_log_df.update(matched_rows[new_cols])

                        # Reset index after operations
                        well_log_df.reset_index(drop=True, inplace=True)

                        # Update the dictionary with the modified DataFrame
                        self.tensor_dict[display_name] = well_log_df

                # Check if there is any well top data that matches this well log
                for welltop_name, welltop_data in self.welltop_dict.items():
                    welltop_well_names = welltop_data['Well'].str.strip('"').str.strip().str.lower().str.replace(r'\s*-\s*',
                                                                                                                 '-',
                                                                                                                 regex=True)  # Remove all spaces around the hyphen
                    display_name_clean = self.well_header_info_library[display_name].get('Well Name').strip().lower()

                    if display_name_clean in welltop_well_names.values:

                        matched_rows = welltop_data[welltop_well_names == display_name_clean].copy()

                        def identify_depth_column():
                            """Identify the depth column, typically the first column, and return its name."""
                            possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                            for name in possible_names:
                                if name in well_log_df.columns:
                                    return name
                            return well_log_df.columns[
                                0]  # Assume the first column is depth if no common name matches

                        depth = identify_depth_column()

                        well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                        # Ensure the index is monotonic increasing
                        well_log_df.sort_index(inplace=True)

                        # Interpolate to find the closest matching depth
                        matched_rows.loc[:, 'MD'] = pd.to_numeric(matched_rows['MD'], errors='coerce')
                        matched_rows.set_index(matched_rows['MD'], inplace=True)

                        # Create new columns if they don't exist
                        new_cols = [col for col in matched_rows.columns if
                                    col not in well_log_df.columns and col != 'Well']
                        for col in new_cols:
                            well_log_df[col] = -999.25

                        # Use KDTree for nearest neighbor search
                        well_log_depths = well_log_df[depth].values.reshape(-1, 1)
                        kdtree = cKDTree(well_log_depths)

                        # Define the maximum allowed distance (threshold)
                        threshold = 2.0

                        for md_value, row_data in matched_rows.iterrows():
                            dist, idx = kdtree.query([[md_value]])
                            nearest_depth = well_log_df.iloc[idx[0]][depth]

                            # Check if the distance is within the threshold
                            if dist[0] <= threshold:
                                # Update well_log_df at the nearest depth with matched row data
                                well_log_df.loc[well_log_df[depth] == nearest_depth, new_cols] = \
                                    row_data[new_cols].values

                        well_log_df.reset_index(drop=True, inplace=True)
                        self.tensor_dict[display_name] = well_log_df

                # Use existing function to add the entry to the tree widget
                self.add_tensor(display_name, well_log_df)
                self.update_recent_files_submenu()

                if hasattr(self, 'well_calc_dialogue') and hasattr(self.well_calc_dialogue,
                                                                   'df_selector'):

                    calc_keys = [key for key in self.tensor_dict.keys() if
                                 isinstance(self.tensor_dict[key], pd.DataFrame)]

                    existing_items = [self.well_calc_dialogue.df_selector.itemText(i) for i in
                                      range(self.well_calc_dialogue.df_selector.count())]

                    # Remove items not in calc_keys
                    for item in existing_items:
                        if item not in calc_keys:
                            index = existing_items.index(item)
                            self.well_calc_dialogue.df_selector.removeItem(index)
                            existing_items.pop(index)  # Update the local list to keep in sync

                    # Add missing items from calc_keys
                    for key in calc_keys:
                        if key not in existing_items:
                            self.well_calc_dialogue.df_selector.addItem(key)

                    # Check if df_selector is empty
                    if self.well_calc_dialogue.df_selector.count() == 0 and hasattr(self.well_calc_dialogue,
                                                                                    'column_selector'):
                        # Clear the column_selector if df_selector is empty
                        self.well_calc_dialogue.column_selector.clear()

                QApplication.restoreOverrideCursor()
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def parse_las_file(self, file_path):
        headers = []
        units = {}
        data_rows = []
        # Get the base name without the extension
        display_name = os.path.splitext(os.path.basename(file_path))[0]

        with open(file_path, 'r') as file:
            lines = file.readlines()
            data_section = False

            for line in lines:
                line = line.strip()

                if line.startswith("~Curve"):
                    # Beginning of headers and units section
                    headers, units = self.extract_headers_and_units(lines)
                elif line.startswith("~Ascii"):
                    # Beginning of a data section
                    data_section = True
                elif data_section:
                    # Collect data rows
                    data_rows.append(line.split())

        # Store units in the dictionary using the file name as the key
        self.parse_well_header(lines, display_name)

        if units is None:
            units = {'DEPT': '.m', 'DEPTH': '.m', 'CALI': '.in', 'DRHO': '.g/cm3', 'DT': '.us/ft', 'GR': '.gAPI', 'RD': '.ohm.m', 'RHOB': '.g/cm3', 'RS': '.sm3/sm3', 'One-waytime1': '.ms', 'Rhom': '.g/cm3', 'Vp': '.m/s', 'Rho': '.g/cm3', 'Vpm': '.m/s', 'PHIE': '.m3/m3', 'PHIT': '.m3/m3', 'SWT': '._', 'SWE': '._', 'VOL_ANHYDR': '._', 'VOL_CALCITE': '._', 'VOL_DOLOM': '._', 'VOL_KAOLIN': '._', 'VOL_QUARTZ': '._', 'VOL_UGAS': '._', 'VOL_UOIL': '._', 'VOL_UWAT': '._', 'VOL_WCS': '._', 'VOL_SIDER': '._'}

        self.units_dict[display_name] = units

        # Convert the collected data to a DataFrame
        df = pd.DataFrame(data_rows, columns=headers)
        return df

    def parse_well_header(self, lines, display_name):

        well_name = None
        processing_well_section = False

        # Temporary dictionary to hold data until we get the well name
        temp_dict = {}

        # Iterate through each line
        for line in lines:
            line = line.strip()

            # Start processing only when we hit the ~Well section
            if line.startswith("~Well"):
                processing_well_section = True
                continue

            # Stop processing if we reach another section
            if processing_well_section and (line.startswith("~") or line.startswith("#")):
                break

            # Skip irrelevant lines
            if not processing_well_section or not line or line.startswith("#"):
                continue

            # Split the line into key and value parts based on '.'
            if '.' in line:
                key_part, value_part = line.split('.', 1)
                key = key_part.strip()
                value = None

                # Handle the WELL key separately to define the main dictionary key
                if key.upper() == "WELL":
                    value_part, _ = value_part.split(':', 1)
                    well_name = value_part.strip().replace(" ", "").replace("- ", "-")
                    self.well_header_info_library[display_name] = temp_dict
                    self.well_header_info_library[display_name]['Well Name'] = well_name
                    self.well_header_info_library[display_name]['Template'] = "Original"
                    continue

                # Handle the DATE key separately due to its unique structure
                if key.upper() == "DATE":
                    value_part, rest = value_part.split(': Log Export Date', 1)
                    value = value_part.strip()

                else:
                    # For all other keys, extract the value
                    if ':' in value_part:
                        value_part, _ = value_part.split(':', 1)
                        value = value_part.strip()
                        pattern = r"([a-zA-Z]+)\s+([\d.]+)"

                        # Search for the pattern in the input string
                        match = re.search(pattern, value)

                        if match:
                            letters = match.group(1)
                            numbers = match.group(2)
                            key = f"{key} ({letters})"
                            value = numbers

                # Only add non-empty values to the dictionary
                if value:
                    if well_name:
                        self.well_header_info_library[display_name][key] = value
                    else:
                        # Store values in temp_dict until we have the well_name
                        temp_dict[key] = value

    def load_top_file(self):
        # Open a file dialog to select a text file
        dialog = QFileDialog()
        options = dialog.options()
        file_name, _ = QFileDialog.getOpenFileName(self, "Open Well Top File", "", "All Files (*)", options=options)

        if file_name:
            QApplication.setOverrideCursor(self.custom_cursor)
            # Read and parse the file to extract data into a DataFrame
            well_top_data = self.parse_top_file(file_name)

            # Ensure all WellTop data columns are positive and replace -999 with -999.25
            well_top_data = well_top_data.applymap(
                lambda x: -999.25 if x == -999 else (abs(x) if isinstance(x, (int, float)) else x)
            )

            # Drop columns that are entirely -999.25
            well_top_data = well_top_data.loc[:, (well_top_data != -999.25).any(axis=0)]

            # Extract the display name (name without an extension)
            display_name = os.path.splitext(os.path.basename(file_name))[0]

            # Store the WellTop data in a separate dictionary
            self.welltop_dict[display_name] = well_top_data
            self.loaded_file_paths[display_name] = file_name

            for well_name, well_log_df in self.tensor_dict.items():

                well_info = self.well_header_info_library.get(well_name)

                # Skip the loop iteration if well_info is None (i.e., well_name is missing)
                if well_info is None:
                    continue

                # Process the well name if found, stripping and converting to lowercase
                well_name_clean = well_info.get('Well Name').strip().lower()

                # Remove quotes and strip whitespace from WellTop well names
                top_well_names = well_top_data['Well'].str.strip('"').str.strip().str.lower().str.replace(r'\s*-\s*',
                                                                                                          '-',
                                                                                                          regex=True)  # Remove all spaces around the hyphen

                if well_name_clean in top_well_names.values:
                    matched_rows = well_top_data[top_well_names == well_name_clean].copy()

                    def identify_depth_column():
                        """Identify the depth column, typically the first column, and return its name."""
                        possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                        for name in possible_names:
                            if name in well_log_df.columns:
                                return name
                        return well_log_df.columns[0]  # If no common name matches, assume the first column is depth

                    depth = identify_depth_column()

                    well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                    # Ensure the index is monotonic increasing
                    well_log_df.sort_index(inplace=True)

                    # Interpolate to find the closest matching depth
                    matched_rows.loc[:, 'MD'] = pd.to_numeric(matched_rows['MD'], errors='coerce')
                    matched_rows.set_index(matched_rows['MD'], inplace=True)

                    # Create new columns if they don't exist
                    new_cols = [col for col in matched_rows.columns if col not in well_log_df.columns and col != 'Well']
                    for col in new_cols:
                        well_log_df[col] = -999.25

                    # Use KDTree for nearest neighbor search
                    well_log_depths = well_log_df[depth].values.reshape(-1, 1)
                    kdtree = cKDTree(well_log_depths)

                    # Define the maximum allowed distance (threshold)
                    threshold = 2.0

                    for md_value, row_data in matched_rows.iterrows():
                        dist, idx = kdtree.query([[md_value]])
                        nearest_depth = well_log_df.iloc[idx[0]][depth]

                        # Check if the distance is within the threshold
                        if dist[0] <= threshold:
                            # Update well_log_df at the nearest depth with matched row data
                            well_log_df.loc[well_log_df[depth] == nearest_depth, new_cols] = row_data[new_cols].values

                    well_log_df.reset_index(drop=True, inplace=True)
                    self.tensor_dict[well_name] = well_log_df

            self.update_recent_files_submenu()
            QApplication.restoreOverrideCursor()
            QMessageBox.information(self, "Loading Completed", "The Loading process has finished successfully.")

    def parse_top_file(self, file_name):
        try:
            with open(file_name, 'r') as file:
                lines = file.readlines()

            # Filter out comment lines and locate header boundaries
            data_start_index = None
            headers = []
            in_header_section = False

            for i, line in enumerate(lines):
                stripped_line = line.strip()
                if stripped_line.startswith('#'):
                    continue
                if "BEGIN HEADER" in stripped_line:
                    in_header_section = True
                    continue
                elif "END HEADER" in stripped_line:
                    in_header_section = False
                    data_start_index = i + 1  # Data starts after END HEADER
                    break
                elif in_header_section:
                    headers.append(stripped_line)

            # Read data into DataFrame
            if data_start_index is not None:
                data_lines = lines[data_start_index:]
                data = []

                # Regular expression pattern to split by spaces, but not inside quotes
                pattern = re.compile(r'''(?:(?<=\s)|(?<=^))"([^"]*)"|([^" \t\n]+)''')

                for line in data_lines:
                    if line.strip():
                        # Match groups using regex, handling spaces within quotes
                        matches = pattern.findall(line)
                        # Extract the actual matched data from regex groups
                        parsed_line = [m[0] if m[0] else m[1] for m in matches]

                        # Remove any trailing newline characters from the last element
                        if parsed_line[-1].endswith('\n'):
                            parsed_line[-1] = parsed_line[-1].rstrip('\n')

                        # Check if the number of parsed columns matches the number of headers
                        if len(parsed_line) != len(headers):
                            # Log a warning or handle it as needed
                            pass

                        data.append(parsed_line)

                try:
                    df = pd.DataFrame(data, columns=headers)
                except ValueError as e:
                    raise ValueError(f"Error while creating DataFrame: {e}")

                # Check for duplicate column names
                duplicate_columns = df.columns[df.columns.duplicated(keep=False)]
                if len(duplicate_columns) > 0:

                    # Process duplicates
                    for col in duplicate_columns.unique():
                        # Find all columns with the same name
                        col_indices = [i for i, x in enumerate(df.columns) if x == col]
                        col_data = df.iloc[:, col_indices]

                        # Check if all columns with the same name have identical data
                        if col_data.nunique(axis=1).eq(1).all():
                            df = df.drop(df.columns[col_indices[1:]], axis=1)
                        else:
                            for i, index in enumerate(col_indices):
                                df.columns.values[index] = f"{col}_{i + 1}"

                # Convert numeric columns to appropriate types
                for col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='ignore')

                return df

            else:
                raise ValueError("Data section not found in the file.")

        except Exception as e:
            error_message = f"An error occurred: {str(e)}"
            QMessageBox.critical(self, "File Parsing Error", error_message)
            return None

    def load_well_head(self):
        """Load and parse a Well Head file."""
        # Open a file dialog to select a text file
        dialog = QFileDialog()
        options = dialog.options()
        file_name, _ = QFileDialog.getOpenFileName(self, "Open Well Head File", "", "All Files (*)", options=options)

        if file_name:
            QApplication.setOverrideCursor(self.custom_cursor)
            try:
                # Extract the display name (name without an extension)
                display_name = os.path.splitext(os.path.basename(file_name))[0]

                # Parse the file and update the well_head_dict
                self.parse_well_head(file_name)
                QApplication.restoreOverrideCursor()
                self.loaded_file_paths[display_name] = file_name
                self.update_recent_files_submenu()
                QMessageBox.information(
                    self, "Loading Completed", "Well Head data loaded successfully."
                )
            except Exception as e:
                QApplication.restoreOverrideCursor()
                # Handle exceptions and display error messages
                QMessageBox.critical(
                    self, "Error", f"An error occurred while loading the file: {e}"
                )

    def parse_well_head(self, file_name):
        """Parse the Well Head file and store data in well_head_dict."""
        with open(file_name, "r") as file:
            lines = file.readlines()

        # Check if this is the new simpler format
        if self.is_simple_format(lines):
            self.parse_simple_format(lines)
        else:
            self.parse_complex_format(lines)

    def is_simple_format(self, lines):
        """Check if the file is in the new simple format."""
        try:
            for line in lines:
                line = line.strip()
                if line:  # Check if the line is not empty
                    return not line.startswith("#")  # If the first non-empty line starts with '#', it's complex
            return True
        except Exception as e:
            QMessageBox.critical(self, "An error occurred", f"Error: {e}")

    def parse_simple_format(self, lines):

        """Parse the new simpler format."""
        well_data = []

        for line in lines:
            line = line.strip()
            if not line or line.startswith("#"):
                continue

            # Extract data by splitting based on whitespace
            data = re.split(r'\s+', line)
            well_data.append(data)

        # Create DataFrame
        headers = ["Well", "Surface X", "Surface Y", "KB"]
        well_df = pd.DataFrame(well_data, columns=headers)

        # Convert numeric columns
        for col in ["Surface X", "Surface Y", "KB"]:
            well_df[col] = pd.to_numeric(well_df[col], errors="coerce")

        # Store parsed data in well_head_dict
        for _, row in well_df.iterrows():
            well_name = re.sub(r'\s*-\s*', '-', row["Well"].strip('"'))  # Remove all spaces around the hyphen

            self.well_head_dict[well_name] = {
                "Surface X": row["Surface X"],
                "Surface Y": row["Surface Y"],
                "KB": row["KB"],
            }

    def parse_complex_format(self, lines):
        # Initialize header and data section indicators
        in_header = False
        headers = []
        well_data = []
        metadata = {}

        # Process each line
        for line in lines:
            line = line.strip()

            # Extract metadata from lines starting with '#'
            if line.startswith("#"):
                key, value = self._extract_metadata(line)
                if key:
                    metadata[key] = value
                continue

            # Skip empty lines
            if not line:
                continue

            # Check for HEADER section
            if "BEGIN HEADER" in line:
                in_header = True
                continue

            if "END HEADER" in line:
                in_header = False
                continue

            # Collect header fields
            if in_header:
                headers.append(line)
                continue

            # Process data lines after headers are collected
            if not in_header and headers:
                # Use a regex to split the line, preserving quoted substrings
                data = re.findall(r'(?:"([^"]*)")|(\S+)', line)
                data = [item[0] if item[0] else item[1] for item in data]

                # Ensure empty values are represented correctly
                data = [item if item else None for item in data]

                # Check if the data matches the headers count
                if len(data) != len(headers):
                    # Handle missing or extra data columns
                    if len(data) < len(headers):
                        data += [None] * (len(headers) - len(data))
                    else:
                        data = data[:len(headers)]

                well_data.append(data)

        # Create DataFrame from the parsed data
        well_df = pd.DataFrame(well_data, columns=headers)

        # Convert numeric columns (only if the column exists in the DataFrame)
        numeric_columns = [
            "Surface X",
            "Surface Y",
            "Well datum value",
            "TD (MD)",
            "Cost",
            "TWT auto",
            "Uncertainty ground level",
            "Uncertainty radius",
            "Uncertainty standard deviation factor",
            "FLOAT,Ambient temperature",
            "FLOAT,Heat transfer coefficient",
            "FLOAT,Replacement velocity",
            "FLOAT,Water surface elevation (from MSL)",
            "FLOAT,Sea bed depth (from MSL)",
            "FLOAT,Sea water velocity",
            "FLOAT,Checkshot reference elevation (from MSL)",
            "FLOAT,Velocity below sea bed",
            "Bottom hole X",
            "Bottom hole Y",
        ]
        for col in numeric_columns:
            if col in well_df.columns:
                well_df[col] = pd.to_numeric(well_df[col], errors="coerce")

        # Fix the splitting of Latitude and Longitude if they're combined
        if "Latitude" in well_df.columns and "Longitude" in well_df.columns:
            for i, row in well_df.iterrows():
                lat_long = row["Latitude"]
                if lat_long and ' ' in lat_long:
                    lat, long = lat_long.split(' ', 1)
                    # Ensure there are no unnecessary slashes
                    lat = lat.replace("/", "")
                    long = long.replace("/", "")
                    well_df.at[i, "Latitude"] = lat
                    well_df.at[i, "Longitude"] = long

        # Store parsed data in well_head_dict using well names as keys
        for _, row in well_df.iterrows():
            well_name = re.sub(r'\s*-\s*', '-', row["Name"].strip(
                '"'))  # Remove all spaces around the hyphen  # Remove quotes from well name

            self.well_head_dict[well_name] = {
                col: row[col] for col in well_df.columns if pd.notna(row[col])
            }
            # Add metadata items directly into the dictionary
            self.well_head_dict[well_name].update(metadata)

    @staticmethod
    def _extract_metadata(line):
        """Extract key-value pairs from metadata lines."""
        if ":" in line:
            key, value = line.lstrip("#").split(":", 1)
            return key.strip(), value.strip()
        return None, None

    def extract_headers_and_units(self, lines):
        headers = []
        units = {}
        curve_section = False

        for line in lines:
            line = line.strip()

            if line.startswith("~Curve"):
                curve_section = True
            elif line.startswith("~Parameter") or line.startswith("~Ascii"):
                break  # End of header section

            if curve_section and not line.startswith("#") and line and '.' in line:
                # Extracting headers and units
                parts = line.split()
                header_name = parts[0].replace(".", "")
                unit = parts[1] if len(parts) > 1 else ""
                headers.append(header_name)
                units[header_name] = unit

        return headers, units

    def show_seismic_calculator(self):
        """
        Show a calculator dialog to perform operations on DataFrames in self.tensorDict.
        """

        # Check if the dialog already exists and is visible
        if self.seismic_calc_dialogue is not None and self.seismic_calc_dialogue.isVisible():
            # Bring the existing dialog to the front
            self.seismic_calc_dialogue.activateWindow()
            return

        class CalculatorDialog(QDialog):
            def __init__(self, parent=None):
                super().__init__(parent)
                self.setWindowTitle("Seismic Calc")
                self.setWindowFlags(self.windowFlags() | Qt.WindowType.WindowMinimizeButtonHint)
                self.setWindowModality(Qt.WindowModality.NonModal)  # Ensure the dialog is non-modal
                self.setFocusPolicy(Qt.FocusPolicy.StrongFocus)
                layout = QVBoxLayout(self)

                # ComboBox for selecting a DataFrame
                self.array_selector = QComboBox(self)
                self.array_selector.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                keys = [key for key in self.parent().tensor_dict.keys() if
                        isinstance(self.parent().tensor_dict[key], np.ndarray)]

                all_channel_names = []  # To store all the generated names

                for key in keys:
                    # Get the tensor associated with the current key
                    tensor = self.parent().tensor_dict[key]

                    # Get the number of channels from the last dimension
                    num_channels = tensor.shape[-1]

                    # Generate names for each channel
                    if num_channels == 1:
                        # Single channel: Use the key directly
                        all_channel_names.append(key)
                    else:
                        # Multi-channel: Add key + channel numbers
                        channel_names = [f"{key} {i + 1}" for i in range(num_channels)]
                        all_channel_names.extend(channel_names)

                # Add all generated names to the array selector
                self.array_selector.addItems(all_channel_names)

                # Connect the selector to update the channel selector dynamically
                self.array_selector.currentIndexChanged.connect(
                    lambda: self.parent().update_array_selectors(self.array_selector)
                )

                # Input field for displaying operations
                self.input_array_display = QLineEdit(self)
                self.input_array_display.setFocusPolicy(Qt.FocusPolicy.NoFocus)

                # Layout for buttons
                button_layout = QGridLayout()

                # Digit buttons
                digits = ['7', '8', '9', '4', '5', '6', '1', '2', '3', '0', '.']
                digit_positions = [(i, j) for i in range(3) for j in range(3)] + [(3, 1), (3, 2)]
                for position, digit in zip(digit_positions, digits):
                    button = QPushButton(digit, self)
                    button.setFixedSize(50, 50)  # Adjust button size
                    button.clicked.connect(lambda _, d=digit: self.input_array_display.insert(d))
                    button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                    button_layout.addWidget(button, *position)

                # Operation buttons
                operations = {'+': '+', '-': '-', '*': '*', '/': '/'}
                op_positions = [(0, 3), (1, 3), (2, 3), (3, 3)]
                for position, (op_text, op_val) in zip(op_positions, operations.items()):
                    button = QPushButton(op_text, self)
                    button.setFixedSize(50, 50)  # Adjust button size
                    button.clicked.connect(lambda _, o=op_val: self.input_array_display.insert(o))
                    button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                    button_layout.addWidget(button, *position)

                # Parentheses buttons
                open_paren_button = QPushButton("(", self)
                open_paren_button.setFixedSize(50, 50)
                open_paren_button.clicked.connect(lambda: self.input_array_display.insert("("))
                open_paren_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(open_paren_button, 4, 0)

                close_paren_button = QPushButton(")", self)
                close_paren_button.setFixedSize(50, 50)
                close_paren_button.clicked.connect(lambda: self.input_array_display.insert(")"))
                close_paren_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(close_paren_button, 4, 1)

                # Power and root buttons
                power_button = QPushButton("^", self)
                power_button.setFixedSize(50, 50)
                power_button.clicked.connect(lambda: self.input_array_display.insert("**"))
                power_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(power_button, 4, 2)

                root_button = QPushButton("√", self)
                root_button.setFixedSize(50, 50)
                root_button.clicked.connect(lambda: self.input_array_display.insert("root(")) # Start root expression
                root_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(root_button, 4, 3)

                # Logarithm button
                log_button = QPushButton("log", self)
                log_button.setFixedSize(50, 50)
                log_button.clicked.connect(lambda: self.input_array_display.insert("log("))
                log_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(log_button, 3, 0)

                # Additional functions
                abs_button = QPushButton("abs", self)
                abs_button.setFixedSize(50, 50)
                abs_button.clicked.connect(lambda: self.input_array_display.insert("abs("))
                abs_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(abs_button, 5, 0)

                # Clear button
                clear_button = QPushButton("C", self)
                clear_button.setFixedSize(50, 50)  # Adjust button size
                clear_button.clicked.connect(lambda: self.input_array_display.clear())
                clear_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(clear_button, 5, 1)

                # Equal button
                equal_button = QPushButton("=", self)
                equal_button.setFixedSize(50, 50)  # Adjust button size
                equal_button.clicked.connect(
                    lambda: self.parent().calculate_and_update_seismic(
                        self, self.array_selector, self.input_array_display
                    )
                )
                equal_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(equal_button, 5, 2)

                # Function to add column name to expression
                def add_array_to_expression():
                    """
                    Add the selected column name to the current expression in the input display.
                    """
                    column_name = self.array_selector.currentText()
                    self.input_array_display.insert(f"[{column_name}]")

                # Add column button
                add_column_button = QPushButton("Add\nColumn", self)
                add_column_button.setFixedSize(50, 50)  # Adjust button size to match others
                add_column_button.clicked.connect(add_array_to_expression)
                add_column_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(add_column_button, 5, 3)

                # Arrange widgets in the layout
                layout.addWidget(QLabel("Choose Seismic Data:", self))
                layout.addWidget(self.array_selector)
                layout.addWidget(self.input_array_display)
                layout.addLayout(button_layout)

                self.setLayout(layout)

            def keyPressEvent(self, event):
                key = event.key()
                key_map = {
                    Qt.Key.Key_0: '0',
                    Qt.Key.Key_1: '1',
                    Qt.Key.Key_2: '2',
                    Qt.Key.Key_3: '3',
                    Qt.Key.Key_4: '4',
                    Qt.Key.Key_5: '5',
                    Qt.Key.Key_6: '6',
                    Qt.Key.Key_7: '7',
                    Qt.Key.Key_8: '8',
                    Qt.Key.Key_9: '9',
                    Qt.Key.Key_Plus: '+',
                    Qt.Key.Key_Minus: '-',
                    Qt.Key.Key_Asterisk: '*',
                    Qt.Key.Key_Slash: '/',
                    Qt.Key.Key_Period: '.',
                    Qt.Key.Key_Comma: ',',  # Add comma
                    Qt.Key.Key_ParenLeft: '(',
                    Qt.Key.Key_ParenRight: ')',
                    Qt.Key.Key_AsciiCircum: '**',  # '^' symbol represents power
                }

                if key in key_map:
                    self.input_array_display.insert(key_map[key])
                    event.accept()  # Accept the event to stop further propagation
                elif key == Qt.Key.Key_Return or key == Qt.Key.Key_Enter:
                    # Trigger the calculation when Enter is pressed
                    self.parent().calculate_and_update_seismic(self, self.array_selector, self.input_array_display)
                elif key == Qt.Key.Key_C:
                    self.input_array_display.clear()
                elif key == Qt.Key.Key_Backspace:
                    self.input_array_display.backspace()  # Handle backspace
                    event.accept()  # Accept the event to stop further propagation

        self.seismic_calc_dialogue = CalculatorDialog(self)
        self.seismic_calc_dialogue.show()

    def update_array_selectors(self, array_selector):
        """
        Populate the array selector with channel names based on the tensor dictionary.
        """
        # Get the keys from tensor_dict that are associated with 4D tensors
        keys = [key for key in self.tensor_dict.keys() if isinstance(self.tensor_dict[key], np.ndarray)]

        # Generate the list of expected items
        all_channel_names = []
        for key in keys:
            tensor = self.tensor_dict[key]
            if tensor.ndim != 4:
                continue
            num_channels = tensor.shape[-1]
            if num_channels == 1:
                all_channel_names.append(key)
            else:
                channel_names = [f"{key} {i + 1}" for i in range(num_channels)]
                all_channel_names.extend(channel_names)

        # Get the current items in the combo box
        current_items = [array_selector.itemText(i) for i in range(array_selector.count())]

        # Remove extra items (items in combo box but not in tensor_dict)
        for item in current_items:
            if item not in all_channel_names:
                index = array_selector.findText(item)
                if index != -1:
                    array_selector.removeItem(index)

        # Add missing items (items in tensor_dict but not in combo box)
        for item in all_channel_names:
            if item not in current_items:
                array_selector.addItem(item)

    def calculate_and_update_seismic(self, dialog, array_selector, input_display):
        """
        Evaluate the expression and update the DataFrame with the new calculated column.
        """
        expression = input_display.text()

        if not expression:
            QMessageBox.warning(dialog, "Error", "No expression provided.")
            return

        try:
            # Extract tensor keys and channels from the expression
            pattern = r"\[([^\]]+)\]"  # Matches content inside brackets (e.g., [key] or [key 1])
            matches = re.findall(pattern, expression)

            # Dictionary to store extracted tensors for evaluation
            data_dict = {}
            key_list = set()

            for match in matches:
                if " " in match:
                    # Multichannel tensor: Split into key and channel index
                    key, channel = match.rsplit(" ", 1)
                    # Check if 'channel' is a valid integer
                    if key in self.tensor_dict and channel.isdigit():
                        try:
                            tensor = self.tensor_dict[key]
                            channel = int(channel) - 1  # Convert to zero-based index
                            # Extract the specific channel (retain 4D shape with singleton channel)
                            data_dict[match] = tensor[..., channel:channel + 1]
                            key_list.add(key)
                        except Exception:
                            data_dict[match] = self.tensor_dict[match]
                            key_list.add(match)
                    else:
                        data_dict[match] = self.tensor_dict[match]
                        key_list.add(match)
                else:
                    # Single-channel tensor: Use the key directly
                    key_list.add(match)
                    data_dict[match] = self.tensor_dict[match]

            # Replace placeholders in the expression with corresponding variables
            for key in matches:
                expression = expression.replace(f"[{key}]", f"data_dict['{key}']")

            def log_with_base(data, base):
                # Strictly check for negatives and raise an error
                if np.any(data <= 0):
                    raise ValueError("Logarithm is undefined for non-positive values in the real number system.")
                return np.log(data) / np.log(base)

            def root(data, n):
                """
                Computes the nth root of the input data while preserving the sign.
                Avoids NaN values for negative numbers.

                Args:
                    data: NumPy array or scalar value.
                    n: Root degree (e.g., 2 for square root, 3 for cube root).

                Returns:
                    NumPy array or scalar with the computed root values.
                """
                # Check if n is even
                if n % 2 == 0:
                    # Raise an error if there are negative values in the data
                    if np.any(data < 0):
                        raise ValueError("Cannot compute even roots of negative values.")
                    # Compute the nth root for non-negative data
                    return data ** (1 / n)

                return np.sign(data) * (np.abs(data) ** (1 / n))

            # Evaluate the expression using NumPy and the extracted tensors
            calculation_result = eval(expression, {"np": np, "data_dict": data_dict, "log": log_with_base, "root": root})

            # Validate the result shape
            if calculation_result.ndim != 4:
                QMessageBox.warning(dialog, "Error", "Resulting tensor is not 4-dimensional.")
                return

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Data Name")
            layout = QVBoxLayout(dialog)
            # Set window flags to enable maximize button and resizing
            dialog.setWindowFlags(
                dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

            # Input field for setting the result column name
            result_name_input = QLineEdit(dialog)
            result_name_input.setPlaceholderText("Enter Data Name")

            layout.addWidget(QLabel("Result Data Name:", dialog))
            layout.addWidget(result_name_input)

            # OK button to accept the units
            ok_button = QPushButton("OK", dialog)
            ok_button.clicked.connect(dialog.accept)
            layout.addWidget(ok_button)

            dialog.setLayout(layout)
            if dialog.exec() == QDialog.DialogCode.Rejected:
                QApplication.restoreOverrideCursor()
                return

            # Use the provided column name or default to 'Result'
            new_data_name = result_name_input.text() if result_name_input.text() else "Result"
            common_key = next(iter(key_list))

            # Add the result as a new np array
            self.tensor_data = calculation_result
            # Store the modified tensor with an informative key
            self.metadata[new_data_name] = self.metadata[common_key].copy()
            self.metadata[new_data_name]['name'] = f"{self.metadata[new_data_name].get('name', '')} {new_data_name}"
            if len(key_list) > 1:  # Check if there's only one item in the set
                # Create a new dialog to select Template Type
                template_dialog = QDialog(self)
                template_dialog.setWindowTitle("Select Template Type")
                layout = QVBoxLayout(template_dialog)
                template_type_combo = QComboBox()
                template_type_combo.addItems([
                    'Seismic',
                    'App. Polarity',
                    'Inst. Frequency',
                    'Cos Phase',
                    'Inst. Phase',
                    'Envelope',
                    'Inst. Bandwidth',
                    'Dom. Frequency',
                    'Sweetness',
                    'RMS Amplitude',
                    'Geobodies',
                    'Anomalies',
                    'Probe',
                    'Upscaled',
                    'Clusters',
                    'Zones',
                    'Annotations',
                    'Other'
                ])

                layout.addWidget(template_type_combo)

                select_button = QPushButton("OK")
                select_button.clicked.connect(template_dialog.accept)  # Close the current dialog
                layout.addWidget(select_button)

                template_dialog.setLayout(layout)
                template_dialog.adjustSize()
                template_dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                template_dialog.setWindowFlags(
                    template_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                QApplication.restoreOverrideCursor()
                result = template_dialog.exec()  # Use exec() to block until the dialog is closed
                if result == QDialog.DialogCode.Accepted:

                    if template_type_combo.currentText() == 'Other':
                        template_name, ok = QInputDialog.getText(self, "Input Dialog", "Enter Template name:")
                        if ok:
                            template_type = template_name
                        else:
                            template_type = 'Seismic'
                    else:
                        template_type = template_type_combo.currentText()
                else:

                    template_type = 'Seismic'

                self.metadata[new_data_name]['template'] = template_type
            self.add_tensor(new_data_name, self.tensor_data)
            self.update_array_selectors(array_selector)

        except Exception as e:
            QMessageBox.warning(dialog, "Error", f"Operation failed: {e}")

    def show_calculator(self):
        """
        Show a calculator dialog to perform operations on DataFrames in self.tensorDict.
        """

        # Check if the dialog already exists and is visible
        if self.well_calc_dialogue is not None and self.well_calc_dialogue.isVisible():
            # Bring the existing dialog to the front
            self.well_calc_dialogue.activateWindow()
            return

        class CalculatorDialog(QDialog):
            def __init__(self, parent=None):
                super().__init__(parent)
                self.setWindowTitle("Well Calc")
                self.setWindowFlags(self.windowFlags() | Qt.WindowType.WindowMinimizeButtonHint)
                self.setWindowModality(Qt.WindowModality.NonModal)  # Ensure the dialog is non-modal
                self.setFocusPolicy(Qt.FocusPolicy.StrongFocus)
                layout = QVBoxLayout(self)

                # ComboBox for selecting a DataFrame
                self.df_selector = QComboBox(self)
                self.df_selector.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                keys = [key for key in self.parent().tensor_dict.keys() if
                        isinstance(self.parent().tensor_dict[key], pd.DataFrame)]
                self.df_selector.addItems(keys)
                self.df_selector.currentIndexChanged.connect(
                    lambda: self.parent().update_column_selectors(self.df_selector, self.column_selector)
                )

                # ComboBox for selecting a column
                self.column_selector = QComboBox(self)
                self.column_selector.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                self.parent().update_column_selectors(self.df_selector, self.column_selector)

                # Input field for displaying operations
                self.input_display = QLineEdit(self)
                self.input_display.setFocusPolicy(Qt.FocusPolicy.NoFocus)

                # Layout for buttons
                button_layout = QGridLayout()

                # Digit buttons
                digits = ['7', '8', '9', '4', '5', '6', '1', '2', '3', '0', '.']
                digit_positions = [(i, j) for i in range(3) for j in range(3)] + [(3, 1), (3, 2)]
                for position, digit in zip(digit_positions, digits):
                    button = QPushButton(digit, self)
                    button.setFixedSize(50, 50)  # Adjust button size
                    button.clicked.connect(lambda _, d=digit: self.input_display.insert(d))
                    button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                    button_layout.addWidget(button, *position)

                # Operation buttons
                operations = {'+': '+', '-': '-', '*': '*', '/': '/'}
                op_positions = [(0, 3), (1, 3), (2, 3), (3, 3)]
                for position, (op_text, op_val) in zip(op_positions, operations.items()):
                    button = QPushButton(op_text, self)
                    button.setFixedSize(50, 50)  # Adjust button size
                    button.clicked.connect(lambda _, o=op_val: self.input_display.insert(o))
                    button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                    button_layout.addWidget(button, *position)

                # Parentheses buttons
                open_paren_button = QPushButton("(", self)
                open_paren_button.setFixedSize(50, 50)
                open_paren_button.clicked.connect(lambda: self.input_display.insert("("))
                open_paren_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(open_paren_button, 4, 0)

                close_paren_button = QPushButton(")", self)
                close_paren_button.setFixedSize(50, 50)
                close_paren_button.clicked.connect(lambda: self.input_display.insert(")"))
                close_paren_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(close_paren_button, 4, 1)

                # Power and root buttons
                power_button = QPushButton("^", self)
                power_button.setFixedSize(50, 50)
                power_button.clicked.connect(lambda: self.input_display.insert("**"))
                power_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(power_button, 4, 2)

                root_button = QPushButton("√", self)
                root_button.setFixedSize(50, 50)
                root_button.clicked.connect(lambda: self.input_display.insert("**(1/"))  # Start root expression
                root_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(root_button, 4, 3)

                # Logarithm button
                log_button = QPushButton("log", self)
                log_button.setFixedSize(50, 50)
                log_button.clicked.connect(lambda: self.input_display.insert("log("))
                log_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(log_button, 3, 0)

                # Additional functions
                abs_button = QPushButton("abs", self)
                abs_button.setFixedSize(50, 50)
                abs_button.clicked.connect(lambda: self.input_display.insert("abs("))
                abs_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(abs_button, 5, 0)

                # Clear button
                clear_button = QPushButton("C", self)
                clear_button.setFixedSize(50, 50)  # Adjust button size
                clear_button.clicked.connect(lambda: self.input_display.clear())
                clear_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(clear_button, 5, 1)

                # Equal button
                equal_button = QPushButton("=", self)
                equal_button.setFixedSize(50, 50)  # Adjust button size
                equal_button.clicked.connect(
                    lambda: self.parent().calculate_and_update(
                        self, self.df_selector, self.input_display
                    )
                )
                equal_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(equal_button, 5, 2)

                # Function to add column name to expression
                def add_column_to_expression():
                    """
                    Add the selected column name to the current expression in the input display.
                    """
                    column_name = self.column_selector.currentText()
                    self.input_display.insert(f"[{column_name}]")

                # Add column button
                add_column_button = QPushButton("Add\nColumn", self)
                add_column_button.setFixedSize(50, 50)  # Adjust button size to match others
                add_column_button.clicked.connect(add_column_to_expression)
                add_column_button.setFocusPolicy(Qt.FocusPolicy.NoFocus)
                button_layout.addWidget(add_column_button, 5, 3)

                # Arrange widgets in the layout
                layout.addWidget(QLabel("Choose Well:", self))
                layout.addWidget(self.df_selector)
                layout.addWidget(QLabel("Choose Log:", self))
                layout.addWidget(self.column_selector)
                layout.addWidget(self.input_display)
                layout.addLayout(button_layout)

                self.setLayout(layout)

            def keyPressEvent(self, event):
                key = event.key()
                key_map = {
                    Qt.Key.Key_0: '0',
                    Qt.Key.Key_1: '1',
                    Qt.Key.Key_2: '2',
                    Qt.Key.Key_3: '3',
                    Qt.Key.Key_4: '4',
                    Qt.Key.Key_5: '5',
                    Qt.Key.Key_6: '6',
                    Qt.Key.Key_7: '7',
                    Qt.Key.Key_8: '8',
                    Qt.Key.Key_9: '9',
                    Qt.Key.Key_Plus: '+',
                    Qt.Key.Key_Minus: '-',
                    Qt.Key.Key_Asterisk: '*',
                    Qt.Key.Key_Slash: '/',
                    Qt.Key.Key_Period: '.',
                    Qt.Key.Key_Comma: ',',  # Add comma
                    Qt.Key.Key_ParenLeft: '(',
                    Qt.Key.Key_ParenRight: ')',
                    Qt.Key.Key_AsciiCircum: '**',  # '^' symbol represents power
                }

                if key in key_map:
                    self.input_display.insert(key_map[key])
                    event.accept()  # Accept the event to stop further propagation
                elif key == Qt.Key.Key_Return or key == Qt.Key.Key_Enter:
                    # Trigger the calculation when Enter is pressed
                    self.parent().calculate_and_update(self, self.df_selector, self.input_display)
                elif key == Qt.Key.Key_C:
                    self.input_display.clear()
                elif key == Qt.Key.Key_Backspace:
                    self.input_display.backspace()  # Handle backspace
                    event.accept()  # Accept the event to stop further propagation

        self.well_calc_dialogue = CalculatorDialog(self)
        self.well_calc_dialogue.show()

    def update_column_selectors(self, df_selector, column_selector):
        """
        Update the column selectors based on the selected DataFrame.
        """
        selected_df_key = df_selector.currentText()
        if selected_df_key:
            df = self.tensor_dict[selected_df_key].copy()
            df = df.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric
            columns = df.columns
            column_selector.clear()
            column_selector.addItems(columns)

    def calculate_and_update(self, dialog, df_selector, input_display):
        """
        Evaluate the expression and update the DataFrame with the new calculated column.
        """
        df_key = df_selector.currentText()
        expression = input_display.text()

        if df_key and expression:
            df = self.tensor_dict[df_key].copy()
            df = df.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric

            try:
                # Replace column placeholders with actual column data in the expression
                for column in df.columns:
                    expression = expression.replace(f"[{column}]", f"df['{column}']")

                # Evaluate the expression using eval, handling division by zero
                try:
                    # Replace log function call with numpy's log and eval
                    expression = re.sub(r'log\(([^,]+),\s*([^)]+)\)', r'np.log(\2) / np.log(\1)', expression)
                    result = eval(expression, {"df": df, "np": np})
                except ZeroDivisionError:
                    QMessageBox.warning(dialog, "Error", "Division by zero is not allowed.")
                    return

                # Calculate the units for the new column
                new_units, result_name_input = self.calculate_units(df_key, input_display.text())

                # Use the provided column name or default to 'Result'
                new_column_name = result_name_input if result_name_input else "Result"

                # Update the units dictionary
                if df_key not in self.units_dict:
                    self.units_dict[df_key] = {}
                self.units_dict[df_key][new_column_name] = new_units

                # Add the result as a new column in the DataFrame
                self.tensor_dict[df_key][new_column_name] = result
                self.add_tensor(df_key, self.tensor_dict[df_key])

                self.update_column_selectors(self.well_calc_dialogue.df_selector,
                                             self.well_calc_dialogue.column_selector)

                QMessageBox.information(dialog, "Success",
                                        f"New column '{new_column_name}' added to DataFrame '{df_key}'. "
                                        f"Units: {new_units}")

            except Exception as e:
                QMessageBox.warning(dialog, "Error", f"Operation failed: {e}")
        else:
            QMessageBox.warning(dialog, "Error",
                                "Please select a DataFrame, enter a valid expression, and provide a result column name.")

    def calculate_units(self, df_key, expression):
        """
        Calculate the resulting units based on the expression and the units of the columns involved.
        """

        # Extract column references from the expression
        columns = re.findall(r"\[([^\]]+)\]", expression)
        unit_expression = expression

        # Function to strip parentheses from a unit
        def strip_parentheses(unit):
            return re.sub(r'[()]', '', unit.strip())

        # Substitute column names with their corresponding units
        for column in columns:
            if df_key in self.units_dict and column in self.units_dict[df_key]:
                unit = self.units_dict[df_key][column]
                stripped_unit = strip_parentheses(unit)
                unit_expression = unit_expression.replace(f"[{column}]", stripped_unit)

        def check_units_for_addition_or_subtraction(unit_expression):
            """
            Check if units can be added or subtracted (they must be the same).
            """
            # Split the expression into components using regex
            tokens = re.split(r'(\s*[\+\-]\s*)', unit_expression)

            # Collect all the units
            units = [token.strip() for token in tokens if token.strip() and not re.match(r'[\+\-]', token)]

            # Function to check if a unit is purely numeric
            def is_numeric(unit):
                return unit.isdigit()

            # Filter out purely numeric units
            filtered_units = [unit for unit in units if not is_numeric(unit)]

            # Check if all stripped units are the same
            if len(filtered_units) > 0 and all(unit == filtered_units[0] for unit in filtered_units):
                return filtered_units[0]  # All units are the same, return the common unit
            else:
                # Raise a warning or return a special message indicating a problem
                raise ValueError("Units must be the same for addition or subtraction.")

        def handle_multiplication(unit_expression):
            """
            Handle multiplication of units and simplify the expression.
            """
            # Split the expression into components using regex for multiplication
            tokens = re.split(r'\s*\*\s*', unit_expression)

            # Collect all the units
            units = [token.strip() for token in tokens if token.strip()]

            # Function to check if a unit is purely numeric
            def is_numeric(unit):
                return unit.isdigit()

            # Filter out purely numeric units
            filtered_units = [unit for unit in units if not is_numeric(unit)]

            # Lists to track numerators and denominators
            numerators = []
            denominators = []

            # Parse each token into numerator and denominator
            for unit in filtered_units:
                num, denom = parse_unit(unit)
                numerators.extend(num)
                denominators.extend(denom)

            # Cancel common units
            for unit in numerators[:]:  # Copy list to avoid issues while iterating
                if unit in denominators:
                    numerators.remove(unit)
                    denominators.remove(unit)

            # Construct the resulting unit
            resulting_unit_parts = []

            # Count occurrences of each unit in the numerator
            while numerators:
                unit = numerators[0]
                count = numerators.count(unit)
                resulting_unit_parts.append(f"{unit}^{count}" if count > 1 else unit)
                numerators = [u for u in numerators if u != unit]

            # Count occurrences of each unit in the denominator
            if denominators:
                resulting_unit_parts.append('/')
                while denominators:
                    unit = denominators[0]
                    count = denominators.count(unit)
                    resulting_unit_parts.append(f"{unit}^{count}" if count > 1 else unit)
                    denominators = [u for u in denominators if u != unit]

            return ''.join(resulting_unit_parts)

        def parse_unit(unit):
            """
            Parse the unit into numerator and denominator components.
            """
            parts = unit.split('/')
            numerator = re.findall(r"[a-zA-Z]+", parts[0]) if parts[0] else []
            denominator = re.findall(r"[a-zA-Z]+", parts[1]) if len(parts) > 1 else []
            return numerator, denominator

        def handle_division(unit_expression):
            """
            Handle division of units and simplify the expression.
            """

            def split_units(expression):
                """
                Splits the expression into numerator and denominator components by flipping at each slash.
                """

                # Initialize empty lists for numerator and denominator
                numerator_units = []
                denominator_units = []

                # Split the expression into parts by the '/' character
                parts = expression.split('/')

                # Track current state of numerator and denominator
                current_numerator = []
                current_denominator = []

                # Initial state: Add the first unit to the numerator
                current_numerator.append(parts[0])

                # Loop over parts to process
                for i in range(1, len(parts)):
                    part = parts[i]

                    if i % 2 == 1:
                        # Flip the current state at each slash
                        current_numerator, current_denominator = current_denominator, current_numerator

                    # Append current part to the correct position
                    current_numerator.append(part)

                # Add the final state to the overall numerator and denominator
                numerator_units.extend(current_numerator)
                denominator_units.extend(current_denominator)

                return numerator_units, denominator_units

            def count_units(units):
                """
                Count the occurrences of each unit.
                """
                counts = {}
                for unit in units:
                    if '^' in unit:
                        base, exp = unit.split('^')
                        counts[base] = counts.get(base, 0) + int(exp)
                    else:
                        counts[unit] = counts.get(unit, 0) + 1
                return counts

            def cancel_units(num_counts, denom_counts):
                """
                Cancel common units in the numerator and denominator.
                """
                for unit in list(num_counts.keys()):
                    if unit in denom_counts:
                        min_count = min(num_counts[unit], denom_counts[unit])
                        num_counts[unit] -= min_count
                        denom_counts[unit] -= min_count
                        if num_counts[unit] == 0:
                            del num_counts[unit]
                        if denom_counts[unit] == 0:
                            del denom_counts[unit]

                return num_counts, denom_counts

            def construct_unit_expression(numerator, denominator):
                """
                Construct the final unit expression from the numerator and denominator.
                """
                if not numerator and not denominator:
                    return 'unitless'

                result = []
                if numerator:
                    num_expression = '*'.join(
                        [f"{unit}^{count}" if count > 1 else unit for unit, count in numerator.items()])
                    result.append(num_expression)
                if denominator:
                    denom_expression = '*'.join(
                        [f"{unit}^{count}" if count > 1 else unit for unit, count in denominator.items()])
                    result.append('/')
                    result.append(denom_expression)
                return ''.join(result)

            numerator, denominator = split_units(unit_expression)

            num_counts = count_units(numerator)
            denom_counts = count_units(denominator)

            num_counts, denom_counts = cancel_units(num_counts, denom_counts)

            return construct_unit_expression(num_counts, denom_counts)

        def handle_absolute(unit_expression):
            """
            Handle absolute value operation by removing 'abs' and any parentheses, returning the plain unit.
            """

            # Use a regular expression to remove 'abs(' and the closing ')'
            result = re.sub(r'abs\((.*?)\)', r'\1', unit_expression)

            return result.strip()

        def handle_power(unit_expression, power):
            """
            Handle power operation. The unit is raised to the given power.
            """
            # Split units and their powers
            units_with_powers = re.findall(r"([a-zA-Z]+)(\^(\d+))?", unit_expression)
            powered_units = []

            for base, _, exp in units_with_powers:
                exp = int(exp) if exp else 1  # Default to 1 if no explicit power is given
                new_exp = exp * power
                powered_units.append(f"{base}^{new_exp}" if new_exp > 1 else base)

            # Join the powered units to form the resulting expression
            return '*'.join(powered_units)

        def handle_log(unit_expression):
            """
            Handle logarithm operation. The result is unitless.
            """
            return 'unitless'

        # In your main function, add a condition for division
        try:
            if 'abs' in unit_expression:
                # Handle the absolute operation
                confirmed_units = handle_absolute(unit_expression)
            elif '**' in unit_expression:
                # Handle the power operation
                base, power_str = unit_expression.split('**')
                power = int(power_str.strip())  # Ensure the power is parsed as an integer
                confirmed_units = handle_power(base.strip(), power)
            elif 'log' in unit_expression:
                # Handle the log operation
                confirmed_units = handle_log(unit_expression)
            elif '*' in unit_expression:
                # Check the units for multiplication
                confirmed_units = handle_multiplication(unit_expression)
            elif '/' in unit_expression:
                # Check the units for division
                confirmed_units = handle_division(unit_expression)
            else:
                # Check the units for addition or subtraction
                confirmed_units = check_units_for_addition_or_subtraction(unit_expression)

            # Show confirmation dialog for units
            confirmed_units, result_name_input = self.confirm_units_dialog(confirmed_units)

            return confirmed_units, result_name_input
        except ValueError as e:
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")
            return None, None

    def confirm_units_dialog(self, suggested_units):
        """
        Display a dialog for the user to confirm or correct the calculated units.
        """
        dialog = QDialog(self)
        dialog.setWindowTitle("Confirm Units")
        layout = QVBoxLayout(dialog)
        # Set window flags to enable maximize button and resizing
        dialog.setWindowFlags(
            dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        layout.addWidget(QLabel("Calculated Units:"))
        unit_input = QLineEdit(suggested_units, dialog)
        layout.addWidget(unit_input)

        # Input field for setting the result column name
        result_name_input = QLineEdit(dialog)
        result_name_input.setPlaceholderText("Enter result column name")

        layout.addWidget(QLabel("Result Column Name:", dialog))
        layout.addWidget(result_name_input)

        # OK button to accept the units
        ok_button = QPushButton("OK", dialog)
        ok_button.clicked.connect(dialog.accept)
        layout.addWidget(ok_button)

        dialog.setLayout(layout)
        if dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        # Return the confirmed or corrected units
        return unit_input.text(), result_name_input.text()

    def show_csv_in_dialog(self):
        """
        Display the CSV data in a new non-modal dialog window.
        """
        # Create a dialog window
        dialog = QDialog(self)
        dialog.setWindowTitle("Table View")
        dialog.setWindowModality(Qt.WindowModality.NonModal)  # Ensure the dialog is non-modal
        dialog.resize(800, 600)  # Set initial size

        # Set window flags to enable maximize button and resizing
        dialog.setWindowFlags(
            dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        # Create a layout for the dialog
        layout = QVBoxLayout(dialog)

        # Create a QTableView to display the CSV data
        table_view = QTableView(dialog)
        model = self.create_pandas_model()
        table_view.setModel(model)
        table_view.setSelectionMode(QAbstractItemView.SelectionMode.ExtendedSelection)

        # Enable editing of column headers
        horizontal_header = table_view.horizontalHeader()
        horizontal_header.sectionDoubleClicked.connect(lambda index: self.edit_header(index, model, table_view))

        # Add the table view to the layout
        layout.addWidget(table_view)

        # Set the layout for the dialog
        dialog.setLayout(layout)

        # Define a function to handle saving changes
        def handle_close_dialog():
            if model.hasChanged():
                reply = QMessageBox.question(
                    dialog,
                    "Save Changes",
                    "Do you want to save the changes to the dataset?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                )
                if reply == QMessageBox.StandardButton.Yes:
                    # Call addTensor() with the updated data
                    self.tensor_data = model._data
                    self.add_tensor(self.file_name, self.tensor_data)

        dialog.finished.connect(handle_close_dialog)

        # Show the dialog
        dialog.show()

    def edit_header(self, index, model, table_view):
        """
        Open an editor for the column header to rename it.
        """
        old_name = model.headerData(index, Qt.Orientation.Horizontal, Qt.ItemDataRole.DisplayRole)

        # Open an input dialog to get the new column name
        new_name, ok = QInputDialog.getText(
            table_view, "Rename Column", f"Rename column '{old_name}' to:", QLineEdit.EchoMode.Normal, old_name
        )

        if ok and new_name:
            # Update the model with the new column name
            success = model.setHeaderData(index, Qt.Orientation.Horizontal, new_name, Qt.ItemDataRole.EditRole)
            if not success:
                QMessageBox.critical(table_view, "Error", f"Failed to rename column '{old_name}' to '{new_name}'.")

    def create_pandas_model(self):
        """
        Convert the CSV data in self.tensor_data to a QAbstractTableModel for use with QTableView.

        :return: A QAbstractTableModel that can be used with QTableView.
        """

        class PandasModel(QAbstractTableModel):
            def __init__(self, data):
                super(PandasModel, self).__init__()
                self._data = data
                self._data_changed = False  # Track if the data has been modified

            def rowCount(self, parent=None):
                return self._data.shape[0]

            def columnCount(self, parent=None):
                return self._data.shape[1]

            def data(self, index, role=Qt.ItemDataRole.DisplayRole):
                if role == Qt.ItemDataRole.DisplayRole or role == Qt.ItemDataRole.EditRole:
                    return str(self._data.iloc[index.row(), index.column()])
                return None

            def setData(self, index, value, role=Qt.ItemDataRole.EditRole):
                if role == Qt.ItemDataRole.EditRole:
                    try:
                        # Update the DataFrame with the new value
                        self._data.iloc[index.row(), index.column()] = value
                        self.dataChanged.emit(index, index, [Qt.ItemDataRole.DisplayRole])
                        self._data_changed = True  # Mark as changed
                        return True
                    except Exception as e:
                        print(f"Failed to set data: {e}")
                        return False
                return False

            def flags(self, index):
                # Make cells editable
                return Qt.ItemFlag.ItemIsSelectable | Qt.ItemFlag.ItemIsEnabled | Qt.ItemFlag.ItemIsEditable

            def headerData(self, section, orientation, role=Qt.ItemDataRole.DisplayRole):
                if role == Qt.ItemDataRole.DisplayRole:
                    if orientation == Qt.Orientation.Horizontal:
                        return str(self._data.columns[section])
                    if orientation == Qt.Orientation.Vertical:
                        return str(self._data.index[section])
                return None

            def setHeaderData(self, section, orientation, value, role=Qt.ItemDataRole.EditRole):
                if role == Qt.ItemDataRole.EditRole and orientation == Qt.Orientation.Horizontal:
                    try:
                        # Update the DataFrame column name
                        self._data.rename(columns={self._data.columns[section]: value}, inplace=True)
                        self.headerDataChanged.emit(orientation, section, section)
                        self._data_changed = True  # Mark as changed
                        return True
                    except Exception as e:
                        print(f"Failed to set header data: {e}")
                        return False
                return False

            def hasChanged(self):
                """Check if the data has been modified."""
                return self._data_changed

        # Create and return a PandasModel instance
        return PandasModel(self.tensor_data.copy())

    def well_log_viewer(self):
        """Open a new non-modal dialog window with well log plots."""
        dialog = QDialog(self)
        dialog.setWindowTitle("Well Logs")
        dialog.setGeometry(100, 100, 1200, 800)

        # Ensure the dialog can be maximized and minimized independently
        dialog.setWindowFlags(
            dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        # Create a scroll area for horizontal scrolling
        scroll_area = QScrollArea(self)
        scroll_area.setWidgetResizable(True)

        # Create a central widget and layout for the scroll area
        central_widget = QWidget()
        h_layout = QHBoxLayout(central_widget)  # Use horizontal layout to align graphs side by side

        # Set the scroll area's central widget
        scroll_area.setWidget(central_widget)

        # Create the main layout for the dialog window
        main_layout = QVBoxLayout()
        main_layout.addWidget(scroll_area)

        # Set the main layout for the dialog
        dialog.setLayout(main_layout)

        # Plot the well log data
        self.create_well_log_plots(h_layout)

        # Show the dialog in non-modal mode
        dialog.show()

    def identify_depth_column(self):
        """Identify the depth column, typically the first column, and return its name."""
        possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
        for name in possible_names:
            if name in self.tensor_data.columns:
                return name
        return self.tensor_data.columns[0]  # If no common name matches, assume the first column is depth

    def create_well_log_plots(self, h_layout):
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            # Define a context menu handler outside the loop
            def show_well_log_plots_context_menu_info(event, widget):
                global_pos = widget.mapToGlobal(event)

                # Create a QMenu
                context_menu = QMenu(widget)

                # Add a "Save" option
                save_action = context_menu.addAction("Save Plot")

                # Connect actions to functionality
                save_action.triggered.connect(lambda: save_well_log_plots_info(widget))

                # Show the context menu at the cursor position
                context_menu.exec(global_pos)

            # Define the save functionality outside the loop
            def save_well_log_plots_info(widget):
                self.export_dialog = exportDialog.ExportDialog(widget.scene())
                if not self.isDarkTheme:
                    stylesheet = """
                                                QWidget {{
                                                    background-color: {light_color_2};
                                                    color: {light_color_3};
                                                }}
                                                QListWidget {{
                                                    background-color: {light_color_5}; /* Light grey background */
                                                    color: {light_color_3}; /* Dark grey text */
                                                }}
                                                QListWidget::item:selected {{
                                                    background-color: {light_color_2}; /* Slightly darker grey for selected item */
                                                    color: {light_color_3}; /* Dark grey text for consistency */
                                                }}
                                                QTreeWidget {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                }}
                                                QTreeWidget::item {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                }}
                                                QTreeWidget::item:selected {{
                                                    background-color: {light_color_2}; /* Selection background */
                                                    color: {light_color_3}; /* Selection text color */
                                                }}
                                                QTreeWidget QMenu {{
                                                    background-color: {light_color_5}; /* Menu background */
                                                    color: {light_color_3}; /* Menu text */
                                                }}
                                                QTreeWidget QMenu::item:selected {{
                                                    background-color: {light_color_2}; /* Selection background */
                                                    color: {light_color_3}; /* Selection text color */
                                                }}
                                                QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                    selection-background-color: {light_color_2}; /* Selection background */
                                                    selection-color: {light_color_3}; /* Selection text color */
                                                }}
                                                QLineEdit {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                    border-radius: 2px; /* Corner rounding */
                                                    height: 20px; /* Height */
                                                    selection-background-color: {light_color_2}; /* Selection background */
                                                    selection-color: {light_color_3}; /* Selection text color */
                                                }}
                                                QLineEdit:focus {{
                                                    border: 1px solid {light_color_8}; /* Focus border */
                                                    border-radius: 2px; /* Corner rounding */
                                                }}
                                                QComboBox {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                    border-radius: 2px; /* Corner rounding */
                                                    height: 20px; /* Height */
                                                    selection-background-color: {light_color_2}; /* Selection background */
                                                    selection-color: {light_color_3}; /* Selection text color */
                                                }}
                                                QComboBox:focus {{
                                                    border: 1px solid {light_color_8}; /* Focus border */
                                                    border-radius: 2px; /* Corner rounding */
                                                }}
                                                QComboBox QAbstractItemView {{
                                                    background-color: {light_color_5}; /* Background color */
                                                    color: {light_color_3}; /* Text color */
                                                    selection-background-color: {light_color_2}; /* Selection background */
                                                    selection-color: {light_color_3}; /* Selection text color */
                                                }}
                                                QDialog {{
                                                    background-color: {light_color_1}; /* Background color */
                                                    color: {light_color_3}; /* Selection text color */
                                                }}
                                                QAbstractItemView {{
                                                    selection-background-color: {light_color_2}; /* Selection background */
                                                    selection-color: {light_color_3}; /* Selection text color */
                                                }}
                                                QScrollArea {{
                                                    background-color: {light_color_5}; /* Scroll area background */
                                                    color: {light_color_3}; /* Scroll area text */
                                                }}
                                                QScrollArea QWidget {{
                                                    background-color: {light_color_5}; /* Content background */
                                                    color: {light_color_3}; /* Content text */
                                                }}
                                                QScrollBar:vertical {{
                                                    background-color: {light_color_5}; /* Scrollbar background */
                                                }}
                                                QScrollBar:horizontal {{
                                                    background-color: {light_color_5}; /* Scrollbar background */
                                                }}
                                                """.format(
                        light_color_1=self.light_color_1,
                        light_color_2=self.light_color_2,
                        light_color_3=self.light_color_3,
                        light_color_4=self.light_color_4,
                        light_color_5=self.light_color_5,
                        light_color_6=self.light_color_6,
                        light_color_7=self.light_color_7,
                        light_color_8=self.light_color_8,
                        light_color_9=self.light_color_9,
                        light_color_10=self.light_color_10,
                    )
                else:
                    stylesheet = """
                                                QWidget {{
                                                    background-color: {darkColor1};
                                                    color: {darkColor8};
                                                }}
                                                QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                                                    background-color: {darkColor1};
                                                    color: {darkColor5};
                                                    selection-background-color: {darkColor2};
                                                    selection-color: {darkColor4};
                                                }}
                                                QLineEdit {{
                                                    background-color: {darkColor3};
                                                    color: {darkColor5};
                                                    border-radius: 2px;
                                                    height: 20px;
                                                    selection-background-color: {darkColor2};
                                                    selection-color: {darkColor4};
                                                }}
                                                QLineEdit:focus {{ 
                                                    border: 1px solid {darkColor8}; 
                                                    border-radius: 2px; /* Rounded corners for the progress bar */
                                                }}
                                                QComboBox {{
                                                    background-color: {darkColor3};
                                                    color: {darkColor5};
                                                    border-radius: 2px;
                                                    height: 20px;
                                                    selection-background-color: {darkColor2};
                                                    selection-color: {darkColor4};
                                                }}
                                                QComboBox:focus {{ 
                                                    border: 1px solid {darkColor8}; 
                                                    border-radius: 2px; /* Rounded corners for the progress bar */
                                                }}
                                                QComboBox QAbstractItemView {{
                                                    background-color: {darkColor3}; /* Same as the combo box background */
                                                    color: {darkColor5}; /* Text color */
                                                    selection-background-color: {darkColor9}; /* Background color when an item is selected */
                                                    selection-color: {darkColor5}; /* Text color when an item is selected */
                                                }}
                                                QTreeWidget {{
                                                    background-color: {darkColor2};
                                                    color: {darkColor5};
                                                }}
                                                QTreeWidget::item {{
                                                    background-color: {darkColor2};
                                                    color: {darkColor5};
                                                }}
                                                QTreeWidget::item:selected {{
                                                    background-color: {darkColor3};
                                                    color: {darkColor5};
                                                }}
                                                QTreeWidget QMenu {{
                                                    background-color: {darkColor2};
                                                    color: {darkColor5};
                                                }}
                                                QTreeWidget QMenu::item:selected {{
                                                    background-color: {darkColor3};
                                                    color: {darkColor5};
                                                }}
                                                QDialog {{
                                                    background-color: {darkColor1};
                                                }}
                                                QAbstractItemView {{
                                                    selection-background-color: {darkColor3};
                                                    selection-color: {darkColor5};
                                                }}
                                                QScrollArea {{
                                                    background-color: {darkColor2}; /* Dark grey background for the scroll area */
                                                    color: {darkColor5}; /* White text for better readability */
                                                }}
                                                QScrollArea QWidget {{
                                                    background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                                                    color: {darkColor5}; /* White text for better readability */
                                                }}
                                                QScrollBar:vertical {{
                                                    background-color: {darkColor2};
                                                }}
                                                QScrollBar:horizontal {{
                                                    background-color: {darkColor2};
                                                }}
                                                """.format(
                        darkColor1=self.darkColor1,
                        darkColor2=self.darkColor2,
                        darkColor3=self.darkColor3,
                        darkColor4=self.darkColor4,
                        darkColor5=self.darkColor5,
                        darkColor6=self.darkColor6,
                        darkColor7=self.darkColor7,
                        darkColor8=self.darkColor8,
                        darkColor9=self.darkColor9,
                        darkColor10=self.darkColor10,
                        darkColor11=self.darkColor11,
                        darkColor12=self.darkColor12,
                        darkColor13=self.darkColor13,
                    )
                self.export_dialog.setStyleSheet(stylesheet)
                self.export_dialog.show(widget.plotItem)
            # Identify the key for the active data (self.tensor_data) in the tensor_dict
            active_data_key = self.file_name

            # Filter the items to be shown in the list widget
            csv_items = [key for key, value in self.tensor_dict.items() if
                         value is not self.tensor_data and isinstance(value, pd.DataFrame)]

            # If there are additional wells
            if csv_items:
                QApplication.restoreOverrideCursor()
                # Ask the user if they want to select additional wells
                confirm_selection = QMessageBox.question(
                    self,
                    "Select Additional Wells",
                    "Do you want to select additional wells?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                    QMessageBox.StandardButton.No
                )
                QApplication.setOverrideCursor(self.custom_cursor)
                # If the user selects 'yes'
                if confirm_selection == QMessageBox.StandardButton.Yes:

                    # Create a new dialog to select additional CSV files
                    csv_selection_dialog = QDialog(self)
                    csv_selection_dialog.setWindowTitle("Select Additional Wells")

                    layout = QVBoxLayout(csv_selection_dialog)

                    csv_list_widget = QListWidget()
                    csv_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)

                    # Add items to the list, excluding the currently active self.tensor_data
                    for key, value in self.tensor_dict.items():
                        if value is not self.tensor_data:  # Skip the currently active tensor data
                            if isinstance(value, pd.DataFrame):
                                csv_list_widget.addItem(key)
                    layout.addWidget(csv_list_widget)

                    select_button = QPushButton("OK")
                    select_button.clicked.connect(csv_selection_dialog.accept)  # Close the current dialog
                    layout.addWidget(select_button)

                    csv_selection_dialog.setLayout(layout)
                    QApplication.restoreOverrideCursor()
                    if csv_selection_dialog.exec() == QDialog.DialogCode.Rejected:
                        return  # Use exec() to block until the dialog is closed
                    QApplication.setOverrideCursor(self.custom_cursor)
                    # Get selected additional CSV files
                    selected_items = csv_list_widget.selectedItems()
                    selected_files = [item.text() for item in selected_items]
                else:
                    selected_files = []
            else:
                selected_files = []

            def valid_columns(df):
                """Return columns that are not entirely NaN or negative after converting to numeric."""
                valid_numeric_columns = []
                formation_columns = []
                cluster_columns = []

                for col in df.columns:

                    numeric_col = pd.to_numeric(df[col], errors='coerce')
                    # Check if the column is not entirely NaN and does not contain only negative values
                    if col.lower() in ['surface', 'member', 'formation']:
                        formation_columns.append(col)
                    elif col.startswith('Clusters') or col.startswith('Zones'):
                        cluster_columns.append(col)
                    # Check if the column is not entirely NaN and does not contain only negative values
                    elif not numeric_col.isna().all() and not (numeric_col < 0).all():
                        valid_numeric_columns.append(col)

                return valid_numeric_columns, formation_columns, cluster_columns

            # Determine the data frames to be plotted
            if len(selected_files) == 0:
                # No additional files selected, only plot the active tensor data
                data_frames = [self.tensor_data.copy()]
                keys = [active_data_key]
            else:
                # Include the currently active tensor data and the selected additional files
                data_frames = [self.tensor_data.copy()] + [self.tensor_dict[file].copy() for file in selected_files]
                keys = [active_data_key] + selected_files

            # Identify valid numeric columns and formation columns
            common_numeric_columns = set(valid_columns(data_frames[0])[0])
            formation_columns = set(valid_columns(data_frames[0])[1])
            cluster_columns = set(valid_columns(data_frames[0])[2])

            for df in data_frames[1:]:
                valid_numeric, formations, clusters = valid_columns(df)
                common_numeric_columns.intersection_update(valid_numeric)
                formation_columns.intersection_update(formations)
                cluster_columns.intersection_update(cluster_columns)

            if not common_numeric_columns and not formation_columns and not cluster_columns:
                QMessageBox.warning(self, "Warning", "No common columns found among the selected CSV files.")
                return

            # Remove the depth column from common columns
            depth_col = self.identify_depth_column()
            common_numeric_columns.discard(depth_col)
            columns = list(common_numeric_columns)

            # Create well log plots for each common column, aligned by dataset
            for col in columns:
                for data_frame, file in zip(data_frames, keys):
                    depth_data = pd.to_numeric(data_frame[depth_col],
                                               errors='coerce')  # Convert to numeric, force NaNs on errors
                    valid_depth_mask = (depth_data.notna()) & (depth_data != -999.25)  # Filter valid depth data
                    depth_data = depth_data[valid_depth_mask]

                    data_col = pd.to_numeric(data_frame[col],
                                             errors='coerce')  # Convert the data to numeric, forcing NaNs for non-numeric values
                    data_col = data_col[valid_depth_mask]  # Apply the same valid mask to other columns
                    data_col.replace(-999.25, np.nan, inplace=True)  # Treat -999.25 as NaN

                    if data_col.isna().all() or data_col.dropna().shape[0] < 20:
                        continue

                    # Create a separate plot for each column using a standard PlotWidget
                    plot_canvas = pg.PlotWidget()
                    plot_canvas.setMenuEnabled(False)
                    plot_canvas.setBackground(self.last_selected_color)  # Set background color

                    # Plot the data
                    plot_canvas.plot(data_col, depth_data, pen=pg.mkPen(color=self.graph_brush, width=2))

                    # Set labels and invert Y axis for depth
                    plot_canvas.setLabel('left', 'Depth (m)')
                    plot_canvas.setLabel('top', f"{col} ({file})")  # Include the file name or key in the label for clarity
                    plot_canvas.invertY(True)

                    # Set x-axis range based on the data
                    if not data_col.dropna().empty:
                        x_min = data_col.min()
                        x_max = data_col.max()
                        plot_canvas.setXRange(x_min, x_max)
                        units = self.units_dict.get(file, {})
                        unit_label = f"({units.get(col, '')})" if col in units else ""
                        plot_canvas.setLabel('top', f"{col} {unit_label} ({file})")

                    # Remove the x-axis at the bottom
                    plot_canvas.getAxis('bottom').setVisible(False)

                    # Show grid lines
                    plot_canvas.showGrid(x=True, y=True)

                    # Adjust the plot width
                    plot_canvas.setFixedWidth(300)  # Adjust the width as needed

                    # Set up context menu for this instance
                    plot_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                    plot_canvas.customContextMenuRequested.connect(
                        lambda event, widget=plot_canvas: show_well_log_plots_context_menu_info(event, widget)
                    )

                    # Add each plot to the horizontal layout
                    h_layout.addWidget(plot_canvas)

                    # Link the y-axis (depth) to the first plot for synchronized scrolling
                    if h_layout.count() > 1:
                        plot_canvas.setYLink(h_layout.itemAt(0).widget())

            # Function to update the visibility of the text items based on the zoom level
            def update_text_visibility():
                # Calculate the current scaling factor from depth to screen pixels
                view_range = plot_canvas.viewRange()
                depth_range = view_range[1]  # vertical range (depth)
                scene_to_view_ratio = plot_canvas.size().height() / (depth_range[1] - depth_range[0])

                for rect, text_item in text_items:
                    # Calculate the width of the rectangle in depth units (meters)
                    rect_start_depth = rect.rect().top()
                    rect_end_depth = rect.rect().bottom()
                    rect_depth_width = abs(rect_end_depth - rect_start_depth)

                    # Convert this width to screen pixels
                    rect_width_in_pixels = rect_depth_width * scene_to_view_ratio

                    # Define the threshold for visibility in pixels
                    threshold_pixels = 20  # Adjust based on your needs

                    # Adjust label visibility based on rectangle width in pixels
                    if rect_width_in_pixels < threshold_pixels:
                        text_item.setVisible(False)
                    else:
                        text_item.setVisible(True)

            # Store text items to manage their visibility
            text_items = []

            # Generate a color map for the cluster labels
            cluster_cmap = plt.get_cmap(self.geophysical_Object_CMap)

            # First, find the intersection of columns that exist in all data_frames
            common_cluster_columns = set(cluster_columns)
            for df in data_frames:
                common_cluster_columns &= set(df.columns)

            # Now loop through only those columns that exist in all data_frames
            for col in common_cluster_columns:
                for data_frame, file in zip(data_frames, keys):

                    depth_data = pd.to_numeric(data_frame[depth_col], errors='coerce')
                    valid_depth_mask = (depth_data.notna()) & (depth_data != -999.25)
                    depth_data = depth_data[valid_depth_mask].values

                    cluster_col = pd.to_numeric(data_frame[col], errors='coerce')[valid_depth_mask]
                    cluster_col.replace(-999.25, np.nan, inplace=True)
                    cluster_col = cluster_col.values  # Convert to numpy array for faster processing

                    unique_clusters = np.unique(cluster_col[~np.isnan(cluster_col)])
                    color_map = {cluster: cluster_cmap(i / len(unique_clusters))[:3] for i, cluster in
                                 enumerate(unique_clusters)}

                    plot_canvas = pg.PlotWidget()
                    plot_canvas.setMenuEnabled(False)
                    plot_canvas.setBackground(self.last_selected_color)

                    # Optimize by working with numpy arrays directly
                    cluster_change_indices = np.where(np.diff(cluster_col) != 0)[0] + 1
                    segment_starts = np.insert(cluster_change_indices, 0, 0)
                    segment_ends = np.append(cluster_change_indices, len(cluster_col))

                    for start_idx, end_idx in zip(segment_starts, segment_ends):
                        current_cluster = cluster_col[start_idx]
                        if np.isnan(current_cluster):
                            continue

                        start_depth = depth_data[start_idx]
                        end_depth = depth_data[end_idx - 1]

                        # Create the rectangle for the cluster
                        color = [int(c * 255) for c in color_map[current_cluster]]
                        rect = QGraphicsRectItem(0, start_depth, 1, end_depth - start_depth)
                        rect.setBrush(pg.mkBrush(color))
                        rect.setPen(pg.mkPen(None))  # No border

                        # Set tooltip to display the cluster number
                        rect.setToolTip(f"{col} {current_cluster}")

                        plot_canvas.addItem(rect)

                        # Add formation name in the middle
                        mid_depth = (start_depth + end_depth) / 2
                        text_item = pg.TextItem(text=f"{col} {current_cluster}", anchor=(0.5, 0.5), color=self.graph_brush)
                        plot_canvas.addItem(text_item)
                        text_item.setPos(0.5, mid_depth)  # Center the text horizontally

                        # Store the text item along with its corresponding rectangle for later use
                        text_items.append((rect, text_item))

                    plot_canvas.setLabel('left', 'Depth (m)')
                    plot_canvas.setLabel('top', f"{col} ({file})")
                    plot_canvas.invertY(True)

                    # Hide all elements of the horizontal axis (top)
                    plot_canvas.getAxis('top').setTicks([])
                    plot_canvas.getAxis('top').setStyle(showValues=False)
                    plot_canvas.getAxis('bottom').setTicks([])
                    plot_canvas.getAxis('bottom').setStyle(showValues=False)

                    # Adjust the plot width
                    plot_canvas.setFixedWidth(300)  # Adjust the width as needed

                    # Set up context menu for this instance
                    plot_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                    plot_canvas.customContextMenuRequested.connect(
                        lambda event, widget=plot_canvas: show_well_log_plots_context_menu_info(event, widget)
                    )

                    # Add the plot canvas to the layout
                    h_layout.addWidget(plot_canvas)

                    if h_layout.count() > 1:
                        plot_canvas.setYLink(h_layout.itemAt(0).widget())

                    # Connect the update function to the view's range change signal
                    plot_canvas.sigRangeChanged.connect(update_text_visibility)
                    # Initial update to set visibility correctly based on initial zoom level
                    update_text_visibility()

            # Example colormap from Matplotlib
            cmap = plt.colormaps.get_cmap(self.geophysical_Object_CMap)  # Updated colormap retrieval method

            common_formation_columns = set(formation_columns)
            for df in data_frames:
                common_formation_columns &= set(df.columns)

            # Now loop through only those columns that exist in all data_frames
            for col in common_formation_columns:
                for data_frame, file in zip(data_frames, keys):

                    depth_data = pd.to_numeric(data_frame[depth_col], errors='coerce')
                    valid_depth_mask = (depth_data.notna()) & (depth_data != -999.25)
                    depth_data = depth_data[valid_depth_mask]

                    formation_col = data_frame[col][valid_depth_mask]

                    # Create plot for formation information
                    plot_canvas = pg.PlotWidget()
                    plot_canvas.setMenuEnabled(False)
                    plot_canvas.setBackground(self.last_selected_color)

                    # Plot formation names with coloring
                    unique_formations = formation_col.dropna().unique()

                    # Generate a color for each formation
                    color_map = {formation: cmap(i / len(unique_formations))[:3] for i, formation in
                                 enumerate(unique_formations)}

                    for i, formation in enumerate(unique_formations):
                        if formation == -999.25 or formation is None:
                            continue
                        formation_depths = depth_data[formation_col == formation]
                        if not formation_depths.empty:
                            start_depth = formation_depths.min()
                            if i + 1 < len(unique_formations):
                                next_formation = unique_formations[i + 1]
                                next_depths = depth_data[formation_col == next_formation]
                                end_depth = next_depths.min() if not next_depths.empty else depth_data.max()
                            else:
                                end_depth = depth_data.max()

                            # Convert color to 0-255 range
                            color = [int(c * 255) for c in color_map[formation]]

                            # Fill the area between formations with a horizontal span
                            rect = QGraphicsRectItem(0, start_depth, 1, end_depth - start_depth)
                            rect.setBrush(pg.mkBrush(color))
                            rect.setPen(pg.mkPen(None))  # No border
                            plot_canvas.addItem(rect)

                            # Add tooltip to the rectangle
                            rect.setToolTip(formation.strip('"'))

                            # Add formation name in the middle
                            mid_depth = (start_depth + end_depth) / 2
                            text_item = pg.TextItem(text=formation.strip('"'), anchor=(0.5, 0.5), color=self.graph_brush)
                            plot_canvas.addItem(text_item)
                            text_item.setPos(0.5, mid_depth)  # Center the text horizontally

                            # Store the text item along with its corresponding rectangle for later use
                            text_items.append((rect, text_item))

                    plot_canvas.setLabel('left', 'Depth (m)')
                    plot_canvas.setLabel('top', f"Formations ({file})")
                    plot_canvas.invertY(True)

                    # Hide all elements of the horizontal axis (top)
                    plot_canvas.getAxis('top').setTicks([])
                    plot_canvas.getAxis('top').setStyle(showValues=False)
                    plot_canvas.getAxis('bottom').setTicks([])
                    plot_canvas.getAxis('bottom').setStyle(showValues=False)

                    plot_canvas.setFixedWidth(300)

                    # Set up context menu for this instance
                    plot_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                    plot_canvas.customContextMenuRequested.connect(
                        lambda event, widget=plot_canvas: show_well_log_plots_context_menu_info(event, widget)
                    )

                    h_layout.addWidget(plot_canvas)

                    if h_layout.count() > 1:
                        plot_canvas.setYLink(h_layout.itemAt(0).widget())

                    # Connect the update function to the view's range change signal
                    plot_canvas.sigRangeChanged.connect(update_text_visibility)
                    # Initial update to set visibility correctly based on initial zoom level
                    update_text_visibility()

            QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def well_context_menu_event(self, event):
        # Create the main context menu
        menu = QMenu(self)

        # "Color Mapping" menu item with submenu
        colormap_menu = menu.addMenu("Color Mapping")

        # List of colormap options
        color_mappings = [
            'jet', 'jet_r', 'turbo', 'turbo_r', 'hsv', 'hsv_r', 'gist_rainbow', 'gist_rainbow_r','gist_ncar_r',
             'tab20', 'tab20_r'
        ]

        # Add colormap options to the submenu
        for color_mapping in color_mappings:
            action = QAction(color_mapping, self.plot_widget)
            action.triggered.connect(lambda checked, cm=color_mapping: self.change_well_colormap(cm))
            colormap_menu.addAction(action)

        # "Export" menu item
        export_action = QAction("Export", self.plot_widget)
        export_action.triggered.connect(self.show_export_dialog)
        menu.addAction(export_action)

        # Add the colormap submenu to the main menu
        menu.addMenu(colormap_menu)

        # Show the context menu at the position of the event
        menu.exec(event.screenPos())

    def show_export_dialog(self):
        # Create and show the export dialog
        self.export_well_dialog = exportDialog.ExportDialog(self.plot_widget.scene())
        if not self.isDarkTheme:
            stylesheet = """
            QWidget {{
                background-color: {light_color_2};
                color: {light_color_3};
            }}
            QListWidget {{
                background-color: {light_color_5}; /* Light grey background */
                color: {light_color_3}; /* Dark grey text */
            }}
            QListWidget::item:selected {{
                background-color: {light_color_2}; /* Slightly darker grey for selected item */
                color: {light_color_3}; /* Dark grey text for consistency */
            }}
            QTreeWidget {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QTreeWidget QMenu {{
                background-color: {light_color_5}; /* Menu background */
                color: {light_color_3}; /* Menu text */
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QComboBox:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox QAbstractItemView {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QDialog {{
                background-color: {light_color_1}; /* Background color */
                color: {light_color_3}; /* Selection text color */
            }}
            QAbstractItemView {{
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QScrollArea {{
                background-color: {light_color_5}; /* Scroll area background */
                color: {light_color_3}; /* Scroll area text */
            }}
            QScrollArea QWidget {{
                background-color: {light_color_5}; /* Content background */
                color: {light_color_3}; /* Content text */
            }}
            QScrollBar:vertical {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            QScrollBar:horizontal {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            """.format(
                light_color_1=self.light_color_1,
                light_color_2=self.light_color_2,
                light_color_3=self.light_color_3,
                light_color_4=self.light_color_4,
                light_color_5=self.light_color_5,
                light_color_6=self.light_color_6,
                light_color_7=self.light_color_7,
                light_color_8=self.light_color_8,
                light_color_9=self.light_color_9,
                light_color_10=self.light_color_10,
            )
        else:
            stylesheet = """
            QWidget {{
                background-color: {darkColor1};
                color: {darkColor8};
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {darkColor1};
                color: {darkColor5};
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QComboBox:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox QAbstractItemView {{
                background-color: {darkColor3}; /* Same as the combo box background */
                color: {darkColor5}; /* Text color */
                selection-background-color: {darkColor9}; /* Background color when an item is selected */
                selection-color: {darkColor5}; /* Text color when an item is selected */
            }}
            QTreeWidget {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QTreeWidget QMenu {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QDialog {{
                background-color: {darkColor1};
            }}
            QAbstractItemView {{
                selection-background-color: {darkColor3};
                selection-color: {darkColor5};
            }}
            QScrollArea {{
                background-color: {darkColor2}; /* Dark grey background for the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollArea QWidget {{
                background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollBar:vertical {{
                background-color: {darkColor2};
            }}
            QScrollBar:horizontal {{
                background-color: {darkColor2};
            }}
            """.format(
                darkColor1=self.darkColor1,
                darkColor2=self.darkColor2,
                darkColor3=self.darkColor3,
                darkColor4=self.darkColor4,
                darkColor5=self.darkColor5,
                darkColor6=self.darkColor6,
                darkColor7=self.darkColor7,
                darkColor8=self.darkColor8,
                darkColor9=self.darkColor9,
                darkColor10=self.darkColor10,
                darkColor11=self.darkColor11,
                darkColor12=self.darkColor12,
                darkColor13=self.darkColor13,
            )
        self.export_well_dialog.setStyleSheet(stylesheet)
        self.export_well_dialog.show(self.plot_widget.plotItem)

    def change_well_colormap(self, color_mapping):
        self.well_color_mapping = color_mapping

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

        # Map labels to colors and convert to QColor for pyqtgraph
        colors = cmap.map(self.well_color_data, mode='qcolor')

        # Create a list of brushes for each color
        brushes = list(map(pg.mkBrush, colors))

        # Update the brushes of the existing scatter plot item
        scatter = self.plot_widget.listDataItems()[0]  # Assuming the scatter plot is the first item
        scatter.setBrush(brushes)

        # Update the ColorBarItem for the color bar
        if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
            self.well_color_bar.setColorMap(cmap)

    def open_plot_dialog(self):
        # Check if the well dialog already exists and is open
        if hasattr(self, 'well_plot_dialog') and self.well_plot_dialog.isVisible():
            # Bring the existing dialog to the front
            self.well_plot_dialog.activateWindow()
            return

        # Create a new dialog only if one doesn't exist or isn't visible
        self.well_plot_dialog = QDialog(self)
        self.well_plot_dialog.resize(1000, 600)
        self.well_plot_dialog.setWindowTitle("Well Analysis")
        # Ensure the dialog can be maximized and minimized independently
        self.well_plot_dialog.setWindowFlags(
            self.well_plot_dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)

        # Set up the main layout
        splitter = QSplitter(Qt.Orientation.Horizontal, self.well_plot_dialog)

        # Left layout: Scrollable results list, ComboBox, Plot button
        left_widget = QWidget(self.well_plot_dialog)  # Set correct parent
        left_layout = QVBoxLayout(left_widget)

        # Scrollable area for results
        result_area = QScrollArea(self.well_plot_dialog)  # Set correct parent
        result_area.setWidgetResizable(True)
        result_area.setFixedHeight(200)  # Set the height to a shorter value
        result_widget = QWidget(self.well_plot_dialog)
        result_layout = QVBoxLayout(result_widget)
        result_area.setWidget(result_widget)
        left_layout.addWidget(result_area)

        # ComboBox for selecting analysis type
        analysis_type_combo = QComboBox(self.well_plot_dialog)
        analysis_type_combo.addItems(["Cross Plots", "Regression", "Clustering", "Classification", "Prediction"])
        left_layout.addWidget(analysis_type_combo)

        # Plot button
        plot_button = QPushButton("Run")
        plot_button.clicked.connect(
            lambda: self.handle_plot_button(self.well_plot_dialog, analysis_type_combo, result_layout))
        left_layout.addWidget(plot_button)

        # Spacer to push everything to the top
        spacer = QSpacerItem(20, 40, QSizePolicy.Policy.Minimum, QSizePolicy.Policy.Expanding)
        left_layout.addItem(spacer)

        splitter.addWidget(left_widget)  # Add the left layout to the splitter

        right_widget = QWidget(self.well_plot_dialog)
        right_layout = QVBoxLayout(right_widget)

        # Right layout: PyQtGraph plot area
        self.plot_widget = pg.PlotWidget()  # Assuming self.plot_widget is still a class attribute
        self.plot_widget.setBackground(self.last_selected_color)
        self.plot_widget.setMenuEnabled(False)
        self.plot_widget.scene().contextMenuEvent = self.well_context_menu_event
        self.plot_widget.hideAxis('left')
        self.plot_widget.hideAxis('bottom')

        # Add padding around the plot widget
        right_layout.setContentsMargins(10, 10, 10, 10)  # Add margins (left, top, right, bottom) as needed
        right_layout.addWidget(self.plot_widget)

        splitter.addWidget(right_widget)  # Add the right layout to the splitter

        # Set initial sizes (ratios) for the splitter
        splitter.setSizes([300, 700])  # Adjust these values as needed to set the initial ratio

        # Use the splitter as the main layout
        dialog_layout = QVBoxLayout(self.well_plot_dialog)
        dialog_layout.addWidget(splitter)
        self.well_plot_dialog.setLayout(dialog_layout)

        self.well_plot_dialog.finished.connect(self.reset_well_color_bar)
        self.well_plot_dialog.show()

    def reset_well_color_bar(self):
        self.well_color_bar = None

    def handle_plot_button(self, parent_dialog, analysis_type_combo, result_layout):
        # Determine the selected analysis type
        analysis_type = analysis_type_combo.currentText()

        if analysis_type == "Cross Plots":
            self.open_cross_plot_options(parent_dialog, result_layout, analysis_type_combo)
        elif analysis_type in ["Regression", "Classification"]:
            self.open_regression_options(parent_dialog, result_layout, analysis_type_combo)
        elif analysis_type == "Clustering":
            self.open_clustering_options(parent_dialog, result_layout, analysis_type_combo)
        elif analysis_type == "Prediction":
            self.process_and_predict()

    def process_and_predict(self):
        try:
            def add_new_model_to_dict():
                """Add a new model with metadata to the dictionary."""

                # Step 1: Browse for the model file
                model_file_path, _ = QFileDialog.getOpenFileName(self, 'Select Model File', '',
                                                                 'Model Files (*.pkl *.h5 *.pth);;All Files (*)')
                if not model_file_path:
                    return  # Cancel if no file selected

                # Step 2: Ask for model type, including an "Else" option
                if model_file_path.endswith(".pkl"):
                    model_type_name = "Machine Learning"
                elif model_file_path.endswith(".h5"):
                    model_type_name = "TF Neural Network"
                elif model_file_path.endswith(".pth"):
                    model_type_name = "torch Neural Network"
                else:
                    QMessageBox.critical(self, "Error", f"Unsupported model format")
                    return  # Cancel if no input

                # Step 4: Determine whether to load the model
                if model_file_path.endswith(".pkl"):
                    try:
                        model = joblib.load(model_file_path)
                    except Exception as e:
                        QMessageBox.critical(self, "Error", f"Failed to load model: {str(e)}")
                        return
                else:
                    model = None  # Neural network models remain uninitialized

                # Step 5: Get features
                # If only one file is selected, directly use its columns
                data_frame = self.tensor_dict[selected_file].copy()
                columns = valid_columns(data_frame)

                feature_dialog = QDialog(self)
                feature_dialog.setWindowTitle("Select Feature Inputs")
                layout = QVBoxLayout(feature_dialog)

                feature_input_list = QListWidget()
                feature_input_list.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
                for col in columns:
                    feature_input_list.addItem(col)
                layout.addWidget(feature_input_list)

                select_button = QPushButton("OK")
                select_button.clicked.connect(feature_dialog.accept)
                layout.addWidget(select_button)

                feature_dialog.setLayout(layout)
                if feature_dialog.exec() == QDialog.DialogCode.Rejected:
                    return

                feature_items = feature_input_list.selectedItems()
                if len(feature_items) == 0:
                    QMessageBox.warning(self, "Warning", "Please select at least one feature input.")
                    return

                features = [item.text() for item in feature_items]

                # Step 6: Get target variable
                target, ok = QInputDialog.getText(
                    self, "Target Variable", "Enter the target variable:"
                )
                if not ok or not target.strip():
                    return
                target = target.strip()

                # Step 7: Ask for dimensionality reduction type
                dim_types = ["None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"]
                dim_type, ok = QInputDialog.getItem(
                    self, "Dimensionality Reduction", "Select type:", dim_types, editable=False
                )
                if not ok:
                    return
                dim_type = dim_type if dim_type != "None" else None

                # Step 8: If dimensionality reduction is selected, ask for number of components
                n_components = None
                if dim_type:
                    n_components, ok = QInputDialog.getInt(
                        self, "Number of Components",
                        f"Enter the number of components (max {len(features)}):",
                        min=1, max=len(features)
                    )
                    if not ok:
                        return

                # Step 9: Add model to the dictionary
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": model_file_path,
                    "dim_type": dim_type,
                    "n_components": n_components,
                }
                combined_key = f"{model_type_name}_{dim_type if dim_type else 'None'}"

                if combined_key not in self.model_dict:
                    self.model_dict[combined_key] = []

                self.model_dict[combined_key].append(model_metadata)

                QMessageBox.information(self, "Success", "Model successfully added to the dictionary!")

            # Step 1: CSV File Selection
            csv_selection_dialog = QDialog(self)
            csv_selection_dialog.setWindowTitle("Select CSV File")
            layout = QVBoxLayout(csv_selection_dialog)

            csv_list_widget = QListWidget()
            csv_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for key in self.tensor_dict:
                if isinstance(self.tensor_dict[key], pd.DataFrame):
                    csv_list_widget.addItem(key)
            layout.addWidget(csv_list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(csv_selection_dialog.accept)
            layout.addWidget(select_button)

            csv_selection_dialog.setLayout(layout)
            if csv_selection_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_items = csv_list_widget.selectedItems()
            if len(selected_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select one CSV file.")
                return

            selected_file = selected_items[0].text()
            selected_df = self.tensor_dict[selected_file].copy()

            def valid_columns(df):
                """Return columns that are not entirely NaN or negative after converting to numeric."""
                valid_columns = []
                for col in df.columns:
                    numeric_col = pd.to_numeric(df[col], errors='coerce')
                    # Check if the column is not entirely NaN and does not contain only negative values
                    if not numeric_col.isna().all() and not (numeric_col < 0).all():
                        valid_columns.append(col)
                return valid_columns

            # If only one file is selected, directly use its columns
            data_frame = self.tensor_dict[selected_file].copy()
            columns = valid_columns(data_frame)

            feature_input_dialog = QDialog(self)
            feature_input_dialog.setWindowTitle("Select Feature Inputs")
            layout = QVBoxLayout(feature_input_dialog)

            feature_input_list = QListWidget()
            feature_input_list.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
            for col in columns:
                feature_input_list.addItem(col)
            layout.addWidget(feature_input_list)

            select_button = QPushButton("OK")
            select_button.clicked.connect(feature_input_dialog.accept)
            layout.addWidget(select_button)

            # Add Add Models buttons
            add_model_button = QPushButton("Import Model")
            add_model_button.clicked.connect(add_new_model_to_dict)
            layout.addWidget(add_model_button)

            feature_input_dialog.setLayout(layout)
            if feature_input_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_feature_items = feature_input_list.selectedItems()
            if len(selected_feature_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select at least one feature input.")
                return

            selected_features = [item.text() for item in selected_feature_items]

            # Step 3: Find Target Variables Based on Selected Features in the Model Dictionary
            potential_target_variables = set()
            for model_type, models in self.model_dict.items():
                for model_info in models:
                    if set(selected_features) == set(model_info['features']):
                        potential_target_variables.add(model_info['target'])

            if not potential_target_variables:
                QMessageBox.warning(self, "No Matching Models",
                                    "No models are available for the selected feature inputs.")
                return

            # Step 4: Target Variable Selection from Model Dictionary
            target_variable_dialog = QDialog(self)
            target_variable_dialog.setWindowTitle("Select Target Variable")
            layout = QVBoxLayout(target_variable_dialog)

            target_variable_list = QListWidget()
            target_variable_list.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for target in potential_target_variables:
                target_variable_list.addItem(target)
            layout.addWidget(target_variable_list)

            select_button = QPushButton("OK")
            select_button.clicked.connect(target_variable_dialog.accept)
            layout.addWidget(select_button)

            target_variable_dialog.setLayout(layout)
            if target_variable_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_target_items = target_variable_list.selectedItems()
            if len(selected_target_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select a target variable.")
                return

            selected_target = selected_target_items[0].text()

            # Step 5: Model Selection Based on Features and Target Variable
            available_models = []
            for model_type, models in self.model_dict.items():
                for model_info in models:
                    if set(selected_features) == set(model_info['features']) and selected_target == model_info[
                        'target']:
                        available_models.append((model_type, model_info))

            if not available_models:
                QMessageBox.warning(self, "No Matching Model",
                                    "No trained models available for the selected features and target variable.")
                return

            model_selection_dialog = QDialog(self)
            model_selection_dialog.setWindowTitle("Select Model for Prediction")
            layout = QVBoxLayout(model_selection_dialog)

            model_list_widget = QListWidget()
            model_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
            for model_type, model_info in available_models:
                # Use a string representation for easy selection, could be more detailed if needed
                model_list_widget.addItem(f"{model_type} model")

            layout.addWidget(model_list_widget)

            select_button = QPushButton("OK")
            select_button.clicked.connect(model_selection_dialog.accept)
            layout.addWidget(select_button)

            model_selection_dialog.setLayout(layout)
            if model_selection_dialog.exec() == QDialog.DialogCode.Rejected:
                return

            selected_model_items = model_list_widget.selectedItems()
            if len(selected_model_items) == 0:
                QMessageBox.warning(self, "Warning", "Please select a model.")
                return

            # Extract the selected model information
            selected_model_index = model_list_widget.currentRow()
            selected_model_type, selected_model_info = available_models[selected_model_index]

            # Get the model object
            model = selected_model_info['model']
            file_path = selected_model_info['file_path']
            dimensionality_reduction_type_combo = selected_model_info['dim_type']
            n_components = selected_model_info['n_components']

            # Step 6: Data Preprocessing
            # Check for corresponding samples in selected columns
            if selected_features:
                # Read selected CSV
                combined_df = selected_df[selected_features]

                # Convert selected columns to numeric and exclude NaNs and -999.25
                combined_df[selected_features].apply(pd.to_numeric, errors='coerce')
                combined_df = combined_df.replace(-999.25, np.nan)

                # Count rows where all selected columns have valid (non-NaN) values
                valid_rows = combined_df.dropna(subset=selected_features)
                num_valid_points = len(valid_rows)

                if num_valid_points == 0:
                    QMessageBox.warning(self, "No Valid Data Points",
                                        "No data points have values for all selected columns.")
                    return

            dialog = QDialog(self)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Min-Max Scaler", "Standard Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Rejected:
                return

            normalization_choice = combo.currentText()

            X = valid_rows[selected_features]

            X_clean = X.to_numpy()

            if normalization_choice == "Min-Max Scaler":
                scaler = MinMaxScaler()
                X_clean = scaler.fit_transform(X_clean)
            elif normalization_choice == "Standard Scaler":
                scaler = StandardScaler()
                X_clean = scaler.fit_transform(X_clean)

            selected_model_type = selected_model_type.split('_')[0]

            if selected_model_type in ["ResNet", "TCN", "Keras Classifier", "Hyper Parameter", "TabNet",
                                       "Keras Regressor", "TF Neural Network", "torch Neural Network"]:
                model = None

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir = os.path.join("logs", "Tensorflow", "autoencoder_DR_predict_well",
                                       datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir = None

            y_pred = self.task_runner.run_task(TensorVisualizer.run_well_inference_neural_net, X_clean,
                                               selected_model_type, file_path, model,
                                               dimensionality_reduction_type_combo, n_components, log_dir)

            if y_pred is not None:

                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir)

                # Step 8: Update CSV with Predictions
                if y_pred.ndim > 1:
                    predicted_col_name = f"{selected_target}_predicted_{selected_model_type}"
                    selected_df[predicted_col_name] = pd.Series(y_pred[:, 0], index=valid_rows.index)
                    selected_df[predicted_col_name] = selected_df[predicted_col_name].fillna(-999.25)
                else:
                    predicted_col_name = f"{selected_target}_predicted_{selected_model_type}"
                    selected_df[predicted_col_name] = pd.Series(y_pred, index=valid_rows.index)
                    selected_df[predicted_col_name] = selected_df[predicted_col_name].fillna(-999.25)

                self.tensor_data = selected_df
                self.add_tensor(selected_file, self.tensor_data)
                QMessageBox.information(self, "Prediction Complete", f"Predictions added to {selected_file}")

        except Exception as e:
            traceback.print_exc()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def run_well_inference_neural_net(X_clean, selected_model_type, file_path, model,
                                      dimensionality_reduction_type_combo, n_components, log_dir):

        # Set up logging
        TensorVisualizer.setup_logging('Well_Inference')

        def load_model_by_type(file_path, selected_model_type):
            """
            Loads the model from the specified file path based on its type.

            :param file_path: Path to the saved model file.
            :param selected_model_type: Type of the model (e.g., "Keras", "PyTorch", "Sklearn").
            :return: The loaded model object.
            """
            if selected_model_type in ["Keras Classifier", "Hyper Parameter", "Keras Regressor",
                                       "TF Neural Network"]:
                # Load the Keras model
                return load_model(file_path, compile=False)
            elif selected_model_type in ["ResNet", "TCN", "TabNet", "torch Neural Network"]:
                # Load the whole PyTorch model
                return torch.load(file_path)

        dtype = X_clean.dtype

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X_clean)

            X_clean = pca.transform(X_clean)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X_clean = tsne.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X_clean)
            X_clean = ica.transform(X_clean)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X_clean = grp.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X_clean.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X_clean, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val), callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X_clean = encoder.predict(X_clean)

        # Predict using the fitted model
        if selected_model_type in ["ResNet", "torch Neural Network"]:
            model = load_model_by_type(file_path, selected_model_type)
            # Make predictions
            model.eval()
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            X_val_tensor = torch.tensor(X_clean.reshape(X_clean.shape[0], 1, -1), dtype=torch.float32)
            with torch.no_grad():
                y_pred = model(
                    X_val_tensor.to(device)).cpu().numpy()  # Move predictions back to CPU for compatibility
                y_pred = np.array(y_pred).squeeze()
        elif selected_model_type == "TabNet":
            model = load_model_by_type(file_path, selected_model_type)
            y_pred = model.predict(X_clean)
            y_pred = np.array(y_pred).squeeze()
        elif selected_model_type == "TCN":
            model = load_model_by_type(file_path, selected_model_type)
            # Make predictions
            model.eval()
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            X_val_tensor = torch.tensor(X_clean.reshape(X_clean.shape[0], 1, -1), dtype=torch.float32)
            with torch.no_grad():
                y_pred = model(
                    X_val_tensor.to(device)).cpu().numpy()  # Move predictions back to CPU for compatibility
                y_pred = np.array(y_pred).squeeze()
        elif selected_model_type in ["Hyper Parameter", "TF Neural Network"]:

            X_clean = X_clean.reshape((X_clean.shape[0], 1, X_clean.shape[1]))

            model = load_model_by_type(file_path, selected_model_type)
            y_pred = model.predict(X_clean)
            y_pred = np.array(y_pred).squeeze()

        elif selected_model_type in ["Keras Classifier", "Keras Regressor"]:
            model = load_model_by_type(file_path, selected_model_type)
            y_pred = model.predict(X_clean)
            y_pred = np.array(y_pred).squeeze()
        else:
            y_pred = model.predict(X_clean)

        """If y_pred has more than one channel, apply argmax along the last axis."""
        if y_pred.ndim > 1 and y_pred.shape[-1] > 1:
            y_pred = np.argmax(y_pred, axis=-1, keepdims=True)

        return y_pred.astype(dtype)

    def open_clustering_options(self, parent_dialog, result_layout, analysis_type_combo):
        # Create a new dialog to select linear or non-linear regression
        cross_plot_dialog = QDialog(parent_dialog)
        cross_plot_dialog.setWindowTitle("Clustering Options")

        layout = QVBoxLayout(cross_plot_dialog)

        clustering_type_combo = QComboBox()
        clustering_type_combo.addItems(["K-Means", "Hierarchical", "DBSCAN", "Gaussian Mixture", "Spectral",
                                        "Agglomerative", "Birch", "Affinity Propagation", "Mean Shift", "OPTICS",
                                        "SOM", "HDBSCAN", "DEC"])
        layout.addWidget(clustering_type_combo)

        select_button = QPushButton("OK")
        select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        cross_plot_dialog.setLayout(layout)
        cross_plot_dialog.adjustSize()
        cross_plot_dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        cross_plot_dialog.setWindowFlags(cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

        result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
        if result == QDialog.DialogCode.Accepted:
            clustering_type_combo = clustering_type_combo.currentText()
            # Trigger the next function after the dialog is closed
            self.open_csv_selection_dialog(parent_dialog, clustering_type_combo, result_layout, analysis_type_combo)

    def open_regression_options(self, parent_dialog, result_layout, analysis_type_combo):

        # Determine the selected analysis type
        analysis_type = analysis_type_combo.currentText()

        # Create a new dialog to select linear or non-linear regression
        cross_plot_dialog = QDialog(parent_dialog)
        if analysis_type == "Classification":
            cross_plot_dialog.setWindowTitle("Classification Options")
        else:
            cross_plot_dialog.setWindowTitle("Regression Options")

        layout = QVBoxLayout(cross_plot_dialog)

        regression_type_combo = QComboBox()
        if analysis_type == "Classification":
            regression_type_combo.addItems([
                "Random Forest Classifier",
                "Gradient Boosting Classifier",
                "Support Vector Classifier",
                "K-Nearest Neighbors Classifier",
                "Decision Tree Classifier",
                "Gaussian Process Classifier",
                "CatBoost Classifier",
                "LightGBM Classifier",
                "Naive Bayes Classifier",
                "Extra Trees Classifier",
                "Bagging Classifier",
                "Voting Classifier",
                "Keras Classifier",
                "ResNet",
                "TabNet",
                "TCN",
                "Hyper Parameter"
            ])
        else:
            regression_type_combo.addItems([
                "Ridge Regression",
                "Lasso Regression",
                "Bayesian Ridge Regression",
                "Random Forest Regressor",
                "Support Vector Regressor",
                "K-Nearest Neighbors Regressor",
                "Decision Tree Regressor",
                "Gaussian Process Regressor",
                "XGBoost Regressor",
                "LightGBM Regressor",
                "NGBoost Regressor",
                "CatBoost Regressor",
                "Gradient Boosting Regressor",
                "AdaBoost Regressor",
                "Keras Regressor",
                "ResNet",
                "TabNet",
                "TCN",
                "Hyper Parameter"
            ])
        layout.addWidget(regression_type_combo)

        select_button = QPushButton("OK")
        select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        cross_plot_dialog.setLayout(layout)
        cross_plot_dialog.adjustSize()
        cross_plot_dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        cross_plot_dialog.setWindowFlags(cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

        result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
        if result == QDialog.DialogCode.Accepted:
            regression_type_combo = regression_type_combo.currentText()
            # Trigger the next function after the dialog is closed
            self.open_csv_selection_dialog(parent_dialog, regression_type_combo, result_layout, analysis_type_combo)

    def open_cross_plot_options(self, parent_dialog, result_layout, analysis_type_combo):
        # Create a new dialog to select linear or non-linear regression
        cross_plot_dialog = QDialog(parent_dialog)
        cross_plot_dialog.setWindowTitle("Cross Plot Options")

        layout = QVBoxLayout(cross_plot_dialog)

        regression_type_combo = QComboBox()
        regression_type_combo.addItems(["Linear Regression", "Non-Linear Regression"])
        layout.addWidget(regression_type_combo)

        select_button = QPushButton("OK")
        select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        cross_plot_dialog.setLayout(layout)
        cross_plot_dialog.adjustSize()
        cross_plot_dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        cross_plot_dialog.setWindowFlags(cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

        result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
        if result == QDialog.DialogCode.Accepted:
            # Trigger the next function after the dialog is closed
            self.open_csv_selection_dialog(parent_dialog, regression_type_combo, result_layout, analysis_type_combo)

    def open_csv_selection_dialog(self, parent_dialog, regression_type_combo, result_layout, analysis_type_combo):
        # Create a new dialog to select CSV files
        csv_selection_dialog = QDialog(parent_dialog)
        csv_selection_dialog.setWindowTitle("Select Wells")

        layout = QVBoxLayout(csv_selection_dialog)

        csv_list_widget = QListWidget()
        csv_list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
        for key in self.tensor_dict:
            if isinstance(self.tensor_dict[key], pd.DataFrame):
                csv_list_widget.addItem(key)
        layout.addWidget(csv_list_widget)

        select_button = QPushButton("OK")
        select_button.clicked.connect(csv_selection_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        csv_selection_dialog.setLayout(layout)
        if csv_selection_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        # Trigger the next function after the dialog is closed
        self.show_common_columns(parent_dialog, csv_list_widget, regression_type_combo, result_layout,
                                 analysis_type_combo)

    def show_common_columns(self, parent_dialog, csv_list_widget, regression_type_combo, result_layout,
                            analysis_type_combo):
        # Get selected CSV files and find common columns
        selected_items = csv_list_widget.selectedItems()
        selected_files = [item.text() for item in selected_items]

        if len(selected_files) == 0:
            QMessageBox.warning(self, "Warning", "Please select at least one CSV file.")
            return

        def valid_columns(df):
            """Return columns that are not entirely NaN or negative after converting to numeric."""
            valid_columns = []
            for col in df.columns:
                numeric_col = pd.to_numeric(df[col], errors='coerce')
                # Check if the column is not entirely NaN and does not contain only negative values
                if not numeric_col.isna().all() and not (numeric_col < 0).all():
                    valid_columns.append(col)
            return valid_columns

        if len(selected_files) == 1:
            # If only one file is selected, directly use its columns
            data_frame = self.tensor_dict[selected_files[0]].copy()
            columns = valid_columns(data_frame)
        else:
            # If multiple files are selected, find common columns
            data_frames = [self.tensor_dict[file].copy() for file in selected_files]
            common_columns = set(valid_columns(data_frames[0]))
            for df in data_frames[1:]:
                common_columns.intersection_update(valid_columns(df))
            if not common_columns:
                QMessageBox.warning(self, "Warning", "No common columns found among the selected CSV files.")
                return
            columns = list(common_columns)

        common_columns_dialog = QDialog(parent_dialog)
        common_columns_dialog.setWindowTitle("Select Features")

        layout = QVBoxLayout(common_columns_dialog)

        common_columns_list = QListWidget()
        common_columns_list.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
        for col in columns:
            common_columns_list.addItem(col)
        layout.addWidget(common_columns_list)

        plot_button = QPushButton("OK")
        plot_button.clicked.connect(common_columns_dialog.accept)  # Close the current dialog
        layout.addWidget(plot_button)

        common_columns_dialog.setLayout(layout)
        if common_columns_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return
        # Get selected feature inputs
        selected_features = [item.text() for item in common_columns_list.selectedItems()]

        # Check for corresponding samples in selected columns
        if selected_features:
            # Read all selected CSVs into a combined DataFrame
            combined_df = pd.concat([self.tensor_dict[file].copy() for file in selected_files])

            # Convert selected columns to numeric and exclude NaNs and -999.25
            combined_df[selected_features] = combined_df[selected_features].apply(pd.to_numeric, errors='coerce')
            combined_df = combined_df.replace(-999.25, np.nan)

            # Count rows where all selected columns have valid (non-NaN) values
            valid_rows = combined_df.dropna(subset=selected_features)
            num_valid_points = len(valid_rows)

            if num_valid_points == 0:
                QMessageBox.warning(parent_dialog, "No Valid Data Points",
                                    "No data points have values for all selected columns.")
                return

            analysis_type = analysis_type_combo.currentText()

            if analysis_type == "Cross Plots":
                self.plot_cross_plot(selected_files, common_columns_list, regression_type_combo, result_layout,
                                     parent_dialog, num_valid_points)
            if analysis_type == "Clustering":

                self.run_Clustering(parent_dialog, selected_files, selected_features, regression_type_combo,
                                    result_layout, num_valid_points)
            elif analysis_type in ["Regression", "Classification"]:

                remaining_columns = list(set(columns) - set(selected_features))

                if not selected_features:
                    QMessageBox.warning(self, "Warning", "Please select at least one feature input.")
                    return
                if not remaining_columns:
                    QMessageBox.warning(self, "Warning", "No remaining columns available for the target variable.")
                    return

                # Proceed to select the target variable from the remaining columns
                self.select_target_variable(parent_dialog, selected_files, selected_features, remaining_columns,
                                            regression_type_combo, result_layout, analysis_type)
        else:
            QMessageBox.warning(self, "Warning", "No columns selected.")

    @staticmethod
    def run_Clustering_task(X_clean, dimensionality_reduction_type_combo, n_components, clustering_algorithm,
                            n_clusters, eps, x, y, learning_rate, sigma, log_dir_dim, log_dir_cluster):

        # Set up logging
        TensorVisualizer.setup_logging('Well_Clustering')

        inertia = None
        dtype = X_clean.dtype

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X_clean)

            X_clean = pca.transform(X_clean)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X_clean = tsne.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X_clean)
            X_clean = ica.transform(X_clean)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X_clean = grp.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X_clean.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X_clean, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(
                attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir_dim, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val),
                            callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X_clean = encoder.predict(X_clean)

        if clustering_algorithm == "K-Means":
            model = KMeans(n_clusters=n_clusters, random_state=42)
            labels = model.fit_predict(X_clean)
            inertia = model.inertia_

        elif clustering_algorithm == "Hierarchical":
            Z = linkage(X_clean, method='ward')
            labels = fcluster(Z, t=n_clusters, criterion='maxclust')

        elif clustering_algorithm == "DBSCAN":
            model = DBSCAN(eps=eps / 100, min_samples=n_clusters)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Gaussian Mixture":
            model = GaussianMixture(n_components=n_clusters, random_state=42)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Spectral":
            model = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Agglomerative":
            model = AgglomerativeClustering(n_clusters=n_clusters)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Birch":
            model = Birch(n_clusters=n_clusters)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Affinity Propagation":
            model = AffinityPropagation(random_state=42)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "Mean Shift":
            model = MeanShift()
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "OPTICS":
            model = OPTICS(min_samples=n_clusters)
            labels = model.fit_predict(X_clean)
        elif clustering_algorithm == "HDBSCAN":
            model = hdbscan.HDBSCAN(min_cluster_size=n_clusters)
            labels = model.fit_predict(X_clean)

        elif clustering_algorithm == "SOM":
            som = MiniSom(x=x, y=y, input_len=X_clean.shape[1], sigma=sigma, learning_rate=learning_rate / 100)
            som.random_weights_init(X_clean)
            som.train_random(X_clean, 100)
            labels = np.array([som.winner(x)[1] for x in X_clean])

        elif clustering_algorithm == "DEC":
            input_dim = X_clean.shape[1]
            input_layer = Input(shape=(input_dim,))
            encoder = Dense(500, activation='relu')(input_layer)
            encoder = Dense(500, activation='relu')(encoder)
            encoder = Dense(2000, activation='relu')(encoder)
            encoder = Dense(10, activation='relu')(encoder)
            decoder = Dense(2000, activation='relu')(encoder)
            decoder = Dense(500, activation='relu')(decoder)
            decoder = Dense(500, activation='relu')(decoder)
            decoder = Dense(input_dim, activation='sigmoid')(decoder)

            autoencoder = Model(inputs=input_layer, outputs=decoder)
            autoencoder.compile(optimizer='adam', loss='mse')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir_cluster, histogram_freq=1)

            autoencoder.fit(X_clean, X_clean, epochs=100, batch_size=256, shuffle=True,
                            callbacks=[early_stopping, tensorboard_callback])

            encoder_model = Model(inputs=input_layer, outputs=encoder)
            encoded_data = encoder_model.predict(X_clean)
            kmeans = KMeans(n_clusters=n_clusters, n_init=20)
            labels = kmeans.fit_predict(encoded_data)
            inertia = kmeans.inertia_

        return labels.astype(dtype), inertia

    def run_Clustering(self, parent_dialog, selected_files, selected_features, regression_type_combo, result_layout,
                       num_valid_points):

        try:
            n_clusters = None
            eps = None
            x = None
            y = None
            learning_rate = None
            sigma = None
            n_components = None

            dialog = QDialog(parent_dialog)
            dialog.setWindowTitle("Select Normalization Type")
            layout = QVBoxLayout()

            label = QLabel("Choose normalization type:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(["Min-Max Scaler", "Standard Scaler", "None"])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            # Disable the '?' help button on the dialog
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Accepted:
                normalization_choice = combo.currentText()
            else:
                return

            # Load and merge data from the selected CSV files
            data_frames = [self.tensor_dict[file].copy() for file in selected_files]
            merged_data = pd.concat(data_frames, join='inner', ignore_index=True)

            # Extract feature inputs and target variable
            X = merged_data[selected_features]

            # Convert all columns to numeric, forcing errors to NaN
            X = X.apply(pd.to_numeric, errors='coerce')

            # Replace negative values with NaN
            X = X.applymap(lambda x: pd.NA if x < 0 else x)

            # Drop rows with NaN values
            X = X.dropna()

            # Extract the cleaned feature inputs and target variable
            X_clean = X[selected_features]
            X_clean_copy = X_clean.copy()

            # Normalize the feature inputs based on the user's choice
            if normalization_choice == "Min-Max Scaler":
                scaler = MinMaxScaler()
                X_clean = scaler.fit_transform(X_clean)
            elif normalization_choice == "Standard Scaler":
                scaler = StandardScaler()
                X_clean = scaler.fit_transform(X_clean)

            # Convert to numpy arrays
            X_clean = np.asarray(X_clean, dtype=np.float32)

            # User input simulation (for demonstration purposes)
            clustering_algorithm = regression_type_combo

            # Create a new dialog to select linear or non-linear regression
            cross_plot_dialog = QDialog(parent_dialog)
            cross_plot_dialog.setWindowTitle("Dimensionality Reduction Options")

            layout = QVBoxLayout(cross_plot_dialog)

            dimensionality_reduction_type_combo = QComboBox()
            dimensionality_reduction_type_combo.addItems([
                "None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"

            ])
            layout.addWidget(dimensionality_reduction_type_combo)

            select_button = QPushButton("OK")
            select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
            layout.addWidget(select_button)

            cross_plot_dialog.setLayout(layout)
            cross_plot_dialog.adjustSize()
            cross_plot_dialog.setMinimumWidth(250)
            # Disable the '?' help button on the dialog
            cross_plot_dialog.setWindowFlags(
                cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

            result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
            if result == QDialog.DialogCode.Accepted:
                dimensionality_reduction_type_combo = dimensionality_reduction_type_combo.currentText()

                if dimensionality_reduction_type_combo in ["t-SNE", "ICA", "Random Projection", "PCA",
                                                           "Autoencoder"]:

                    dialog = QDialog(parent_dialog)
                    dialog.setWindowTitle("Principal Components")
                    layout = QVBoxLayout()

                    input_field_label = QLabel()
                    input_field_label.setText("Select the Number of Components:")
                    layout.addWidget(input_field_label)

                    input_field = QLineEdit()
                    # Using QDoubleValidator to allow decimals and integers
                    validator = QDoubleValidator(0, X_clean_copy.shape[1], 2, input_field)
                    validator.setNotation(QDoubleValidator.Notation.StandardNotation)
                    input_field.setValidator(validator)
                    layout.addWidget(input_field)

                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(225)
                    # Disable the '?' help button on the dialog
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        n_components = float(input_field.text())
                        if n_components.is_integer():
                            # Convert to int if the number is an integer
                            n_components = int(n_components)
                    else:
                        return
            else:
                return

            if clustering_algorithm == "DBSCAN":

                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Parameters")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of minimum samples")
                layout.addWidget(label)

                # Add a spin box for integer input
                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(99)
                spinBox.setValue(10)
                layout.addWidget(spinBox)

                epsilonlabel = QLabel("Choose Epsilon")
                layout.addWidget(epsilonlabel)

                # Add a spin box for integer input
                epsilonBox = QSpinBox()
                epsilonBox.setMinimum(1)
                epsilonBox.setMaximum(99)
                epsilonBox.setValue(10)
                layout.addWidget(epsilonBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
            elif clustering_algorithm == "OPTICS":
                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Number of minimum samples")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of clusters")
                layout.addWidget(label)

                # Add a spin box for integer input
                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(99)
                spinBox.setValue(10)
                layout.addWidget(spinBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
            elif clustering_algorithm == "SOM":
                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Parameters")
                layout = QVBoxLayout()

                label = QLabel("Choose the X number")
                layout.addWidget(label)

                # Add a spin box for integer input
                xspinBox = QSpinBox()
                xspinBox.setMinimum(1)
                xspinBox.setMaximum(99)
                xspinBox.setValue(10)
                layout.addWidget(xspinBox)

                epsilonlabel = QLabel("Choose the Y number")
                layout.addWidget(epsilonlabel)

                # Add a spin box for integer input
                yBox = QSpinBox()
                yBox.setMinimum(1)
                yBox.setMaximum(99)
                yBox.setValue(10)
                layout.addWidget(yBox)

                label = QLabel("Choose Sigma")
                layout.addWidget(label)

                # Add a spin box for integer input
                SigmaspinBox = QSpinBox()
                SigmaspinBox.setMinimum(1)
                SigmaspinBox.setMaximum(99)
                SigmaspinBox.setValue(1)
                layout.addWidget(SigmaspinBox)

                epsilonlabel = QLabel("Choose learning_rate")
                layout.addWidget(epsilonlabel)

                # Add a spin box for integer input
                learning_rateBox = QSpinBox()
                learning_rateBox.setMinimum(1)
                learning_rateBox.setMaximum(99)
                learning_rateBox.setValue(50)
                layout.addWidget(learning_rateBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
            else:
                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Number of clusters")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of clusters")
                layout.addWidget(label)

                # Add a spin box for integer input
                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(99)
                spinBox.setValue(10)
                layout.addWidget(spinBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()

            if dialog.exec() == QDialog.DialogCode.Accepted:
                if clustering_algorithm == "DBSCAN":
                    eps = epsilonBox.value()
                    n_clusters = spinBox.value()
                elif clustering_algorithm == "SOM":
                    x = xspinBox.value()
                    y = yBox.value()
                    learning_rate = learning_rateBox.value()
                    sigma = SigmaspinBox.value()
                else:
                    n_clusters = spinBox.value()

                if dimensionality_reduction_type_combo == "Autoencoder":  # Set up TensorBoard callback
                    log_dir_dim = os.path.join("logs", "Tensorflow", "autoencoder_DR_clustering_well",
                                               datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
                else:
                    log_dir_dim = None

                if clustering_algorithm == "DEC":  # Set up TensorBoard callback
                    log_dir_cluster = os.path.join("logs", "Tensorflow", "autoencoder_DEC_clustering_well",
                                                   datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
                else:
                    log_dir_cluster = None

                result = self.task_runner.run_task(TensorVisualizer.run_Clustering_task, X_clean,
                                                            dimensionality_reduction_type_combo, n_components,
                                                            clustering_algorithm, n_clusters, eps, x, y, learning_rate,
                                                            sigma, log_dir_dim, log_dir_cluster)

                if result is not None:

                    labels, inertia = result

                    if dimensionality_reduction_type_combo == "Autoencoder":  # Set up TensorBoard callback
                        self.show_tensorboard(log_dir=log_dir_dim)

                    if clustering_algorithm == "DEC":  # Set up TensorBoard callback
                        self.show_tensorboard(log_dir=log_dir_cluster)

                    # Create a unique column name based on the clustering algorithm
                    cluster_column_name = f"Clusters_{clustering_algorithm}"

                    # Assign the labels back to the merged data
                    merged_data_clean = X_clean_copy.copy()  # Make a copy to preserve the cleaned data
                    merged_data_clean[cluster_column_name] = labels  # Add the cluster labels

                    # Now map the labels back to each original DataFrame in self.tensor_dict
                    for file in selected_files:
                        # Get the original DataFrame
                        original_df = self.tensor_dict[file]

                        # Drop any existing column with the same name
                        if cluster_column_name in original_df.columns:
                            original_df.drop(columns=[cluster_column_name], inplace=True)

                        # Initialize the new column with the placeholder value -999.25
                        original_df[cluster_column_name] = -999.25

                        # Ensure the data types of the columns in original_df and merged_data_clean match
                        for feature in selected_features:
                            original_df[feature] = original_df[feature].astype(float)
                            merged_data_clean[feature] = merged_data_clean[feature].astype(float)

                        # Create a unique identifier by concatenating the selected features
                        original_df['merge_key'] = original_df[selected_features].astype(str).agg('-'.join, axis=1)
                        merged_data_clean['merge_key'] = merged_data_clean[selected_features].astype(str).agg('-'.join,
                                                                                                              axis=1)

                        # Create a mapping from the unique keys to the labels
                        label_mapping = merged_data_clean.set_index('merge_key')[cluster_column_name].to_dict()

                        # Map the labels back to the original DataFrame
                        original_df[cluster_column_name] = original_df['merge_key'].map(label_mapping).fillna(-999.25)

                        # Drop the auxiliary column
                        original_df.drop(columns=['merge_key'], inplace=True)

                        # Update the original DataFrame in tensor_dict
                        self.tensor_dict[file] = original_df

                    # Pass data to the plotting function
                    self.plot_cluster_results(labels, X_clean_copy, clustering_algorithm, result_layout, selected_files,
                                              selected_features, parent_dialog, inertia, num_valid_points)

        except Exception as e:
            traceback.print_exc()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_cluster_results(self, labels, X_clean_copy, clustering_algorithm, result_layout, selected_files,
                             selected_features, parent_dialog, inertia, num_valid_points):

        # Create a dialog to select the target variable
        target_variable_dialog = QDialog(parent_dialog)
        target_variable_dialog.setWindowTitle("Select plotting features")

        layout = QVBoxLayout(target_variable_dialog)

        label = QLabel()
        label.setText("Select two features:")
        layout.addWidget(label)

        target_variable_list = QListWidget()
        target_variable_list.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
        for col in selected_features:
            target_variable_list.addItem(col)
        layout.addWidget(target_variable_list)

        select_button = QPushButton("OK")
        select_button.clicked.connect(target_variable_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        target_variable_dialog.setLayout(layout)
        if target_variable_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        # Get the selected target variable
        selected_target_items = target_variable_list.selectedItems()
        if len(selected_target_items) < 2 or len(selected_target_items) > 2:
            QMessageBox.warning(self, "Warning", "Please select two features.")
            return

        QApplication.setOverrideCursor(self.custom_cursor)

        selected_item_x = selected_target_items[0].text()
        selected_item_y = selected_target_items[1].text()

        # Extract the cleaned feature inputs and target variable
        X_clean = X_clean_copy[selected_item_x]
        Y_clean = X_clean_copy[selected_item_y]

        # Convert to numpy arrays
        X_clean = np.asarray(X_clean, dtype=np.float32)
        Y_clean = np.asarray(Y_clean, dtype=np.float32)

        # Define a discrete colormap
        unique_labels = np.unique(labels)
        num_labels = len(unique_labels)

        self.well_color_data = labels / num_labels

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

        # Map labels to colors and convert to QColor for pyqtgraph
        colors = cmap.map(self.well_color_data, mode='qcolor')

        # Create a list of brushes for each color
        brushes = [pg.mkBrush(color) for color in colors]

        # Clear the previous plot and reset the view
        self.plot_widget.clear()
        self.plot_widget.autoRange()

        # Create scatter plot item for actual vs. predicted values
        scatter = pg.ScatterPlotItem(x=X_clean, y=Y_clean, pen=None, brush=brushes)
        self.plot_widget.addItem(scatter)

        # Retrieve the units for the target variable from the selected file
        selected_file_key = selected_files[0] if selected_files else None
        units = self.units_dict.get(selected_file_key, {})  # Dictionary of units for the selected file
        x_unit = units.get(selected_item_x, '')  # Get the unit for the target column
        y_unit = units.get(selected_item_y, '')  # Get the unit for the target column

        # Set axis labels with units
        x_unit_label = f"{selected_item_x} ({x_unit})"
        y_unit_label = f"{selected_item_y} ({y_unit})"

        self.plot_widget.setLabel('bottom', x_unit_label)  # x-axis label with unit
        self.plot_widget.setLabel('left', y_unit_label)  # y-axis label with unit

        # Create or update the ColorBarItem for the color bar
        if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
            self.well_color_bar.setLevels((labels.min(), labels.max()))
            self.plot_widget.getPlotItem().layout.removeItem(self.color_bar_label_item)
            self.well_color_bar.label = "Clusters"  # Update the color bar title with the third column name
            self.well_color_bar.setColorMap(cmap)
        else:
            color_bar = pg.ColorBarItem(values=(labels.min(), labels.max()))
            color_bar.setColorMap(cmap)
            color_bar.label = "Clusters"  # Set the color bar title to the third column name
            self.well_color_bar = color_bar
            self.plot_widget.getPlotItem().layout.addItem(self.well_color_bar, 2,
                                                          2)  # Add to the layout in the next column

        # Add vertical label for the color bar with units
        self.color_bar_label_item = pg.LabelItem(text="Clusters", angle=90,
                                                 color='k', size='10pt')
        self.plot_widget.getPlotItem().layout.addItem(self.color_bar_label_item, 2, 2, 1, 1)

        self.plot_widget.getPlotItem().layout.setContentsMargins(0, 0, 30,
                                                                 0)  # Adjust the values as needed

        self.plot_widget.autoRange()

        self.well_color_bar.show()

        # Optionally, display statistics in the result layout
        result_text = f"{clustering_algorithm} Results:\n"
        result_text += f"Features: {' '.join(selected_features)}\n"

        result_text += f"Number of Samples: {num_valid_points}\n"

        # Silhouette Score
        silhouette_avg = silhouette_score(X_clean_copy, labels)
        result_text += f"Silhouette Score: {silhouette_avg:.2f}\n"

        # Davies-Bouldin Index
        db_index = davies_bouldin_score(X_clean_copy, labels)
        result_text += f"Davies-Bouldin Index: {db_index:.2f}\n"

        # Calinski-Harabasz Index
        ch_index = calinski_harabasz_score(X_clean_copy, labels)
        result_text += f"Calinski-Harabasz Index: {ch_index:.2f}\n"

        # Inertia (K-Means specific)
        if inertia is not None:
            result_text += f"Inertia: {inertia:.2f}\n"

        QApplication.restoreOverrideCursor()

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def select_target_variable(self, parent_dialog, selected_files, selected_features, remaining_columns,
                               regression_type_combo, result_layout, analysis_type):
        # Create a dialog to select the target variable
        target_variable_dialog = QDialog(parent_dialog)
        target_variable_dialog.setWindowTitle("Select Target Variable")

        layout = QVBoxLayout(target_variable_dialog)

        target_variable_list = QListWidget()
        target_variable_list.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
        for col in remaining_columns:
            target_variable_list.addItem(col)

        layout.addWidget(target_variable_list)

        select_button = QPushButton("OK")
        select_button.clicked.connect(target_variable_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        target_variable_dialog.setLayout(layout)
        if target_variable_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        # Get the selected target variable
        selected_target_items = target_variable_list.selectedItems()
        if len(selected_target_items) == 0:
            QMessageBox.warning(self, "Warning", "Please select a target variable.")
            return

        selected_target = selected_target_items[0].text()

        # Check for valid data points across selected features and target variable
        combined_df = pd.concat([self.tensor_dict[file].copy() for file in selected_files])
        combined_df[selected_features + [selected_target]] = combined_df[selected_features + [selected_target]].apply(
            pd.to_numeric, errors='coerce')
        combined_df = combined_df.replace(-999.25, np.nan)
        valid_rows = combined_df.dropna(subset=selected_features + [selected_target])
        num_valid_points = len(valid_rows)

        if num_valid_points == 0:
            QMessageBox.warning(parent_dialog, "No Valid Data Points",
                                "No data points have values for all selected columns including the target variable.")
            return

        # Now you have selected_features and selected_target, and can proceed to the next steps
        # e.g., applying the regression methods on the selected columns
        self.data_preparation(selected_files, selected_features, selected_target, regression_type_combo, result_layout,
                              parent_dialog, analysis_type, num_valid_points)

    def data_preparation(self, selected_files, selected_features, selected_target, regression_type_combo,
                         result_layout, parent_dialog, analysis_type, num_valid_points):

        dialog = QDialog(parent_dialog)
        dialog.setWindowTitle("Select Normalization Type")
        layout = QVBoxLayout()

        label = QLabel("Choose normalization type:")
        layout.addWidget(label)

        combo = QComboBox()
        combo.addItems(["Min-Max Scaler", "Standard Scaler", "None"])
        layout.addWidget(combo)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()
        if dialog.exec() == QDialog.DialogCode.Accepted:
            normalization_choice = combo.currentText()
        else:
            return

        # Load and merge data from the selected CSV files
        data_frames = [self.tensor_dict[file].copy() for file in selected_files]
        merged_data = pd.concat(data_frames, join='inner', ignore_index=True)

        # Replace -999.25 with NaN
        merged_data = merged_data.apply(pd.to_numeric, errors='coerce').replace(-999.25, np.nan)

        # Extract feature inputs and target variable
        X = merged_data[selected_features]
        y = merged_data[selected_target]

        # Convert all columns to numeric, forcing errors to NaN
        X = X.apply(pd.to_numeric, errors='coerce')
        y = y.apply(pd.to_numeric, errors='coerce')

        # Replace negative values with NaN
        X = X.applymap(lambda x: pd.NA if x < 0 else x)
        y = y.apply(lambda x: pd.NA if x < 0 else x)

        # Drop rows with NaN values
        X = X.dropna()
        y = y.dropna()

        # Ensure X and y have the same length
        merged_cleaned_data = X.join(y, how='inner')

        # Extract the cleaned feature inputs and target variable
        X_clean = merged_cleaned_data[selected_features]
        X_clean_copy = X_clean.copy()
        y_clean = merged_cleaned_data[selected_target]

        # Normalize the feature inputs based on the user's choice
        if normalization_choice == "Min-Max Scaler":
            scaler = MinMaxScaler()
            X_clean = scaler.fit_transform(X_clean)
        elif normalization_choice == "Standard Scaler":
            scaler = StandardScaler()
            X_clean = scaler.fit_transform(X_clean)

        # Convert to numpy arrays
        X_clean = np.asarray(X_clean, dtype=np.float32)
        y_clean = np.asarray(y_clean, dtype=np.float32)

        self.dimensionality_reduction(parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                                      selected_files, selected_target, selected_features, regression_type_combo,
                                      num_valid_points, analysis_type)

    @staticmethod
    def well_dimensionality_reduction_task(X_clean, dimensionality_reduction_type_combo, n_components, log_dir):

        # Set up logging
        TensorVisualizer.setup_logging('Well_Dimensionality_Reduction')

        dtype = X_clean.dtype

        if dimensionality_reduction_type_combo == "PCA":
            # Create PCA object to retain 90% of variance
            pca = PCA(n_components=n_components)

            # Fit PCA model to data
            pca.fit(X_clean)

            X_clean = pca.transform(X_clean)

        elif dimensionality_reduction_type_combo == "t-SNE":
            # Create t-SNE object
            tsne = TSNE(n_components=n_components)
            # Fit and transform data
            X_clean = tsne.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "ICA":
            # Create ICA object
            ica = FastICA(n_components=n_components, max_iter=500)
            # Fit ICA model to data
            ica.fit(X_clean)
            X_clean = ica.transform(X_clean)

        elif dimensionality_reduction_type_combo == "Random Projection":
            # Create Random Projection object
            grp = GaussianRandomProjection(n_components=n_components)
            # Fit and transform data
            X_clean = grp.fit_transform(X_clean)

        elif dimensionality_reduction_type_combo == "Autoencoder":
            # Define the input dimension based on X_queen's shape
            input_dim = X_clean.shape[1]

            # Split the data into training and validation sets
            X_train, X_val = train_test_split(X_clean, test_size=0.2, random_state=42)

            # Define the encoding dimension
            encoding_dim = max(1, n_components)

            # Input layer
            input_layer = Input(shape=(input_dim,))

            # Encoder: Dense layers
            x = Dense(128, activation='relu')(input_layer)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Check if we can reshape it properly for Conv1D
            if x.shape[1] == input_dim:  # Only reshape if it matches input_dim
                x = Reshape((input_dim, 1))(x)  # Reshape for Conv1D
                x = Conv1D(32, 3, activation='relu', padding='same')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.2)(x)
                x = Flatten()(x)  # Flatten to pass to dense layers
            else:
                # Skip Conv1D if reshape isn't possible directly from Dense
                x = Dense(64, activation='relu')(x)

            # Multi-Head Attention for capturing different "views" or dependencies
            x = Reshape((1, -1))(x)  # Reshape to (batch_size, 1, features)
            attention_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            attention_output = Dropout(0.2)(attention_output)
            attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection
            attention_output = Reshape((-1,))(attention_output)  # Flatten

            # Final dense layer to reduce to the encoding dimension
            encoded = Dense(encoding_dim, activation='relu')(attention_output)

            # Decoder: Reverse the process
            x = Dense(128, activation='relu')(encoded)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Adding Dense layers
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)

            # Reconstructing the original input dimensions
            decoded = Dense(input_dim, activation='sigmoid', dtype='float32')(x)

            # Autoencoder model
            autoencoder = Model(input_layer, decoded)

            # Compile the model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')

            # Encoder model to reduce dimensionality
            encoder = Model(input_layer, encoded)

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            # Train the autoencoder with training-validation split
            autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True,
                            validation_data=(X_val, X_val),
                            callbacks=[early_stopping, tensorboard_callback])

            # Use the encoder to transform the data
            X_clean = encoder.predict(X_clean)

        return X_clean.astype(dtype)

    def dimensionality_reduction(self, parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                                 selected_files, selected_target, selected_features, regression_type_combo,
                                 num_valid_points, analysis_type):
        n_components = None
        # Create a new dialog to select linear or non-linear regression
        cross_plot_dialog = QDialog(parent_dialog)
        cross_plot_dialog.setWindowTitle("Dimensionality Reduction Options")

        layout = QVBoxLayout(cross_plot_dialog)

        dimensionality_reduction_type_combo = QComboBox()
        dimensionality_reduction_type_combo.addItems([
            "None", "t-SNE", "ICA", "Random Projection", "PCA", "Autoencoder"

        ])
        layout.addWidget(dimensionality_reduction_type_combo)

        select_button = QPushButton("OK")
        select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        cross_plot_dialog.setLayout(layout)
        cross_plot_dialog.adjustSize()
        cross_plot_dialog.setMinimumWidth(250)
        # Disable the '?' help button on the dialog
        cross_plot_dialog.setWindowFlags(cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

        result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
        if result == QDialog.DialogCode.Accepted:
            dimensionality_reduction_type_combo = dimensionality_reduction_type_combo.currentText()

            if dimensionality_reduction_type_combo in ["t-SNE", "ICA", "Random Projection", "PCA",
                                                       "Autoencoder"]:

                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Principal Components")
                layout = QVBoxLayout()

                input_field_label = QLabel()
                input_field_label.setText("Select the Number of Components:")
                layout.addWidget(input_field_label)

                input_field = QLineEdit()
                # Using QDoubleValidator to allow decimals and integers
                validator = QDoubleValidator(0, X_clean_copy.shape[1], 2, input_field)
                validator.setNotation(QDoubleValidator.Notation.StandardNotation)
                input_field.setValidator(validator)
                layout.addWidget(input_field)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                # Disable the '?' help button on the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    n_components = float(input_field.text())
                    if n_components.is_integer():
                        # Convert to int if the number is an integer
                        n_components = int(n_components)

            if dimensionality_reduction_type_combo == "Autoencoder":
                # Set up TensorBoard callback
                log_dir = os.path.join("logs", "Tensorflow", f"autoencoder_DR_{analysis_type}_well",
                                       datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            else:
                log_dir = None

            if dimensionality_reduction_type_combo in ["t-SNE", "ICA", "Random Projection", "PCA",
                                                       "Autoencoder"]:
                X_clean = self.task_runner.run_task(TensorVisualizer.well_dimensionality_reduction_task, X_clean,
                                                dimensionality_reduction_type_combo, n_components, log_dir)

            if X_clean is not None:

                if dimensionality_reduction_type_combo == "Autoencoder":
                    self.show_tensorboard(log_dir=log_dir)

                if analysis_type == "Regression":
                    self.run_regression(parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                                        regression_type_combo, selected_files, selected_target, selected_features,
                                        num_valid_points, dimensionality_reduction_type_combo, n_components)
                elif analysis_type == "Classification":
                    self.run_well_categorization(parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                                                 regression_type_combo, selected_files, selected_target,
                                                 selected_features,
                                                 num_valid_points, dimensionality_reduction_type_combo, n_components)

    @staticmethod
    def run_well_regression_task(X_clean, y_clean, train_test, num_lstm_layers_min, regression_type_combo, epochs,
                                 max_trials, num_lstm_layers_max, num_lstm_layers_step, lstm_units_min,
                                 lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                 l2_lstm_step, min_dropout_lstm, max_dropout_lstm, dropout_lstm_layer_step,
                                 dropout_lstm_min, dropout_lstm_max, dropout_lstm_step, num_conv_layers_min,
                                 num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                 conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                 conv_kernel_size_max, conv_kernel_size_step, l2_conv_min, l2_conv_max,
                                 l2_conv_step, min_dropout_conv, max_dropout_conv, dropout_conv_layer_step,
                                 dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                 num_attention_layers_min, num_attention_layers_max, num_attention_layers_step,
                                 attention_heads_min, attention_heads_max, attention_heads_step,
                                 attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                                 l2_attention_min, l2_attention_max, l2_attention_step,
                                 min_dropout_attention, max_dropout_attention, dropout_attention_layer_step,
                                 dropout_attention_min, dropout_attention_max, dropout_attention_step,
                                 num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                                 dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                 l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                                 dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                 dropout_dense_step, learning_rate_min, learning_rate_max,
                                 learning_rate_step, Loss_type_combo, log_dir, batch_size, file_path):

        # Set up logging
        TensorVisualizer.setup_logging('Well_Regression')

        dtype = y_clean.dtype

        # Split into training and validation sets (80% train, 20% validation)
        X_train, X_val, y_train, y_val = train_test_split(X_clean, y_clean, test_size=train_test / 100, random_state=42)

        # Function to create a simple Keras model for regression
        def create_keras_model():
            model = Sequential()
            model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
            model.add(Dense(32, activation='relu'))
            model.add(Dense(1, activation='linear'))
            model.compile(loss='mse', optimizer='adam', metrics=['mae'])
            return model

        # Define the hypermodel function using the Functional API
        def build_model(hp):
            input_shape = (X_train.shape[1], X_train.shape[2])
            inputs = tf.keras.Input(shape=input_shape)
            x = inputs

            # Add LSTM layers with Dropout and L2 regularization
            num_lstm_layers = hp.Int('num_lstm_layers', min_value=num_lstm_layers_min, max_value=num_lstm_layers_max,
                                     step=num_lstm_layers_step)

            for i in range(num_lstm_layers):
                x = tf.keras.layers.LSTM(
                    units=hp.Int(f'lstm_units_{i}', min_value=lstm_units_min, max_value=lstm_units_max,
                                 step=lstm_units_step),
                    return_sequences=True,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_lstm_{i}', min_value=l2_lstm_min, max_value=l2_lstm_max, step=l2_lstm_step)
                    )
                )(x)

                num_dropout_lstm = hp.Int(f'num_dropout_lstm_{i}', min_value=min_dropout_lstm,
                                          max_value=max_dropout_lstm, step=dropout_lstm_layer_step)
                for _ in range(num_dropout_lstm):
                    dropout_rate = hp.Float(f'dropout_rate_lstm_{i}', min_value=dropout_lstm_min,
                                            max_value=dropout_lstm_max, step=dropout_lstm_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add Conv1D layers with Dropout and L2 regularization
            num_conv_layers = hp.Int('num_conv_layers', min_value=num_conv_layers_min, max_value=num_conv_layers_max,
                                     step=num_conv_layers_step)

            for i in range(num_conv_layers):
                x = tf.keras.layers.Conv1D(
                    filters=hp.Int(f'conv_filters_{i}', min_value=conv_filters_min, max_value=conv_filters_max,
                                   step=conv_filters_step),
                    kernel_size=hp.Int(f'conv_kernel_size_{i}', min_value=conv_kernel_size_min,
                                       max_value=conv_kernel_size_max, step=conv_kernel_size_step),
                    activation='relu',
                    padding='same',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_conv_{i}', min_value=l2_conv_min, max_value=l2_conv_max, step=l2_conv_step)
                    )
                )(x)

                num_dropout_conv = hp.Int(f'num_dropout_conv_{i}', min_value=min_dropout_conv,
                                          max_value=max_dropout_conv, step=dropout_conv_layer_step)
                for _ in range(num_dropout_conv):
                    dropout_rate = hp.Float(f'dropout_rate_conv_{i}', min_value=dropout_conv_min,
                                            max_value=dropout_conv_max, step=dropout_conv_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add MultiHeadAttention layers with Dropout and L2 regularization
            num_attention_layers = hp.Int('num_attention_layers', min_value=num_attention_layers_min,
                                          max_value=num_attention_layers_max, step=num_attention_layers_step)

            for i in range(num_attention_layers):
                num_heads = hp.Int(f'num_attention_heads_{i}', min_value=attention_heads_min,
                                   max_value=attention_heads_max, step=attention_heads_step)
                key_dim = hp.Int(f'attention_key_dim_{i}', min_value=attention_key_dim_min,
                                 max_value=attention_key_dim_max, step=attention_key_dim_step)

                attention_layer = tf.keras.layers.MultiHeadAttention(
                    num_heads=num_heads,
                    key_dim=key_dim,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_attention_{i}', min_value=l2_attention_min, max_value=l2_attention_max,
                                 step=l2_attention_step)
                    )
                )
                attention_output = attention_layer(query=x, value=x)
                x = tf.keras.layers.Add()([x, attention_output])  # Residual connection

                num_dropout_attention = hp.Int(f'num_dropout_attention_{i}', min_value=min_dropout_attention,
                                               max_value=max_dropout_attention, step=dropout_attention_layer_step)
                for _ in range(num_dropout_attention):
                    dropout_rate = hp.Float(f'dropout_rate_attention_{i}', min_value=dropout_attention_min,
                                            max_value=dropout_attention_max, step=dropout_attention_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Flatten the output for Dense layers
            x = tf.keras.layers.Flatten()(x)

            # Add Dense layers with Dropout and L2 regularization
            num_dense_layers = hp.Int('num_dense_layers', min_value=num_dense_layers_min,
                                      max_value=num_dense_layers_max, step=num_dense_layers_step)

            for i in range(num_dense_layers):
                x = tf.keras.layers.Dense(
                    units=hp.Int(f'dense_units_{i}', min_value=dense_units_min, max_value=dense_units_max,
                                 step=dense_units_step),
                    activation='relu',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_dense_{i}', min_value=l2_dense_min, max_value=l2_dense_max, step=l2_dense_step)
                    )
                )(x)

                num_dropout_dense = hp.Int(f'num_dropout_dense_{i}', min_value=min_dropout_dense,
                                           max_value=max_dropout_dense, step=dropout_dense_layer_step)
                for _ in range(num_dropout_dense):
                    dropout_rate = hp.Float(f'dropout_rate_dense_{i}', min_value=dropout_dense_min,
                                            max_value=dropout_dense_max, step=dropout_dense_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Output layer
            output_units = y_train.shape[1] if len(y_train.shape) > 1 else 1
            outputs = tf.keras.layers.Dense(output_units)(x)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=hp.Float('learning_rate', min_value=learning_rate_min, max_value=learning_rate_max,
                                           step=learning_rate_step)
                ),
                loss=Loss_type_combo
            )

            return model

        # Initialize the model based on the regression type
        if regression_type_combo == "Ridge Regression":
            model = Ridge()

        elif regression_type_combo == "Lasso Regression":
            model = Lasso()

        elif regression_type_combo == "Bayesian Ridge Regression":
            model = BayesianRidge()

        elif regression_type_combo == "Random Forest Regressor":
            model = RandomForestRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Gradient Boosting Regressor":
            model = GradientBoostingRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "AdaBoost Regressor":
            model = AdaBoostRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Support Vector Regressor":
            model = SVR()

        elif regression_type_combo == "K-Nearest Neighbors Regressor":
            model = KNeighborsRegressor()

        elif regression_type_combo == "Decision Tree Regressor":
            model = DecisionTreeRegressor()

        elif regression_type_combo == "Gaussian Process Regressor":
            model = GaussianProcessRegressor()

        elif regression_type_combo == "XGBoost Regressor":
            model = XGBRegressor(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "CatBoost Regressor":
            model = CatBoostRegressor(iterations=epochs)  # Using epochs as number of iterations

        elif regression_type_combo == "Keras Regressor":
            model = create_keras_model()

        elif regression_type_combo == "ResNet":
            input_dim = X_train.shape[1]
            model = ResNet1D(input_dim=input_dim)

        elif regression_type_combo == "TabNet":
            model = TabNetRegressor()

        elif regression_type_combo == "TCN":
            num_channels = [25, 50, 100]  # Example configuration
            model = TCN(num_inputs=1, num_channels=num_channels)

        elif regression_type_combo == "LightGBM Regressor":
            model = LGBMRegressor(n_estimators=100 * epochs)

        elif regression_type_combo == "NGBoost Regressor":
            model = NGBRegressor(n_estimators=100 * epochs)  # Using epochs as number of estimators
        elif regression_type_combo == "Hyper Parameter":

            X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
            X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

            # Create a temporary directory for the tuner
            with tempfile.TemporaryDirectory() as temp_dir:
                tuner = BayesianOptimization(
                    hypermodel=build_model,
                    objective='val_loss',
                    max_trials=max_trials,
                    directory=temp_dir,  # Use the temporary directory
                    project_name='Hyper_Parameter_Tuning'
                )

            # Perform the search
            tuner.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=batch_size)

            # Get the best model
            model = tuner.get_best_models(num_models=1)[0]

        # Fit the model
        if regression_type_combo == "Keras Regressor":
            # Set up TensorBoard callback
            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=10,
                      callbacks=[tensorboard_callback])

        elif regression_type_combo == "Hyper Parameter":
            # Set up TensorBoard callback
            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

            model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[tensorboard_callback])

        elif regression_type_combo == "ResNet":
            # Check if GPU is available and move model to GPU
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)

            # Define the loss function and optimizer
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            scaler = GradScaler()

            # TensorBoard setup
            writer = SummaryWriter(log_dir)

            # Convert training and validation data to PyTorch tensors and move to GPU if available
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
            y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)  # Reshape y only for ResNet
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
            y_val_tensor = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float32)  # Reshape y only for ResNet

            # Create DataLoader for mini-batches
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

            # Log the model graph using a sample batch
            example_input_batch = next(iter(train_loader))[0].to(device)
            writer.add_graph(model, example_input_batch)

            # Training loop with mini-batches and mixed precision
            for epoch in range(epochs):
                model.train()
                epoch_loss = 0

                for X_batch, y_batch in train_loader:
                    X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)
                    optimizer.zero_grad()

                    # Forward pass with mixed precision
                    with autocast():
                        outputs = model(X_batch)
                        loss = criterion(outputs, y_batch)

                    # Backward pass with mixed precision
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()

                    epoch_loss += loss.item()

                # Validation
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for X_val_batch, y_val_batch in val_loader:
                        X_val_batch, y_val_batch = X_val_batch.to(device, non_blocking=True), \
                            y_val_batch.to(device, non_blocking=True)
                        with autocast():
                            val_outputs = model(X_val_batch)
                            val_loss += criterion(val_outputs, y_val_batch).item()

                avg_loss = epoch_loss / len(train_loader)
                # Log the average training loss for this epoch
                writer.add_scalar('Loss/Train', avg_loss, epoch)

                avg_val_loss = val_loss / len(val_loader)
                # Log validation loss
                writer.add_scalar('Loss/Validation', avg_val_loss, epoch)

            writer.close()

        elif regression_type_combo == "TabNet":
            y_train = y_train.reshape(-1, 1)
            y_val = y_val.reshape(-1, 1)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_name=['val'],
                eval_metric=['rmse'],
                max_epochs=epochs,
                patience=10,
                batch_size=64,
                virtual_batch_size=32,
                num_workers=0,
                drop_last=False
            )

        elif regression_type_combo == "TCN":
            # GPU handling for TCN
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)

            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            scaler = GradScaler()

            # TensorBoard setup
            writer = SummaryWriter(log_dir)

            # TCN expects a 1D input in the format (batch_size, channels, length)
            X_train_tensor = torch.tensor(X_train.reshape(X_train.shape[0], 1, -1), dtype=torch.float32)
            y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)
            X_val_tensor = torch.tensor(X_val.reshape(X_val.shape[0], 1, -1), dtype=torch.float32)
            y_val_tensor = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float32)

            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

            # Log the model graph using a sample batch
            example_input_batch = next(iter(train_loader))[0].to(device)
            writer.add_graph(model, example_input_batch)

            for epoch in range(epochs):
                model.train()
                epoch_loss = 0

                for X_batch, y_batch in train_loader:
                    X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)
                    optimizer.zero_grad()

                    with autocast():
                        outputs = model(X_batch)
                        loss = criterion(outputs, y_batch)

                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()

                    epoch_loss += loss.item()

                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for X_val_batch, y_val_batch in val_loader:
                        X_val_batch, y_val_batch = X_val_batch.to(device, non_blocking=True), \
                            y_val_batch.to(device, non_blocking=True)
                        with autocast():
                            val_outputs = model(X_val_batch)
                            val_loss += criterion(val_outputs, y_val_batch).item()

                avg_loss = epoch_loss / len(train_loader)
                # Log the average training loss for this epoch
                writer.add_scalar('Loss/Train', avg_loss, epoch)
                avg_val_loss = val_loss / len(val_loader)
                # Log validation loss
                writer.add_scalar('Loss/Validation', avg_val_loss, epoch)

            writer.close()

        elif regression_type_combo == "LightGBM Regressor":
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
        elif regression_type_combo in ["NGBoost Regressor"]:
            model.fit(X_train, y_train)
        else:
            model.fit(X_train, y_train)

        # Predict using the fitted model
        if regression_type_combo == "ResNet":
            # Make predictions
            model.eval()
            with torch.no_grad():
                y_pred = model(X_val_tensor.to(device)).cpu().numpy()  # Move predictions back to CPU for compatibility
                y_pred = np.array(y_pred).squeeze()
            torch.cuda.empty_cache()
        elif regression_type_combo == "TabNet":
            y_pred = model.predict(X_val)
            y_pred = np.array(y_pred).squeeze()
            torch.cuda.empty_cache()
        elif regression_type_combo == "TCN":
            # Make predictions
            model.eval()
            with torch.no_grad():
                y_pred = model(X_val_tensor.to(device)).cpu().numpy()  # Move predictions back to CPU for compatibility
                y_pred = np.array(y_pred).squeeze()
            torch.cuda.empty_cache()
        elif regression_type_combo in ["Hyper Parameter", "Keras Regressor"]:
            y_pred = model.predict(X_val)
            y_pred = np.array(y_pred).squeeze()
        else:
            y_pred = model.predict(X_val)

        if regression_type_combo in ["Keras Regressor", "Hyper Parameter"]:
            if hasattr(model, 'model'):
                model = model.model  # Extract the actual Keras model
            model.save(file_path)
            return y_pred.astype(dtype)

        elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype)

        return y_pred.astype(dtype), model

    def run_regression(self, parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                       regression_type_combo, selected_files, selected_target, selected_features, num_valid_points,
                       dimensionality_reduction_type_combo, n_components):

        num_lstm_layers_min = None
        epochs = None
        max_trials = None
        num_lstm_layers_max = None
        num_lstm_layers_step = None
        lstm_units_min = None
        lstm_units_max = None
        lstm_units_step = None
        l2_lstm_min = None
        l2_lstm_max = None
        l2_lstm_step = None
        min_dropout_lstm = None
        max_dropout_lstm = None
        dropout_lstm_layer_step = None
        dropout_lstm_min = None
        dropout_lstm_max = None
        dropout_lstm_step = None
        num_conv_layers_min = None
        num_conv_layers_max = None
        num_conv_layers_step = None
        conv_filters_min = None
        conv_filters_max = None
        conv_filters_step = None
        conv_kernel_size_min = None
        conv_kernel_size_max = None
        conv_kernel_size_step = None
        l2_conv_min = None
        l2_conv_max = None
        l2_conv_step = None
        min_dropout_conv = None
        max_dropout_conv = None
        dropout_conv_layer_step = None
        dropout_conv_min = None
        dropout_conv_max = None
        dropout_conv_step = None
        num_attention_layers_min = None
        num_attention_layers_max = None
        num_attention_layers_step = None
        attention_heads_min = None
        attention_heads_max = None
        attention_heads_step = None
        attention_key_dim_min = None
        attention_key_dim_max = None
        attention_key_dim_step = None
        l2_attention_min = None
        l2_attention_max = None
        l2_attention_step = None
        min_dropout_attention = None
        max_dropout_attention = None
        dropout_attention_layer_step = None
        dropout_attention_min = None
        dropout_attention_max = None
        dropout_attention_step = None
        num_dense_layers_min = None
        num_dense_layers_max = None
        num_dense_layers_step = None
        dense_units_min = None
        dense_units_max = None
        dense_units_step = None
        l2_dense_min = None
        l2_dense_max = None
        l2_dense_step = None
        min_dropout_dense = None
        max_dropout_dense = None
        dropout_dense_layer_step = None
        dropout_dense_min = None
        dropout_dense_max = None
        dropout_dense_step = None
        learning_rate_min = None
        learning_rate_max = None
        learning_rate_step = None
        Loss_type_combo = None
        batch_size = None
        file_path = None

        def add_model_to_dict(model_dict=None, model_type=None, model=None, features=None, target=None, file_path=None,
                              dimensionality_reduction_type_combo=None, n_components=None):
            """
            Adds a trained model, its metadata, and file path to the model dictionary.

            :param model_dict: The dictionary to store models and their metadata.
            :param model_type: Type of the model (e.g., "RandomForest", "LinearRegression").
            :param model: The trained model object.
            :param features: List of feature input names used for training the model.
            :param target: The target variable name the model predicts.
            :param file_path: Path to the saved model file.
            """
            model_metadata = {
                "model": model,
                "features": features,
                "target": target,
                "file_path": file_path,
                "dim_type": dimensionality_reduction_type_combo,
                "n_components": n_components

            }

            combined_key = f"{model_type}_{dimensionality_reduction_type_combo}"

            if combined_key not in model_dict:
                model_dict[combined_key] = []

            model_dict[combined_key].append(model_metadata)

        dialog = QDialog(parent_dialog)
        dialog.setWindowTitle("Train test split")
        layout = QVBoxLayout()

        label = QLabel("Choose the train test split")
        layout.addWidget(label)

        # Add a spin box for integer input
        spinBox = QSpinBox()
        spinBox.setMinimum(1)
        spinBox.setMaximum(99)
        spinBox.setValue(20)
        layout.addWidget(spinBox)

        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(225)
        # Disable the '?' help button on the dialog
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()
        if dialog.exec() == QDialog.DialogCode.Accepted:
            train_test = spinBox.value()
        else:
            return

        # Split into training and validation sets (80% train, 20% validation)
        X_train, X_val, y_train, y_val = train_test_split(X_clean, y_clean, test_size=train_test / 100,
                                                          random_state=42)

        if regression_type_combo in ["Random Forest Regressor", "Gradient Boosting Regressor", "AdaBoost Regressor",
                                     "XGBoost Regressor", "CatBoost Regressor", "Keras Regressor", "ResNet", "TabNet",
                                     "TCN", "LightGBM Regressor", "NGBoost Regressor", "Hyper Parameter"]:

            dialog = QDialog(parent_dialog)
            dialog.setWindowTitle("Epochs")
            layout = QVBoxLayout()

            label = QLabel("Choose the number of epochs")
            layout.addWidget(label)

            # Add a spin box for integer input
            spinBox = QSpinBox()
            spinBox.setMinimum(1)  # Set minimum number of epochs to 1
            spinBox.setMaximum(1000)  # Set maximum number of epochs to 1000
            spinBox.setValue(20)
            layout.addWidget(spinBox)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            # Disable the '?' help button on the dialog
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Accepted:
                epochs = spinBox.value()
            else:
                return

            if regression_type_combo == "Hyper Parameter":

                # Create a new dialog to select linear or non-linear regression
                cross_plot_dialog = QDialog(parent_dialog)
                cross_plot_dialog.setWindowTitle("Loss Options")

                layout = QVBoxLayout(cross_plot_dialog)

                Loss_type_combo = QComboBox()
                Loss_type_combo.addItems([
                    "mean_squared_error", "mean_absolute_error", "mean_absolute_percentage_error",
                    "mean_squared_logarithmic_error", "huber", "log_cosh", "cosine_similarity", "poisson"

                ])
                layout.addWidget(Loss_type_combo)

                select_button = QPushButton("OK")
                select_button.clicked.connect(cross_plot_dialog.accept)  # Close the current dialog
                layout.addWidget(select_button)

                cross_plot_dialog.setLayout(layout)
                cross_plot_dialog.adjustSize()
                cross_plot_dialog.setMinimumWidth(250)
                # Disable the '?' help button on the dialog
                cross_plot_dialog.setWindowFlags(
                    cross_plot_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                result = cross_plot_dialog.exec()  # Use exec() to block until the dialog is closed
                if result == QDialog.DialogCode.Accepted:
                    Loss_type_combo = Loss_type_combo.currentText()
                else:
                    return

                # Create the dialog
                dialog = QDialog(parent_dialog)
                dialog.setWindowTitle("Hyperparameters")
                layout = QGridLayout()  # Use a grid layout for better organization

                # Add spin boxes for different hyperparameters
                def add_spin_boxes(label_text, min_value, max_value, default_value, step=1, is_float=False):
                    label = QLabel(label_text)
                    label.setMinimumWidth(150)  # Set a minimum width for labels
                    min_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                    if is_float:
                        min_spin_box.setDecimals(6)
                    min_spin_box.setMinimum(min_value)
                    min_spin_box.setMaximum(max_value)
                    min_spin_box.setSingleStep(step)
                    min_spin_box.setValue(min_value)

                    max_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                    if is_float:
                        max_spin_box.setDecimals(6)
                    max_spin_box.setMinimum(min_value)
                    max_spin_box.setMaximum(max_value)
                    max_spin_box.setSingleStep(step)
                    max_spin_box.setValue(max_value)

                    step_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                    if is_float:
                        step_spin_box.setDecimals(6)
                    step_spin_box.setMinimum(0.000001 if is_float else 1)
                    step_spin_box.setMaximum(max_value - min_value)
                    step_spin_box.setSingleStep(step)
                    step_spin_box.setValue(step)

                    return label, min_spin_box, max_spin_box, step_spin_box

                # Layout management function
                def add_spin_boxes_to_grid(grid, start_row, column, *spinbox_data):
                    for i, (label, min_box, max_box, step_box) in enumerate(spinbox_data):
                        row = start_row + i * 4
                        grid.addWidget(label, row, column, 1, 2)
                        grid.addWidget(QLabel("Min"), row + 1, column)
                        grid.addWidget(min_box, row + 1, column + 1)
                        grid.addWidget(QLabel("Max"), row + 2, column)
                        grid.addWidget(max_box, row + 2, column + 1)
                        grid.addWidget(QLabel("Step"), row + 3, column)
                        grid.addWidget(step_box, row + 3, column + 1)

                # LSTM layers
                lstm_spin_boxes = [
                    add_spin_boxes("Number of LSTM Layers", 0, 100, 10),
                    add_spin_boxes("LSTM Units", 16, 256, 128, step=32),
                    add_spin_boxes("L2 Regularization LSTM", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                    add_spin_boxes("Number of LSTM Dropout Layers", 0, 5, 1),
                    add_spin_boxes("Dropout Rate LSTM", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                ]

                # Conv1D layers
                conv_spin_boxes = [
                    add_spin_boxes("Number of Conv Layers", 0, 100, 10),
                    add_spin_boxes("Conv Filters", 16, 256, 128, step=32),
                    add_spin_boxes("Conv Kernel Size", 1, 5, 1),
                    add_spin_boxes("L2 Regularization Conv", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                    add_spin_boxes("Number of Conv Dropout Layers", 0, 5, 1),
                    add_spin_boxes("Dropout Rate Conv", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                ]

                # MultiHeadAttention layers
                attention_spin_boxes = [
                    add_spin_boxes("Number of Attention Layers", 0, 100, 10),
                    add_spin_boxes("Attention Heads", 2, 8, 4, step=2),
                    add_spin_boxes("Attention Key Dim", 16, 256, 128, step=32),
                    add_spin_boxes("L2 Regularization Attention", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                    add_spin_boxes("Number of Attention Dropout Layers", 0, 5, 1),
                    add_spin_boxes("Dropout Rate Attention", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                ]

                # Dense layers
                dense_spin_boxes = [
                    add_spin_boxes("Number of Dense Layers", 0, 100, 10),
                    add_spin_boxes("Dense Units", 16, 256, 128, step=32),
                    add_spin_boxes("L2 Regularization Dense", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                    add_spin_boxes("Number of Dense Dropout Layers", 0, 5, 1),
                    add_spin_boxes("Dropout Rate Dense", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                ]

                # Add the spin boxes to the grid layout
                add_spin_boxes_to_grid(layout, 0, 0, *lstm_spin_boxes)
                add_spin_boxes_to_grid(layout, 0, 2, *conv_spin_boxes)
                add_spin_boxes_to_grid(layout, 0, 4, *attention_spin_boxes)
                add_spin_boxes_to_grid(layout, 0, 6, *dense_spin_boxes)

                # Learning rate
                lr_label, lr_min, lr_max, lr_step = add_spin_boxes("Learning Rate", 1e-4, 1e-2, 1e-3, step=1e-4,
                                                                   is_float=True)
                layout.addWidget(lr_label, 20, 0, 1, 2)
                layout.addWidget(QLabel("Min"), 21, 0)
                layout.addWidget(lr_min, 21, 1)
                layout.addWidget(QLabel("Max"), 22, 0)
                layout.addWidget(lr_max, 22, 1)
                layout.addWidget(QLabel("Step"), 23, 0)
                layout.addWidget(lr_step, 23, 1)

                # Max trials
                label_max_trials = QLabel("Maximum Trials")
                spinBox_max_trials = QSpinBox()
                spinBox_max_trials.setMinimum(0)
                spinBox_max_trials.setMaximum(2000)
                spinBox_max_trials.setValue(10)
                layout.addWidget(label_max_trials, 24, 0, 1, 2)
                layout.addWidget(spinBox_max_trials, 25, 0, 1, 2)

                # Batch size
                label_batch_size = QLabel("Batch Size")
                spinBox_batch_size = QSpinBox()
                spinBox_batch_size.setMinimum(32)
                spinBox_batch_size.setMaximum(2048)
                spinBox_batch_size.setValue(16)
                layout.addWidget(label_batch_size, 26, 0, 1, 2)
                layout.addWidget(spinBox_batch_size, 27, 0, 1, 2)

                # OK button
                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button, 28, 0, 1, 2)

                layout.setColumnStretch(1, 1)
                layout.setColumnStretch(3, 1)
                layout.setColumnStretch(5, 1)
                layout.setColumnStretch(7, 1)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(800)  # Increase minimum width
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()

                # Execute the dialog and get the values
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    # LSTM layer parameters
                    num_lstm_layers_min = lstm_spin_boxes[0][1].value()
                    num_lstm_layers_max = lstm_spin_boxes[0][2].value()
                    num_lstm_layers_step = lstm_spin_boxes[0][3].value()
                    lstm_units_min = lstm_spin_boxes[1][1].value()
                    lstm_units_max = lstm_spin_boxes[1][2].value()
                    lstm_units_step = lstm_spin_boxes[1][3].value()
                    l2_lstm_min = lstm_spin_boxes[2][1].value()
                    l2_lstm_max = lstm_spin_boxes[2][2].value()
                    l2_lstm_step = lstm_spin_boxes[2][3].value()
                    dropout_lstm_min = lstm_spin_boxes[4][1].value()
                    dropout_lstm_max = lstm_spin_boxes[4][2].value()
                    dropout_lstm_step = lstm_spin_boxes[4][3].value()
                    min_dropout_lstm = lstm_spin_boxes[3][1].value()
                    max_dropout_lstm = lstm_spin_boxes[3][2].value()
                    dropout_lstm_layer_step = lstm_spin_boxes[3][3].value()

                    # Conv layer parameters
                    num_conv_layers_min = conv_spin_boxes[0][1].value()
                    num_conv_layers_max = conv_spin_boxes[0][2].value()
                    num_conv_layers_step = conv_spin_boxes[0][3].value()
                    conv_filters_min = conv_spin_boxes[1][1].value()
                    conv_filters_max = conv_spin_boxes[1][2].value()
                    conv_filters_step = conv_spin_boxes[1][3].value()
                    conv_kernel_size_min = conv_spin_boxes[2][1].value()
                    conv_kernel_size_max = conv_spin_boxes[2][2].value()
                    conv_kernel_size_step = conv_spin_boxes[2][3].value()
                    l2_conv_min = conv_spin_boxes[3][1].value()
                    l2_conv_max = conv_spin_boxes[3][2].value()
                    l2_conv_step = conv_spin_boxes[3][3].value()
                    dropout_conv_min = conv_spin_boxes[5][1].value()
                    dropout_conv_max = conv_spin_boxes[5][2].value()
                    dropout_conv_step = conv_spin_boxes[5][3].value()
                    min_dropout_conv = conv_spin_boxes[4][1].value()
                    max_dropout_conv = conv_spin_boxes[4][2].value()
                    dropout_conv_layer_step = conv_spin_boxes[4][3].value()

                    # Attention layer parameters
                    num_attention_layers_min = attention_spin_boxes[0][1].value()
                    num_attention_layers_max = attention_spin_boxes[0][2].value()
                    num_attention_layers_step = attention_spin_boxes[0][3].value()
                    attention_heads_min = attention_spin_boxes[1][1].value()
                    attention_heads_max = attention_spin_boxes[1][2].value()
                    attention_heads_step = attention_spin_boxes[1][3].value()
                    attention_key_dim_min = attention_spin_boxes[2][1].value()
                    attention_key_dim_max = attention_spin_boxes[2][2].value()
                    attention_key_dim_step = attention_spin_boxes[2][3].value()
                    l2_attention_min = attention_spin_boxes[3][1].value()
                    l2_attention_max = attention_spin_boxes[3][2].value()
                    l2_attention_step = attention_spin_boxes[3][3].value()
                    dropout_attention_min = attention_spin_boxes[5][1].value()
                    dropout_attention_max = attention_spin_boxes[5][2].value()
                    dropout_attention_step = attention_spin_boxes[5][3].value()
                    min_dropout_attention = attention_spin_boxes[4][1].value()
                    max_dropout_attention = attention_spin_boxes[4][2].value()
                    dropout_attention_layer_step = attention_spin_boxes[4][3].value()

                    # Dense layer parameters
                    num_dense_layers_min = dense_spin_boxes[0][1].value()
                    num_dense_layers_max = dense_spin_boxes[0][2].value()
                    num_dense_layers_step = dense_spin_boxes[0][3].value()
                    dense_units_min = dense_spin_boxes[1][1].value()
                    dense_units_max = dense_spin_boxes[1][2].value()
                    dense_units_step = dense_spin_boxes[1][3].value()
                    l2_dense_min = dense_spin_boxes[2][1].value()
                    l2_dense_max = dense_spin_boxes[2][2].value()
                    l2_dense_step = dense_spin_boxes[2][3].value()
                    min_dropout_dense = dense_spin_boxes[3][1].value()
                    max_dropout_dense = dense_spin_boxes[3][2].value()
                    dropout_dense_layer_step = dense_spin_boxes[3][3].value()
                    dropout_dense_min = dense_spin_boxes[4][1].value()
                    dropout_dense_max = dense_spin_boxes[4][2].value()
                    dropout_dense_step = dense_spin_boxes[4][3].value()

                    # Other parameters
                    learning_rate_min = lr_min.value()
                    learning_rate_max = lr_max.value()
                    learning_rate_step = lr_step.value()
                    max_trials = spinBox_max_trials.value()
                    batch_size = spinBox_batch_size.value()
                else:
                    return

        # Fit the model
        if regression_type_combo == "Keras Regressor":
            # Set up TensorBoard callback
            log_dir = os.path.join("logs", "Tensorflow", "keras_well_regressor",
                                   datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

        elif regression_type_combo == "Hyper Parameter":
            # Set up TensorBoard callback
            log_dir = os.path.join("logs", "Tensorflow", "hyper_parameter_well_regressor",
                                   datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

        elif regression_type_combo == "ResNet":
            # TensorBoard setup
            log_dir = f"logs/Pytorch/{regression_type_combo}_well_regressor/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"

        elif regression_type_combo == "TCN":
            # TensorBoard setup
            log_dir = f"logs/Pytorch/{regression_type_combo}_well_regressor/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
        else:
            log_dir = None

        if regression_type_combo in ["Keras Regressor", "Hyper Parameter"]:
            # Check if it's a KerasRegressor and extract the underlying model
            # Add models to the dictionary
            file_dialog = QFileDialog()
            file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
            file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.h5"

        elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
            # Add models to the dictionary
            file_dialog = QFileDialog()
            file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
            file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.pth"

        result = self.task_runner.run_task(TensorVisualizer.run_well_regression_task, X_clean, y_clean,
                                                   train_test,
                                                   num_lstm_layers_min, regression_type_combo, epochs,
                                                   max_trials, num_lstm_layers_max, num_lstm_layers_step,
                                                   lstm_units_min,
                                                   lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                                   l2_lstm_step, min_dropout_lstm, max_dropout_lstm,
                                                   dropout_lstm_layer_step,
                                                   dropout_lstm_min, dropout_lstm_max, dropout_lstm_step,
                                                   num_conv_layers_min,
                                                   num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                                   conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                                   conv_kernel_size_max, conv_kernel_size_step, l2_conv_min,
                                                   l2_conv_max,
                                                   l2_conv_step, min_dropout_conv, max_dropout_conv,
                                                   dropout_conv_layer_step,
                                                   dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                                   num_attention_layers_min, num_attention_layers_max,
                                                   num_attention_layers_step,
                                                   attention_heads_min, attention_heads_max, attention_heads_step,
                                                   attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                                                   l2_attention_min, l2_attention_max, l2_attention_step,
                                                   min_dropout_attention, max_dropout_attention,
                                                   dropout_attention_layer_step,
                                                   dropout_attention_min, dropout_attention_max, dropout_attention_step,
                                                   num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                                                   dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                                   l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                                                   dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                                   dropout_dense_step, learning_rate_min, learning_rate_max,
                                                   learning_rate_step, Loss_type_combo, log_dir, batch_size, file_path)

        if result is not None:

            if regression_type_combo in ["Keras Regressor", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                y_pred = result
            else:
                y_pred, model = result

            if regression_type_combo in ["Keras Regressor", "Hyper Parameter", "ResNet", "TCN"]:
                self.show_tensorboard(log_dir=log_dir)

            if regression_type_combo in ["Keras Regressor", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                # Adding model to dictionary with the file path
                add_model_to_dict(model_dict=self.model_dict, model_type=regression_type_combo, model=None,
                                  features=selected_features, target=selected_target, file_path=file_path,
                                  dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                  n_components=n_components)

            else:  # Assume Sklearn or similar

                # Ask the user if they want to save the model
                reply = QMessageBox.question(self, 'Save Model', 'Do you want to save the model?',
                                             QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                                             QMessageBox.StandardButton.No)

                if reply == QMessageBox.StandardButton.Yes:
                    file_dialog = QFileDialog()
                    file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                    file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.pkl"
                    joblib.dump(model, file_path)
                else:
                    file_path = None
                # Adding model to dictionary with the file path
                add_model_to_dict(model_dict=self.model_dict, model_type=regression_type_combo, model=model,
                                  features=selected_features, target=selected_target, file_path=file_path,
                                  dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                  n_components=n_components)

            # Pass data to the plotting function
            self.plot_results(y_val, y_pred, regression_type_combo, result_layout, selected_files, selected_target,
                              selected_features, parent_dialog, X_clean_copy, train_test, num_valid_points)

    def plot_results(self, y_actual, y_pred, regression_type, result_layout, selected_files, selected_target,
                     selected_features, parent_dialog, X_clean_copy, train_test, num_valid_points):
        """
        Plot the predicted values against the actual values for regression analysis.
        """

        # Create a dialog to select the target variable
        target_variable_dialog = QDialog(parent_dialog)
        target_variable_dialog.setWindowTitle("Select Color-coded Feature")

        layout = QVBoxLayout(target_variable_dialog)

        target_variable_list = QListWidget()
        target_variable_list.setSelectionMode(QAbstractItemView.SelectionMode.SingleSelection)
        for col in selected_features:
            target_variable_list.addItem(col)
        layout.addWidget(target_variable_list)

        select_button = QPushButton("OK")
        select_button.clicked.connect(target_variable_dialog.accept)  # Close the current dialog
        layout.addWidget(select_button)

        target_variable_dialog.setLayout(layout)
        if target_variable_dialog.exec() == QDialog.DialogCode.Rejected:
            QApplication.restoreOverrideCursor()
            return

        # Get the selected target variable
        selected_target_items = target_variable_list.selectedItems()
        if len(selected_target_items) == 0:
            QMessageBox.warning(self, "Warning", "Please select a target variable.")
            return

        selected_item = selected_target_items[0].text()

        # Extract the cleaned feature inputs and target variable
        X_clean = X_clean_copy[selected_item]

        # Convert to numpy arrays
        X_clean = np.asarray(X_clean, dtype=np.float32)

        # Split into training and validation sets (80% train, 20% validation)
        X_train, c_data = train_test_split(X_clean, test_size=train_test / 100, random_state=42)

        # Normalize c_data for color mapping
        norm_c_data = (c_data - c_data.min()) / (c_data.max() - c_data.min())

        self.well_color_data = norm_c_data

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')
        colors = cmap.map(self.well_color_data, mode='qcolor')

        # Clear the previous plot and reset the view
        self.plot_widget.clear()
        self.plot_widget.autoRange()

        # Create scatter plot item for actual vs. predicted values
        scatter = pg.ScatterPlotItem(x=y_actual, y=y_pred, pen=None, brush=colors)
        self.plot_widget.addItem(scatter)

        # Retrieve the units for the target variable from the selected file
        selected_file_key = selected_files[0] if selected_files else None
        units = self.units_dict.get(selected_file_key, {})  # Dictionary of units for the selected file
        target_unit = units.get(selected_target, '')  # Get the unit for the target column

        # Set axis labels with units
        x_unit_label = f"Actual {selected_target} ({target_unit})"
        y_unit_label = f"Predicted {selected_target} ({target_unit})"

        self.plot_widget.setLabel('bottom', x_unit_label)  # x-axis label with unit
        self.plot_widget.setLabel('left', y_unit_label)  # y-axis label with unit

        # Optionally, you can add a line y=x to indicate perfect predictions
        diag_line = pg.PlotCurveItem(x=[min(y_actual), max(y_actual)], y=[min(y_actual), max(y_actual)],
                                     pen=pg.mkPen('r', style=Qt.PenStyle.DashLine))
        self.plot_widget.addItem(diag_line)

        # Create or update the ColorBarItem for the color bar
        if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
            self.well_color_bar.setLevels((c_data.min(), c_data.max()))
            self.plot_widget.getPlotItem().layout.removeItem(self.color_bar_label_item)
            self.well_color_bar.label = selected_item  # Update the color bar title with the third column name
            self.well_color_bar.setColorMap(cmap)
        else:
            color_bar = pg.ColorBarItem(values=(c_data.min(), c_data.max()))
            color_bar.setColorMap(cmap)
            color_bar.label = selected_item  # Set the color bar title to the third column name
            self.well_color_bar = color_bar
            self.plot_widget.getPlotItem().layout.addItem(self.well_color_bar, 2,
                                                          2)  # Add to the layout in the next column

        # Add vertical label for the color bar with units
        color_bar_unit_label = f"({units.get(selected_item, '')})"
        self.color_bar_label_item = pg.LabelItem(text=f"{selected_item} {color_bar_unit_label}", angle=90,
                                                 color='k', size='10pt')
        self.plot_widget.getPlotItem().layout.addItem(self.color_bar_label_item, 2, 2, 1, 1)

        self.plot_widget.getPlotItem().layout.setContentsMargins(0, 0, 30,
                                                                 0)  # Adjust the values as needed

        self.plot_widget.autoRange()

        def robust_mape(y_true, y_pred, epsilon=1e-10, max_error=None):
            """
            Calculate the Mean Absolute Percentage Error (MAPE) with robust handling of edge cases.

            Parameters:
            - y_true: array-like, true values.
            - y_pred: array-like, predicted values.
            - epsilon: small value to replace zeros or very small values in y_true to prevent division by zero (default: 1e-10).
            - max_error: optional, maximum percentage error to clip extreme values (e.g., 1000 for capping at 1000%).

            Returns:
            - A string with the computed MAPE as a percentage or an error message if calculation is not possible.
            """
            try:
                # Convert inputs to numpy arrays for element-wise operations
                y_true = np.array(y_true)
                y_pred = np.array(y_pred)

                # Check for NaNs or Infs in the inputs
                if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)):
                    return "Error: Input arrays contain NaNs."
                if np.any(np.isinf(y_true)) or np.any(np.isinf(y_pred)):
                    return "Error: Input arrays contain Infs."

                # Replace zeros or very small values in y_true with epsilon to prevent division by zero
                y_true_safe = np.where(np.abs(y_true) < epsilon, epsilon, y_true)

                # Compute the absolute percentage errors
                percentage_errors = np.abs((y_true - y_pred) / y_true_safe) * 100

                # Optionally clip extreme percentage errors if max_error is provided
                if max_error is not None:
                    percentage_errors = np.clip(percentage_errors, 0, max_error)

                # Calculate the mean of the absolute percentage errors
                mape = np.mean(percentage_errors)

                return f"MAPE: {mape:.2f}%"

            except Exception as e:
                QMessageBox.critical(self, "Error", f"Error: {str(e)}")

        # Optionally, display statistics in the result layout
        result_text = f"{regression_type} Results:\n"
        result_text += f"Number of Samples: {num_valid_points}\n"
        result_text += f"Features: {' '.join(selected_features)}\n"
        result_text += f"Correlation Coefficient: {pearsonr(y_actual, y_pred)[0]:.2f}\n"
        result_text += f"R-squared Score: {r2_score(y_actual, y_pred):.2f}\n"
        result_text += f"MSE: {mean_squared_error(y_actual, y_pred):.6f}\n"
        result_text += f"RMSE: {root_mean_squared_error(y_actual, y_pred):.6f}\n"
        result_text += f"MAE: {mean_absolute_error(y_actual, y_pred):.6f}\n"
        result_text += robust_mape(y_actual, y_pred, epsilon=1e-10, max_error=1000)

        # Update the result area
        self.update_result_area(result_text, result_layout)

    @staticmethod
    def run_well_categorization_task(X_clean, y_clean, train_test, num_lstm_layers_min, regression_type_combo, epochs,
                                     max_trials, num_lstm_layers_max, num_lstm_layers_step, lstm_units_min,
                                     lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                                     l2_lstm_step, min_dropout_lstm, max_dropout_lstm, dropout_lstm_layer_step,
                                     dropout_lstm_min, dropout_lstm_max, dropout_lstm_step, num_conv_layers_min,
                                     num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                                     conv_filters_max, conv_filters_step, conv_kernel_size_min,
                                     conv_kernel_size_max, conv_kernel_size_step, l2_conv_min, l2_conv_max,
                                     l2_conv_step, min_dropout_conv, max_dropout_conv, dropout_conv_layer_step,
                                     dropout_conv_min, dropout_conv_max, dropout_conv_step,
                                     num_attention_layers_min, num_attention_layers_max, num_attention_layers_step,
                                     attention_heads_min, attention_heads_max, attention_heads_step,
                                     attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                                     l2_attention_min, l2_attention_max, l2_attention_step,
                                     min_dropout_attention, max_dropout_attention, dropout_attention_layer_step,
                                     dropout_attention_min, dropout_attention_max, dropout_attention_step,
                                     num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                                     dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                                     l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                                     dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                                     dropout_dense_step, learning_rate_min, learning_rate_max,
                                     learning_rate_step, log_dir, batch_size, file_path, output_dim):

        # Set up logging
        TensorVisualizer.setup_logging('Well_Classification')

        dtype = y_clean.dtype

        # Split into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(X_clean, y_clean, test_size=train_test / 100, random_state=42)

        class MetricsCallback(Callback):
            def __init__(self):
                super(MetricsCallback, self).__init__()
                self.best_accuracy = 0
                self.best_top_k = 0
                self.best_crossentropy = float('inf')  # Initialize as infinity because lower is better

            def on_epoch_end(self, epoch, logs=None):
                if logs is not None:
                    # Update best accuracy
                    current_accuracy = logs.get('sparse_categorical_accuracy')
                    if current_accuracy is not None and current_accuracy > self.best_accuracy:
                        self.best_accuracy = current_accuracy

                    # Update best top-k accuracy
                    current_top_k_accuracy = logs.get('sparse_top_k_categorical_accuracy')
                    if current_top_k_accuracy is not None and current_top_k_accuracy > self.best_top_k:
                        self.best_top_k = current_top_k_accuracy

                    # Update best cross-entropy loss (lower is better)
                    current_crossentropy = logs.get('sparse_categorical_crossentropy')
                    if current_crossentropy is not None and current_crossentropy < self.best_crossentropy:
                        self.best_crossentropy = current_crossentropy

                    # Store metrics in model
                    self.model.best_accuracy = self.best_accuracy
                    self.model.best_top_k = self.best_top_k
                    self.model.best_crossentropy = self.best_crossentropy

        metrics_callback = MetricsCallback()

        # Function to create the Keras model for multi-class classification
        def create_keras_model():
            inputs = Input(shape=(X_train.shape[1],))
            x = Dense(128, activation='relu')(inputs)
            x = Dense(64, activation='relu')(x)
            x = Dense(32, activation='relu')(x)

            # Output layer
            num_classes = len(np.unique(y_train))  # Assuming y_train is integer encoded
            # Modify your output layer like this:
            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

            # Define a custom clipping function
            def clip_output(x):
                epsilon = 1e-7
                return tf.clip_by_value(x, epsilon, 1 - epsilon)

            # Apply a custom Lambda layer using the function instead of a lambda
            outputs = tf.keras.layers.Lambda(clip_output)(outputs)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=0.001, clipnorm=1.0
                ),
                loss='sparse_categorical_crossentropy',
                metrics=[
                    'sparse_categorical_accuracy',
                    'sparse_top_k_categorical_accuracy',
                    'sparse_categorical_crossentropy'
                ]
            )
            return model

        # Define the hypermodel function using the Functional API
        def build_model(hp):
            input_shape = (X_train.shape[1], X_train.shape[2])
            inputs = tf.keras.Input(shape=input_shape)
            x = inputs

            # Add LSTM layers with Dropout and L2 regularization
            num_lstm_layers = hp.Int('num_lstm_layers', min_value=num_lstm_layers_min,
                                     max_value=num_lstm_layers_max,
                                     step=num_lstm_layers_step)

            for i in range(num_lstm_layers):
                x = tf.keras.layers.LSTM(
                    units=hp.Int(f'lstm_units_{i}', min_value=lstm_units_min, max_value=lstm_units_max,
                                 step=lstm_units_step),
                    return_sequences=True,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_lstm_{i}', min_value=l2_lstm_min, max_value=l2_lstm_max, step=l2_lstm_step)
                    )
                )(x)

                num_dropout_lstm = hp.Int(f'num_dropout_lstm_{i}', min_value=min_dropout_lstm,
                                          max_value=max_dropout_lstm, step=dropout_lstm_layer_step)
                for _ in range(num_dropout_lstm):
                    dropout_rate = hp.Float(f'dropout_rate_lstm_{i}', min_value=dropout_lstm_min,
                                            max_value=dropout_lstm_max, step=dropout_lstm_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add Conv1D layers with Dropout and L2 regularization
            num_conv_layers = hp.Int('num_conv_layers', min_value=num_conv_layers_min,
                                     max_value=num_conv_layers_max,
                                     step=num_conv_layers_step)

            for i in range(num_conv_layers):
                x = tf.keras.layers.Conv1D(
                    filters=hp.Int(f'conv_filters_{i}', min_value=conv_filters_min, max_value=conv_filters_max,
                                   step=conv_filters_step),
                    kernel_size=hp.Int(f'conv_kernel_size_{i}', min_value=conv_kernel_size_min,
                                       max_value=conv_kernel_size_max, step=conv_kernel_size_step),
                    activation='relu',
                    padding='same',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_conv_{i}', min_value=l2_conv_min, max_value=l2_conv_max, step=l2_conv_step)
                    )
                )(x)

                num_dropout_conv = hp.Int(f'num_dropout_conv_{i}', min_value=min_dropout_conv,
                                          max_value=max_dropout_conv, step=dropout_conv_layer_step)
                for _ in range(num_dropout_conv):
                    dropout_rate = hp.Float(f'dropout_rate_conv_{i}', min_value=dropout_conv_min,
                                            max_value=dropout_conv_max, step=dropout_conv_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Add MultiHeadAttention layers with Dropout and L2 regularization
            num_attention_layers = hp.Int('num_attention_layers', min_value=num_attention_layers_min,
                                          max_value=num_attention_layers_max, step=num_attention_layers_step)

            for i in range(num_attention_layers):
                num_heads = hp.Int(f'num_attention_heads_{i}', min_value=attention_heads_min,
                                   max_value=attention_heads_max, step=attention_heads_step)
                key_dim = hp.Int(f'attention_key_dim_{i}', min_value=attention_key_dim_min,
                                 max_value=attention_key_dim_max, step=attention_key_dim_step)

                attention_layer = tf.keras.layers.MultiHeadAttention(
                    num_heads=num_heads,
                    key_dim=key_dim,
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_attention_{i}', min_value=l2_attention_min, max_value=l2_attention_max,
                                 step=l2_attention_step)
                    )
                )
                attention_output = attention_layer(query=x, value=x)
                x = tf.keras.layers.Add()([x, attention_output])  # Residual connection

                num_dropout_attention = hp.Int(f'num_dropout_attention_{i}', min_value=min_dropout_attention,
                                               max_value=max_dropout_attention, step=dropout_attention_layer_step)
                for _ in range(num_dropout_attention):
                    dropout_rate = hp.Float(f'dropout_rate_attention_{i}', min_value=dropout_attention_min,
                                            max_value=dropout_attention_max, step=dropout_attention_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Flatten the output for Dense layers
            x = tf.keras.layers.Flatten()(x)

            # Add Dense layers with Dropout and L2 regularization
            num_dense_layers = hp.Int('num_dense_layers', min_value=num_dense_layers_min,
                                      max_value=num_dense_layers_max, step=num_dense_layers_step)

            for i in range(num_dense_layers):
                x = tf.keras.layers.Dense(
                    units=hp.Int(f'dense_units_{i}', min_value=dense_units_min, max_value=dense_units_max,
                                 step=dense_units_step),
                    activation='relu',
                    kernel_regularizer=regularizers.l2(
                        hp.Float(f'l2_dense_{i}', min_value=l2_dense_min, max_value=l2_dense_max,
                                 step=l2_dense_step)
                    )
                )(x)

                num_dropout_dense = hp.Int(f'num_dropout_dense_{i}', min_value=min_dropout_dense,
                                           max_value=max_dropout_dense, step=dropout_dense_layer_step)
                for _ in range(num_dropout_dense):
                    dropout_rate = hp.Float(f'dropout_rate_dense_{i}', min_value=dropout_dense_min,
                                            max_value=dropout_dense_max, step=dropout_dense_step)
                    if dropout_rate > 0:
                        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)

            # Output layer
            num_classes = len(np.unique(y_train))  # Assuming y_train is integer encoded
            # Modify your output layer like this:
            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

            # Define a custom clipping function
            def clip_output(x):
                epsilon = 1e-7
                return tf.clip_by_value(x, epsilon, 1 - epsilon)

            # Apply a custom Lambda layer using the function instead of a lambda
            outputs = tf.keras.layers.Lambda(clip_output)(outputs)

            # Create the model
            model = tf.keras.Model(inputs=inputs, outputs=outputs)

            # Compile the model
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=hp.Float('learning_rate', min_value=learning_rate_min,
                                           max_value=learning_rate_max,
                                           step=learning_rate_step), clipnorm=1.0
                ),
                loss='sparse_categorical_crossentropy',
                metrics=[
                    'sparse_categorical_accuracy',
                    'sparse_top_k_categorical_accuracy',
                    'sparse_categorical_crossentropy'
                ]
            )

            return model

        # Initialize the model based on the classification type
        if regression_type_combo == "Random Forest Classifier":
            model = RandomForestClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Gradient Boosting Classifier":
            model = GradientBoostingClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Support Vector Classifier":
            model = SVC()

        elif regression_type_combo == "K-Nearest Neighbors Classifier":
            model = KNeighborsClassifier()

        elif regression_type_combo == "Decision Tree Classifier":
            model = DecisionTreeClassifier()

        elif regression_type_combo == "Gaussian Process Classifier":
            model = GaussianProcessClassifier()

        elif regression_type_combo == "CatBoost Classifier":
            model = CatBoostClassifier(iterations=epochs)

        elif regression_type_combo == "LightGBM Classifier":
            model = LGBMClassifier(n_estimators=100 * epochs)

        elif regression_type_combo == "Naive Bayes Classifier":
            model = GaussianNB()

        elif regression_type_combo == "Extra Trees Classifier":
            model = ExtraTreesClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Bagging Classifier":
            model = BaggingClassifier(n_estimators=epochs)  # Using epochs as number of estimators

        elif regression_type_combo == "Voting Classifier":
            model = VotingClassifier(estimators=[
                ('rf', RandomForestClassifier(n_estimators=epochs)),
                ('gb', GradientBoostingClassifier(n_estimators=epochs)),
                ('svc', SVC(probability=True))
            ], voting='soft')

        elif regression_type_combo == "Keras Classifier":
            # Determine output_dim based on the shape of y_train
            output_dim = len(np.unique(y_train))

            # Check for NaN values
            assert not np.isnan(X_train).any(), 'X_train contains NaN values'
            assert not np.isnan(y_train).any(), 'Y_train contains NaN values'

            # Check for infinite values
            assert not np.isinf(X_train).any(), 'X_train contains infinite values'
            assert not np.isinf(y_train).any(), 'Y_train contains infinite values'

            # Create Keras Regressor model
            model = create_keras_model()

        elif regression_type_combo == "ResNet":
            model = ResNet1D_Model(input_dim=X_train.shape[1], output_dim=output_dim)

        elif regression_type_combo == "TabNet":
            # Create TabNet Regressor model
            model = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.001))

        elif regression_type_combo == "TCN":
            model = TCN_Model(num_inputs=1, num_channels=[25, 50, 100], num_outputs=output_dim)

        elif regression_type_combo == "Hyper Parameter":

            X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
            X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

            # Create a temporary directory for the tuner
            with tempfile.TemporaryDirectory() as temp_dir:
                tuner = BayesianOptimization(
                    hypermodel=build_model,
                    objective='val_sparse_categorical_accuracy',
                    max_trials=max_trials,
                    directory=temp_dir,  # Use the temporary directory
                    project_name='Hyper_Parameter_Tuning'
                )

            # Perform the search
            tuner.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=batch_size)

            # Get the best model
            model = tuner.get_best_models(num_models=1)[0]

        # Assuming y_train and y_val are numpy arrays with labels starting from 1 to 10
        # Convert to tensors with appropriate shapes and types
        X_train_tensor = torch.tensor(X_train.reshape(X_train.shape[0], 1, -1), dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train,
                                      dtype=torch.long).squeeze()  # Ensure zero-based indexing and 1D shape

        X_val_tensor = torch.tensor(X_val.reshape(X_val.shape[0], 1, -1), dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val,
                                    dtype=torch.long).squeeze()  # Ensure zero-based indexing and 1D shape

        # Create datasets
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

        # DataLoader and model training setup
        train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)

        if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
            if regression_type_combo == "Keras Classifier":

                # Early stopping to prevent overfitting
                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

                # Set up TensorBoard callback
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                # Fit the model
                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32,
                          callbacks=[early_stopping, metrics_callback, tensorboard_callback])

                # Make predictions
                y_pred = model.predict(X_val)

                # Reshape y_pred if output_dim is 1
                if output_dim == 1:
                    y_pred = y_pred.reshape(-1, 1)

                # Access the best accuracy recorded during training
                best_accuracy = metrics_callback.best_accuracy
                best_top_k = metrics_callback.best_top_k
                best_crossentropy = metrics_callback.best_crossentropy

            elif regression_type_combo == "Hyper Parameter":
                # Early stopping to prevent overfitting
                early_stopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=10,
                                               restore_best_weights=True)

                # Set up TensorBoard callback
                tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

                # Fit the model
                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs,
                          callbacks=[early_stopping, metrics_callback, tensorboard_callback])
                y_pred = model.predict(X_val)

                # Access the best accuracy recorded during training
                best_accuracy = metrics_callback.best_accuracy
                best_top_k = metrics_callback.best_top_k
                best_crossentropy = metrics_callback.best_crossentropy

            elif regression_type_combo == "TabNet":
                y_train = y_train.ravel()  # or y_train.reshape(-1)
                y_val = y_val.ravel()  # or y_val.reshape(-1)

                # Train the model
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric=['accuracy'],  # Use accuracy or other classification metrics
                    max_epochs=epochs,
                    patience=10,
                    batch_size=32,
                    virtual_batch_size=16
                )

                # Make predictions
                y_pred = model.predict(X_val)

            elif regression_type_combo in ["TCN", "ResNet"]:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                model.to(device)

                criterion = nn.CrossEntropyLoss()
                optimizer = optim.Adam(model.parameters(), lr=0.001)
                scaler = GradScaler()

                # TensorBoard setup
                writer = SummaryWriter(log_dir)

                # Log the model graph using a sample batch
                example_input_batch = next(iter(train_loader))[0].to(device)
                writer.add_graph(model, example_input_batch)

                best_val_accuracy = 0
                best_val_loss = float('inf')  # Lower is better for loss

                for epoch in range(epochs):
                    model.train()
                    epoch_loss = 0

                    for X_batch, y_batch in train_loader:
                        X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device,
                                                                                             non_blocking=True)
                        optimizer.zero_grad()

                        with autocast():
                            outputs = model(X_batch)
                            loss = criterion(outputs, y_batch)

                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                        epoch_loss += loss.item()

                    model.eval()
                    val_loss = 0
                    correct_predictions = 0
                    total_predictions = 0

                    with torch.no_grad():
                        for X_val_batch, y_val_batch in val_loader:
                            X_val_batch, y_val_batch = X_val_batch.to(device,
                                                                      non_blocking=True), y_val_batch.to(device,
                                                                                                         non_blocking=True)
                            with autocast():
                                val_outputs = model(X_val_batch)
                                val_loss += criterion(val_outputs, y_val_batch).item()

                                # Calculate validation accuracy
                                _, predicted_labels = torch.max(val_outputs, 1)
                                correct_predictions += (predicted_labels == y_val_batch).sum().item()
                                total_predictions += y_val_batch.size(0)

                    avg_loss = epoch_loss / len(train_loader)
                    # Log the average training loss for this epoch
                    writer.add_scalar('Loss/Train', avg_loss, epoch)

                    avg_val_loss = val_loss / len(val_loader)
                    val_accuracy = correct_predictions / total_predictions

                    # Log validation loss and accuracy
                    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)
                    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

                    # Check if we have the best validation metrics so far
                    if val_accuracy > best_val_accuracy:
                        best_val_accuracy = val_accuracy
                        best_val_loss = avg_val_loss  # Optional, if you want to track corresponding loss

                writer.close()

                # Final evaluation
                model.eval()
                with torch.no_grad():
                    y_pred = model(X_val_tensor.to(device)).cpu().numpy()
                    if output_dim == 1:
                        y_pred = np.array(y_pred).squeeze()
        else:
            y_train = y_train.ravel()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            y_pred = np.array(y_pred).squeeze()

        if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
            if hasattr(model, 'model'):
                model = model.model  # Extract the actual Keras model
            model.save(file_path)
            return y_pred.astype(dtype), best_accuracy, best_top_k, best_crossentropy

        elif regression_type_combo == "TabNet":
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype)

        elif regression_type_combo in ["TCN", "ResNet"]:
            torch.save(model, file_path)  # Save the whole PyTorch model
            return y_pred.astype(dtype), best_val_accuracy, best_val_loss

        return y_pred.astype(dtype), model

    def run_well_categorization(self, parent_dialog, result_layout, X_clean, y_clean, X_clean_copy,
                                regression_type_combo, selected_files, selected_target, selected_features,
                                num_valid_points, dimensionality_reduction_type_combo, n_components):
        try:

            num_lstm_layers_min = None
            epochs = None
            max_trials = None
            num_lstm_layers_max = None
            num_lstm_layers_step = None
            lstm_units_min = None
            lstm_units_max = None
            lstm_units_step = None
            l2_lstm_min = None
            l2_lstm_max = None
            l2_lstm_step = None
            min_dropout_lstm = None
            max_dropout_lstm = None
            dropout_lstm_layer_step = None
            dropout_lstm_min = None
            dropout_lstm_max = None
            dropout_lstm_step = None
            num_conv_layers_min = None
            num_conv_layers_max = None
            num_conv_layers_step = None
            conv_filters_min = None
            conv_filters_max = None
            conv_filters_step = None
            conv_kernel_size_min = None
            conv_kernel_size_max = None
            conv_kernel_size_step = None
            l2_conv_min = None
            l2_conv_max = None
            l2_conv_step = None
            min_dropout_conv = None
            max_dropout_conv = None
            dropout_conv_layer_step = None
            dropout_conv_min = None
            dropout_conv_max = None
            dropout_conv_step = None
            num_attention_layers_min = None
            num_attention_layers_max = None
            num_attention_layers_step = None
            attention_heads_min = None
            attention_heads_max = None
            attention_heads_step = None
            attention_key_dim_min = None
            attention_key_dim_max = None
            attention_key_dim_step = None
            l2_attention_min = None
            l2_attention_max = None
            l2_attention_step = None
            min_dropout_attention = None
            max_dropout_attention = None
            dropout_attention_layer_step = None
            dropout_attention_min = None
            dropout_attention_max = None
            dropout_attention_step = None
            num_dense_layers_min = None
            num_dense_layers_max = None
            num_dense_layers_step = None
            dense_units_min = None
            dense_units_max = None
            dense_units_step = None
            l2_dense_min = None
            l2_dense_max = None
            l2_dense_step = None
            min_dropout_dense = None
            max_dropout_dense = None
            dropout_dense_layer_step = None
            dropout_dense_min = None
            dropout_dense_max = None
            dropout_dense_step = None
            learning_rate_min = None
            learning_rate_max = None
            learning_rate_step = None
            batch_size = None
            file_path = None
            log_dir = None
            output_dim = None

            def add_model_to_dict(model_dict=None, model_type=None, model=None, features=None, target=None,
                                  file_path=None,
                                  dimensionality_reduction_type_combo=None, n_components=None):
                """
                Adds a trained model, its metadata, and file path to the model dictionary.

                :param model_dict: The dictionary to store models and their metadata.
                :param model_type: Type of the model (e.g., "RandomForest", "LinearRegression").
                :param model: The trained model object.
                :param features: List of feature input names used for training the model.
                :param target: The target variable name the model predicts.
                :param file_path: Path to the saved model file.
                """
                model_metadata = {
                    "model": model,
                    "features": features,
                    "target": target,
                    "file_path": file_path,
                    "dim_type": dimensionality_reduction_type_combo,
                    "n_components": n_components

                }

                combined_key = f"{model_type}_{dimensionality_reduction_type_combo}"

                if combined_key not in model_dict:
                    model_dict[combined_key] = []

                model_dict[combined_key].append(model_metadata)

            dialog = QDialog(parent_dialog)
            dialog.setWindowTitle("Train test split")
            layout = QVBoxLayout()

            label = QLabel("Choose the train test split")
            layout.addWidget(label)

            # Add a spin box for integer input
            spinBox = QSpinBox()
            spinBox.setMinimum(1)
            spinBox.setMaximum(99)
            spinBox.setValue(5)
            layout.addWidget(spinBox)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            # Disable the '?' help button on the dialog
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()
            if dialog.exec() == QDialog.DialogCode.Accepted:
                train_test = spinBox.value()
            else:
                return

            min_label = np.min(y_clean)
            if min_label != 0:
                y_clean = y_clean - min_label

            # Handle single target variable correctly
            if len(y_clean.shape) == 1:
                y_clean = y_clean.reshape(-1, 1)

            if regression_type_combo in ["TCN", "ResNet", "Hyper Parameter"]:
                # Determine output_dim based on the shape of y_train
                output_dim = len(np.unique(y_clean))

            # Split into training and validation sets
            X_train, X_val, y_train, y_val = train_test_split(X_clean, y_clean, test_size=train_test / 100,
                                                              random_state=42)

            # Get the number of epochs for certain regressors
            if regression_type_combo in [
                "Random Forest Classifier",
                "Gradient Boosting Classifier",
                "Support Vector Classifier",
                "K-Nearest Neighbors Classifier",
                "Decision Tree Classifier",
                "Gaussian Process Classifier",
                "CatBoost Classifier",
                "LightGBM Classifier",
                "Naive Bayes Classifier",
                "Extra Trees Classifier",
                "Bagging Classifier",
                "Voting Classifier",
                "Keras Classifier",
                "ResNet",
                "TabNet",
                "TCN",
                "Hyper Parameter"
            ]:

                dialog = QDialog(self)
                dialog.setWindowTitle("Epochs")
                layout = QVBoxLayout()

                label = QLabel("Choose the number of epochs")
                layout.addWidget(label)

                spinBox = QSpinBox()
                spinBox.setMinimum(1)
                spinBox.setMaximum(1000)
                spinBox.setValue(20)
                layout.addWidget(spinBox)

                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                layout.addWidget(button)

                dialog.setLayout(layout)
                dialog.setMinimumWidth(225)
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                dialog.adjustSize()
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    epochs = spinBox.value()
                else:
                    return

                if regression_type_combo == "Hyper Parameter":

                    # Create the dialog
                    dialog = QDialog(self)
                    dialog.setWindowTitle("Hyperparameters")
                    layout = QGridLayout()  # Use a grid layout for better organization

                    # Add spin boxes for different hyperparameters
                    def add_spin_boxes(label_text, min_value, max_value, default_value, step=1, is_float=False):
                        label = QLabel(label_text)
                        label.setMinimumWidth(150)  # Set a minimum width for labels
                        min_spin_box = QDoubleSpinBox() if is_float else QSpinBox()
                        if is_float:
                            min_spin_box.setDecimals(6)
                        min_spin_box.setMinimum(min_value)
                        min_spin_box.setMaximum(max_value)
                        min_spin_box.setSingleStep(step)
                        min_spin_box.setValue(min_value)

                        max_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            max_spin_box.setDecimals(6)
                        max_spin_box.setMinimum(min_value)
                        max_spin_box.setMaximum(max_value)
                        max_spin_box.setSingleStep(step)
                        max_spin_box.setValue(max_value)

                        step_spin_box = QDoubleSpinBox(self) if is_float else QSpinBox()
                        if is_float:
                            step_spin_box.setDecimals(6)
                        step_spin_box.setMinimum(0.000001 if is_float else 1)
                        step_spin_box.setMaximum(max_value - min_value)
                        step_spin_box.setSingleStep(step)
                        step_spin_box.setValue(step)

                        return label, min_spin_box, max_spin_box, step_spin_box

                    # Layout management function
                    def add_spin_boxes_to_grid(grid, start_row, column, *spinbox_data):
                        for i, (label, min_box, max_box, step_box) in enumerate(spinbox_data):
                            row = start_row + i * 4
                            grid.addWidget(label, row, column, 1, 2)
                            grid.addWidget(QLabel("Min"), row + 1, column)
                            grid.addWidget(min_box, row + 1, column + 1)
                            grid.addWidget(QLabel("Max"), row + 2, column)
                            grid.addWidget(max_box, row + 2, column + 1)
                            grid.addWidget(QLabel("Step"), row + 3, column)
                            grid.addWidget(step_box, row + 3, column + 1)

                    # LSTM layers
                    lstm_spin_boxes = [
                        add_spin_boxes("Number of LSTM Layers", 0, 100, 10),
                        add_spin_boxes("LSTM Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization LSTM", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of LSTM Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate LSTM", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Conv1D layers
                    conv_spin_boxes = [
                        add_spin_boxes("Number of Conv Layers", 0, 100, 10),
                        add_spin_boxes("Conv Filters", 16, 256, 128, step=32),
                        add_spin_boxes("Conv Kernel Size", 1, 5, 1),
                        add_spin_boxes("L2 Regularization Conv", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Conv Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Conv", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # MultiHeadAttention layers
                    attention_spin_boxes = [
                        add_spin_boxes("Number of Attention Layers", 0, 100, 10),
                        add_spin_boxes("Attention Heads", 2, 8, 4, step=2),
                        add_spin_boxes("Attention Key Dim", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Attention", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Attention Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Attention", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Dense layers
                    dense_spin_boxes = [
                        add_spin_boxes("Number of Dense Layers", 0, 100, 10),
                        add_spin_boxes("Dense Units", 16, 256, 128, step=32),
                        add_spin_boxes("L2 Regularization Dense", 1e-6, 1e-2, 1e-4, step=1e-6, is_float=True),
                        add_spin_boxes("Number of Dense Dropout Layers", 0, 5, 1),
                        add_spin_boxes("Dropout Rate Dense", 0.0, 0.2, 0.1, step=0.1, is_float=True)
                    ]

                    # Add the spin boxes to the grid layout
                    add_spin_boxes_to_grid(layout, 0, 0, *lstm_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 2, *conv_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 4, *attention_spin_boxes)
                    add_spin_boxes_to_grid(layout, 0, 6, *dense_spin_boxes)

                    # Learning rate
                    lr_label, lr_min, lr_max, lr_step = add_spin_boxes("Learning Rate", 1e-4, 1e-2, 1e-3, step=1e-4,
                                                                       is_float=True)
                    layout.addWidget(lr_label, 20, 0, 1, 2)
                    layout.addWidget(QLabel("Min"), 21, 0)
                    layout.addWidget(lr_min, 21, 1)
                    layout.addWidget(QLabel("Max"), 22, 0)
                    layout.addWidget(lr_max, 22, 1)
                    layout.addWidget(QLabel("Step"), 23, 0)
                    layout.addWidget(lr_step, 23, 1)

                    # Max trials
                    label_max_trials = QLabel("Maximum Trials")
                    spinBox_max_trials = QSpinBox(self)
                    spinBox_max_trials.setMinimum(0)
                    spinBox_max_trials.setMaximum(2000)
                    spinBox_max_trials.setValue(10)
                    layout.addWidget(label_max_trials, 24, 0, 1, 2)
                    layout.addWidget(spinBox_max_trials, 25, 0, 1, 2)

                    # Batch size
                    label_batch_size = QLabel("Batch Size")
                    spinBox_batch_size = QSpinBox(self)
                    spinBox_batch_size.setMinimum(32)
                    spinBox_batch_size.setMaximum(2048)
                    spinBox_batch_size.setValue(16)
                    layout.addWidget(label_batch_size, 26, 0, 1, 2)
                    layout.addWidget(spinBox_batch_size, 27, 0, 1, 2)

                    # OK button
                    button = QPushButton("OK")
                    button.clicked.connect(dialog.accept)
                    layout.addWidget(button, 28, 0, 1, 2)

                    layout.setColumnStretch(1, 1)
                    layout.setColumnStretch(3, 1)
                    layout.setColumnStretch(5, 1)
                    layout.setColumnStretch(7, 1)

                    dialog.setLayout(layout)
                    dialog.setMinimumWidth(800)  # Increase minimum width
                    dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
                    dialog.adjustSize()

                    # Execute the dialog and get the values
                    if dialog.exec() == QDialog.DialogCode.Accepted:
                        # LSTM layer parameters
                        num_lstm_layers_min = lstm_spin_boxes[0][1].value()
                        num_lstm_layers_max = lstm_spin_boxes[0][2].value()
                        num_lstm_layers_step = lstm_spin_boxes[0][3].value()
                        lstm_units_min = lstm_spin_boxes[1][1].value()
                        lstm_units_max = lstm_spin_boxes[1][2].value()
                        lstm_units_step = lstm_spin_boxes[1][3].value()
                        l2_lstm_min = lstm_spin_boxes[2][1].value()
                        l2_lstm_max = lstm_spin_boxes[2][2].value()
                        l2_lstm_step = lstm_spin_boxes[2][3].value()
                        dropout_lstm_min = lstm_spin_boxes[4][1].value()
                        dropout_lstm_max = lstm_spin_boxes[4][2].value()
                        dropout_lstm_step = lstm_spin_boxes[4][3].value()
                        min_dropout_lstm = lstm_spin_boxes[3][1].value()
                        max_dropout_lstm = lstm_spin_boxes[3][2].value()
                        dropout_lstm_layer_step = lstm_spin_boxes[3][3].value()

                        # Conv layer parameters
                        num_conv_layers_min = conv_spin_boxes[0][1].value()
                        num_conv_layers_max = conv_spin_boxes[0][2].value()
                        num_conv_layers_step = conv_spin_boxes[0][3].value()
                        conv_filters_min = conv_spin_boxes[1][1].value()
                        conv_filters_max = conv_spin_boxes[1][2].value()
                        conv_filters_step = conv_spin_boxes[1][3].value()
                        conv_kernel_size_min = conv_spin_boxes[2][1].value()
                        conv_kernel_size_max = conv_spin_boxes[2][2].value()
                        conv_kernel_size_step = conv_spin_boxes[2][3].value()
                        l2_conv_min = conv_spin_boxes[3][1].value()
                        l2_conv_max = conv_spin_boxes[3][2].value()
                        l2_conv_step = conv_spin_boxes[3][3].value()
                        dropout_conv_min = conv_spin_boxes[5][1].value()
                        dropout_conv_max = conv_spin_boxes[5][2].value()
                        dropout_conv_step = conv_spin_boxes[5][3].value()
                        min_dropout_conv = conv_spin_boxes[4][1].value()
                        max_dropout_conv = conv_spin_boxes[4][2].value()
                        dropout_conv_layer_step = conv_spin_boxes[4][3].value()

                        # Attention layer parameters
                        num_attention_layers_min = attention_spin_boxes[0][1].value()
                        num_attention_layers_max = attention_spin_boxes[0][2].value()
                        num_attention_layers_step = attention_spin_boxes[0][3].value()
                        attention_heads_min = attention_spin_boxes[1][1].value()
                        attention_heads_max = attention_spin_boxes[1][2].value()
                        attention_heads_step = attention_spin_boxes[1][3].value()
                        attention_key_dim_min = attention_spin_boxes[2][1].value()
                        attention_key_dim_max = attention_spin_boxes[2][2].value()
                        attention_key_dim_step = attention_spin_boxes[2][3].value()
                        l2_attention_min = attention_spin_boxes[3][1].value()
                        l2_attention_max = attention_spin_boxes[3][2].value()
                        l2_attention_step = attention_spin_boxes[3][3].value()
                        dropout_attention_min = attention_spin_boxes[5][1].value()
                        dropout_attention_max = attention_spin_boxes[5][2].value()
                        dropout_attention_step = attention_spin_boxes[5][3].value()
                        min_dropout_attention = attention_spin_boxes[4][1].value()
                        max_dropout_attention = attention_spin_boxes[4][2].value()
                        dropout_attention_layer_step = attention_spin_boxes[4][3].value()

                        # Dense layer parameters
                        num_dense_layers_min = dense_spin_boxes[0][1].value()
                        num_dense_layers_max = dense_spin_boxes[0][2].value()
                        num_dense_layers_step = dense_spin_boxes[0][3].value()
                        dense_units_min = dense_spin_boxes[1][1].value()
                        dense_units_max = dense_spin_boxes[1][2].value()
                        dense_units_step = dense_spin_boxes[1][3].value()
                        l2_dense_min = dense_spin_boxes[2][1].value()
                        l2_dense_max = dense_spin_boxes[2][2].value()
                        l2_dense_step = dense_spin_boxes[2][3].value()
                        min_dropout_dense = dense_spin_boxes[3][1].value()
                        max_dropout_dense = dense_spin_boxes[3][2].value()
                        dropout_dense_layer_step = dense_spin_boxes[3][3].value()
                        dropout_dense_min = dense_spin_boxes[4][1].value()
                        dropout_dense_max = dense_spin_boxes[4][2].value()
                        dropout_dense_step = dense_spin_boxes[4][3].value()

                        # Other parameters
                        learning_rate_min = lr_min.value()
                        learning_rate_max = lr_max.value()
                        learning_rate_step = lr_step.value()
                        max_trials = spinBox_max_trials.value()
                        batch_size = spinBox_batch_size.value()
                    else:
                        return

            if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
                if regression_type_combo == "Keras Classifier":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "keras_well_classifier",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo == "Hyper Parameter":
                    # Set up TensorBoard callback
                    log_dir = os.path.join("logs", "Tensorflow", "hyper_parameter_well_classifier",
                                           datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

                elif regression_type_combo in ["TCN", "ResNet"]:
                    # TensorBoard setup
                    log_dir = f"logs/Pytorch/{regression_type_combo}_well_classifier/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"

            if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                # Check if it's a KerasRegressor and extract the underlying model
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.h5"

            elif regression_type_combo in ["TCN", "ResNet", "TabNet"]:
                # Add models to the dictionary
                file_dialog = QFileDialog()
                file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.pth"

            result = self.task_runner.run_task(
                            TensorVisualizer.run_well_categorization_task, X_clean, y_clean,
                            train_test, num_lstm_layers_min, regression_type_combo, epochs,
                            max_trials, num_lstm_layers_max, num_lstm_layers_step,
                            lstm_units_min,
                            lstm_units_max, lstm_units_step, l2_lstm_min, l2_lstm_max,
                            l2_lstm_step, min_dropout_lstm, max_dropout_lstm,
                            dropout_lstm_layer_step,
                            dropout_lstm_min, dropout_lstm_max, dropout_lstm_step,
                            num_conv_layers_min,
                            num_conv_layers_max, num_conv_layers_step, conv_filters_min,
                            conv_filters_max, conv_filters_step, conv_kernel_size_min,
                            conv_kernel_size_max, conv_kernel_size_step, l2_conv_min,
                            l2_conv_max,
                            l2_conv_step, min_dropout_conv, max_dropout_conv,
                            dropout_conv_layer_step,
                            dropout_conv_min, dropout_conv_max, dropout_conv_step,
                            num_attention_layers_min, num_attention_layers_max,
                            num_attention_layers_step,
                            attention_heads_min, attention_heads_max, attention_heads_step,
                            attention_key_dim_min, attention_key_dim_max, attention_key_dim_step,
                            l2_attention_min, l2_attention_max, l2_attention_step,
                            min_dropout_attention, max_dropout_attention,
                            dropout_attention_layer_step,
                            dropout_attention_min, dropout_attention_max, dropout_attention_step,
                            num_dense_layers_min, num_dense_layers_max, num_dense_layers_step,
                            dense_units_min, dense_units_max, dense_units_step, l2_dense_min,
                            l2_dense_max, l2_dense_step, min_dropout_dense, max_dropout_dense,
                            dropout_dense_layer_step, dropout_dense_min, dropout_dense_max,
                            dropout_dense_step, learning_rate_min, learning_rate_max,
                            learning_rate_step, log_dir, batch_size, file_path, output_dim)

            if result is not None:

                if regression_type_combo in ["Keras Classifier", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                        y_pred, best_accuracy, best_top_k, best_crossentropy = result
                    elif regression_type_combo in ["TCN", "ResNet"]:
                        y_pred, val_accuracy, avg_val_loss = result
                    elif regression_type_combo == "TabNet":
                        y_pred = result
                else:
                    y_pred, model = result

                if regression_type_combo in ["Keras Classifier", "ResNet", "TCN", "TabNet", "Hyper Parameter"]:
                    if regression_type_combo in ["Keras Classifier", "Hyper Parameter"]:
                        # Access the best accuracy recorded during training
                        self.best_accuracy = best_accuracy
                        self.best_top_k = best_top_k
                        self.best_crossentropy = best_crossentropy
                        self.show_tensorboard(log_dir=log_dir)

                    elif regression_type_combo in ["TCN", "ResNet"]:
                        self.best_val_accuracy = val_accuracy
                        self.best_val_loss = avg_val_loss  # Optional, if you want to track corresponding loss
                        self.show_tensorboard(log_dir=log_dir)

                if regression_type_combo in ["Keras Classifier", "Hyper Parameter", "TCN", "ResNet", "TabNet"]:
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.model_dict, model_type=regression_type_combo, model=None,
                                      features=selected_features, target=selected_target, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                else:  # Assume Sklearn or similar

                    # Ask the user if they want to save the model
                    reply = QMessageBox.question(self, 'Save Model', 'Do you want to save the model?',
                                                 QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                                                 QMessageBox.StandardButton.No)

                    if reply == QMessageBox.StandardButton.Yes:
                        file_dialog = QFileDialog()
                        file_directory = file_dialog.getExistingDirectory(self, "Select Directory to Save Model")
                        file_path = f"{file_directory}/{regression_type_combo}model{selected_files, selected_target, selected_features}.pkl"
                        joblib.dump(model, file_path)
                    else:
                        file_path = None
                    # Adding model to dictionary with the file path
                    add_model_to_dict(model_dict=self.model_dict, model_type=regression_type_combo, model=model,
                                      features=selected_features, target=selected_target, file_path=file_path,
                                      dimensionality_reduction_type_combo=dimensionality_reduction_type_combo,
                                      n_components=n_components)

                # Pass data to the plotting function
                self.plot_well_category_results(y_val, y_pred, regression_type_combo, result_layout, selected_files,
                                                selected_target, selected_features, parent_dialog, X_clean_copy,
                                                train_test,
                                                num_valid_points)
        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_well_category_results(self, y_actual, y_pred, regression_type, result_layout, selected_files,
                                   selected_target, selected_features, parent_dialog, X_clean_copy, train_test,
                                   num_valid_points):
        """
        Plot the predicted values against the actual values for regression analysis.
        """

        if y_actual.ndim > 1:
            y_actual = y_actual.ravel()

        if y_pred.ndim > 1 and y_pred.shape[1] == 1:
            y_pred = y_pred.ravel()

        elif y_pred.ndim > 1 and y_pred.shape[1] > 1:
            y_pred = np.argmax(y_pred, axis=1)

        # Use a colormap with sufficient distinct colors
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

        # Clear the previous plot and reset the view
        self.plot_widget.clear()
        self.plot_widget.autoRange()

        # Calculate confusion matrix
        conf_matrix = confusion_matrix(y_actual, y_pred)

        # Get the actual range of the confusion matrix
        conf_min, conf_max = conf_matrix.min(), conf_matrix.max()

        # Create an ImageItem for the confusion matrix
        img = pg.ImageItem(conf_matrix.T)

        # Set the colormap to the ImageItem
        levels = (conf_min, conf_max)  # Use the actual range of the raw values
        img.setLookupTable(cmap.getLookupTable(nPts=256))  # 256 points for the color map
        img.setLevels(levels)  # Set levels to match the actual data range

        # Add the image to the plot
        self.plot_widget.addItem(img)

        # Set axis labels
        self.plot_widget.setLabel('bottom', 'Predicted Class')
        self.plot_widget.setLabel('left', 'Actual Class')

        # Adjust the view to align tiles correctly with axes
        img.setRect(QRectF(0, 0, conf_matrix.shape[1], conf_matrix.shape[0]))

        # Add text labels for confusion matrix values
        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                value = conf_matrix[i, j]
                text = pg.TextItem(str(value), anchor=(0.5, 0.5))
                text.setPos(j + 0.5, i + 0.5)  # Center the text in each tile
                self.plot_widget.addItem(text)

        # Create or update the ColorBarItem for the color bar
        if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
            self.well_color_bar.setLevels(levels)  # Update levels to match the actual range
            self.well_color_bar.setColorMap(cmap)
        else:
            color_bar = pg.ColorBarItem(values=levels)
            color_bar.setColorMap(cmap)
            self.well_color_bar = color_bar
            self.plot_widget.getPlotItem().layout.addItem(self.well_color_bar, 2, 2)

        self.plot_widget.autoRange()

        if regression_type in ["Keras Classifier", "Hyper Parameter"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual, y_pred):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Sparse Categorical Accuracy: {self.best_accuracy:.2f}\n"
            result_text += f"Sparse Top K Categorical Accuracy: {self.best_top_k:.2f}\n"
            result_text += f"Sparse Categorical Crossentropy: {self.best_crossentropy :.2f}\n"

        elif regression_type in ["ResNet", "TCN"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual, y_pred):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Sparse Categorical Accuracy: {self.best_val_accuracy:.2f}\n"
            result_text += f"Sparse Categorical Crossentropy: {self.best_val_loss:.2f}\n"

        elif regression_type in ["TabNet"]:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual, y_pred):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
        else:
            result_text = f"{regression_type} Results:\n"
            result_text += f"Accuracy: {accuracy_score(y_actual, y_pred):.2f}\n"
            result_text += f"Precision: {precision_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"Recall: {recall_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"
            result_text += f"F1 Score: {f1_score(y_actual, y_pred, average='weighted', zero_division=0):.2f}\n"

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def plot_cross_plot(self, selected_files, common_columns_list, regression_type_combo, result_layout, parent_dialog,
                        num_valid_points):

        # Plot the cross plot based on selected columns and regression type
        selected_columns = [item.text() for item in common_columns_list.selectedItems()]

        if len(selected_columns) < 2:
            QMessageBox.warning(self, "Warning", "Please select at least 2 columns.")
            return

        # Assume there's only one selected file for simplicity in retrieving units
        selected_file_key = selected_files[0] if selected_files else None

        # Retrieve the units dictionary for the selected file
        units = self.units_dict.get(selected_file_key, {})

        # Initialize x_data, y_data, and c_data as None
        x_data, y_data, c_data = None, None, None

        if len(selected_files) == 1:
            # Only one file selected
            df = self.tensor_dict.get(selected_files[0]).copy()
            if df is not None:
                x_data = df[selected_columns[0]]
                y_data = df[selected_columns[1]]

                if len(selected_columns) == 3:
                    c_data = df[selected_columns[2]]
        else:
            # Multiple files selected
            data_frames = [self.tensor_dict.get(file).copy() for file in selected_files if
                           self.tensor_dict.get(file) is not None]
            if not data_frames:
                QMessageBox.warning(self, "Warning", "Selected columns not found in the DataFrames.")
                return

            merged_data = data_frames[0]
            for df in data_frames[1:]:
                merged_data = pd.merge(merged_data, df, on=selected_columns, how='outer')

            x_data = merged_data[selected_columns[0]]
            y_data = merged_data[selected_columns[1]]

            if len(selected_columns) == 3:
                c_data = merged_data[selected_columns[2]]

        # Convert data to numeric type, ignoring non-numeric data and treating negative values as NaN
        x_data = pd.to_numeric(x_data, errors='coerce')
        y_data = pd.to_numeric(y_data, errors='coerce')

        # Replace negative values with NaN
        x_data = x_data.apply(lambda x: pd.NA if x < 0 else x)
        y_data = y_data.apply(lambda x: pd.NA if x < 0 else x)

        # Filter out NaN values
        mask = x_data.notna() & y_data.notna()
        if c_data is not None:
            c_data = pd.to_numeric(c_data, errors='coerce')
            c_data = c_data.apply(lambda x: pd.NA if x < 0 else x)
            mask &= c_data.notna()

        x_data = x_data[mask]
        y_data = y_data[mask]
        if c_data is not None:
            c_data = c_data[mask]

        # Ensure all data is numeric and finite
        x_data = x_data.astype(float)
        y_data = y_data.astype(float)
        if c_data is not None:
            c_data = c_data.astype(float)

        self.plot_widget.clear()
        self.plot_widget.autoRange()

        if c_data is not None:
            # Normalize c_data for color mapping
            self.well_color_data = (c_data - c_data.min()) / (c_data.max() - c_data.min())

            # Use a colormap with sufficient distinct colors
            cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')
            colors = cmap.map(self.well_color_data, mode='qcolor')

            scatter = pg.ScatterPlotItem(x=x_data, y=y_data, pen=None, brush=colors)
            self.plot_widget.addItem(scatter)

            # Create or update the ColorBarItem for the color bar
            if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
                self.well_color_bar.setLevels((c_data.min(), c_data.max()))
                self.plot_widget.getPlotItem().layout.removeItem(self.color_bar_label_item)
                self.well_color_bar.label = selected_columns[2]  # Update the color bar title with the third column name
                self.well_color_bar.setColorMap(cmap)
            else:
                color_bar = pg.ColorBarItem(values=(c_data.min(), c_data.max()))
                color_bar.setColorMap(cmap)
                color_bar.label = selected_columns[2]  # Set the color bar title to the third column name
                self.well_color_bar = color_bar
                self.plot_widget.getPlotItem().layout.addItem(self.well_color_bar, 2,
                                                              2)  # Add to the layout in the next column

            # Add vertical label for the color bar with units
            color_bar_unit_label = f"({units.get(selected_columns[2], '')})"
            self.color_bar_label_item = pg.LabelItem(text=f"{selected_columns[2]} {color_bar_unit_label}", angle=90,
                                                     color='k', size='10pt')
            self.plot_widget.getPlotItem().layout.addItem(self.color_bar_label_item, 2, 2, 1, 1)

            self.plot_widget.getPlotItem().layout.setContentsMargins(0, 0, 30, 0)  # Adjust the values as needed

        else:
            cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

            color_value = 1

            # Get the QColor object for the specific value from the colormap
            color = cmap.map(color_value, mode='qcolor')

            # Create the ScatterPlotItem with the chosen brush color
            scatter = pg.ScatterPlotItem(x=x_data, y=y_data, pen=None, symbol='o', brush=color)
            self.plot_widget.addItem(scatter)
            self.well_color_data = np.ones(len(x_data), dtype=int)
            # Remove the color bar if it exists
            if hasattr(self, 'well_color_bar') and self.well_color_bar is not None:
                self.plot_widget.getPlotItem().layout.removeItem(self.color_bar_label_item)
                self.plot_widget.getPlotItem().layout.removeItem(self.well_color_bar)
                self.well_color_bar = None
                self.color_bar_label_item = None

        # Set axis labels with units
        x_unit_label = f"({units.get(selected_columns[0], '')})"
        y_unit_label = f"({units.get(selected_columns[1], '')})"
        self.plot_widget.setLabel('bottom', f"{selected_columns[0]} {x_unit_label}")  # x-axis label with unit
        self.plot_widget.setLabel('left', f"{selected_columns[1]} {y_unit_label}")  # y-axis label with unit

        # Assuming self.regression_type_combo is accessible or pass it as a parameter
        regression_type = regression_type_combo.currentText()
        if regression_type == "Linear Regression":
            self.perform_linear_fit(x_data, y_data, result_layout, num_valid_points)
        else:
            self.perform_nonlinear_fit(x_data, y_data, result_layout, num_valid_points)

    def perform_linear_fit(self, x_data, y_data, result_layout, num_valid_points):
        # Reshape the data to fit the model requirements
        x_data_reshaped = x_data.values.reshape(-1, 1)
        y_data_reshaped = y_data.values.reshape(-1, 1)

        # Fit the linear regression model
        model = LinearRegression()
        model.fit(x_data_reshaped, y_data_reshaped)

        # Make predictions
        y_fit = model.predict(x_data_reshaped).flatten()  # Flatten to 1D

        # Calculate correlation coefficients
        pearson_corr, _ = pearsonr(x_data.values.flatten(), y_data.values.flatten())
        spearman_corr, _ = spearmanr(x_data.values.flatten(), y_data.values.flatten())
        r_squared = model.score(x_data_reshaped, y_data_reshaped)

        # Prepare the result text
        result_text = f"y = {model.coef_[0][0]:.2f}x + {model.intercept_[0]:.2f}\n"
        result_text += f"Number of Samples: {num_valid_points}\n"
        result_text += f"Intercept: {model.intercept_[0]:.2f}\n"
        result_text += f"Pearson Correlation: {pearson_corr:.2f}\n"
        result_text += f"Spearman Correlation: {spearman_corr:.2f}\n"
        result_text += f"R-squared Score: {r_squared:.2f}\n"

        # Plot the fitted line using PyQtGraph
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

        color_value = 0.5

        # Get the QColor object for the specific value from the colormap
        color = cmap.map(color_value, mode='qcolor')

        line = pg.PlotCurveItem(x_data.values, y_fit, pen=color)  # Ensure x_data is 1D
        self.plot_widget.autoRange()
        self.plot_widget.addItem(line)

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def perform_nonlinear_fit(self, x_data, y_data, result_layout, num_valid_points):
        # Reshape the data to fit the model requirements
        x_data_reshaped = x_data.values.reshape(-1, 1)
        y_data_reshaped = y_data.values.reshape(-1, 1)

        # Transform the data into polynomial features
        poly = PolynomialFeatures(degree=2)
        x_poly = poly.fit_transform(x_data_reshaped)

        # Fit the polynomial regression model
        model = LinearRegression()
        model.fit(x_poly, y_data_reshaped)

        # Make predictions
        y_fit = model.predict(x_poly).flatten()  # Flatten to 1D

        # Generate a dense range of x values for a smooth curve
        x_dense = np.linspace(x_data.min(), x_data.max(), 1000).reshape(-1, 1)
        x_dense_poly = poly.transform(x_dense)
        y_dense_fit = model.predict(x_dense_poly).flatten()

        # Calculate correlation coefficients
        pearson_corr, _ = pearsonr(y_data.values.flatten(), y_fit)
        spearman_corr, _ = spearmanr(y_data.values.flatten(), y_fit)
        r_squared = model.score(x_poly, y_data_reshaped)

        # Prepare the result text
        result_text = f"y = {model.coef_[0][2]:.2f}x^2 + {model.coef_[0][1]:.2f}x + {model.intercept_[0]:.2f}\n"
        result_text += f"Number of Samples: {num_valid_points}\n"
        result_text += f"Intercept: {model.intercept_[0]:.2f}\n"
        result_text += f"Pearson Correlation: {pearson_corr:.2f}\n"
        result_text += f"Spearman Correlation: {spearman_corr:.2f}\n"
        result_text += f"R-squared Score: {r_squared:.2f}\n"

        # Plot the smooth fitted curve using PyQtGraph
        # Plot the fitted line using PyQtGraph
        cmap = pg.colormap.get(self.well_color_mapping, source='matplotlib')

        color_value = 0.5

        # Get the QColor object for the specific value from the colormap
        color = cmap.map(color_value, mode='qcolor')

        curve = pg.PlotCurveItem(x_dense.flatten(), y_dense_fit, pen=color)  # Ensure x_dense is flattened
        self.plot_widget.autoRange()
        self.plot_widget.addItem(curve)

        # Update the result area
        self.update_result_area(result_text, result_layout)

    def update_result_area(self, text, result_layout):
        # Clear the previous results
        for i in reversed(range(result_layout.count())):
            result_layout.itemAt(i).widget().setParent(None)

        # Add the new result
        label = QLabel(text)

        # Replace '^' with HTML superscript tags for proper formatting
        formatted_text = text.replace("^2", "<sup>2</sup>").replace("^", "<sup>")
        label.setText(f"<p style='white-space: pre;'>{formatted_text}</p>")

        # Set font
        font = QFont('Times New Roman',
                     10)  # You can change 'Times New Roman' to your preferred font and '14' to your preferred size
        label.setFont(font)

        # Set stylesheet
        label.setStyleSheet("""
        QLabel {
        color: #555;              /* Text color */
        background-color: #ddd;   /* Background color */
        border: 2px solid #ccc;   /* Border with 2px solid line and color */
        padding: 4px;             /* Padding around text */
        border-radius: 4px;       /* Rounded corners */
        }
        """)

        # Set alignment
        label.setAlignment(Qt.AlignmentFlag.AlignCenter)  # Align the text to the center
        result_layout.addWidget(label)

    def _fftfreq(self, n_samples):
        """Get FFT frequencies using either CuPy (GPU) or NumPy (CPU)."""
        if gpu_available:
            return cp.fft.fftfreq(n_samples,
                                  d=self.metadata[self.file_name].get('sampling_interval_ms') / 1000.0)  # Convert ms to seconds
        else:
            return fftfreq(n_samples, d=self.metadata[self.file_name].get('sampling_interval_ms') / 1000.0)

    def automatic_band_selection(self):
        """
        Manually select a frequency band or a single frequency.

        Returns:
            A tuple representing the selected frequency range (start, end).
            If a single frequency is selected, start == end.
        """

        data = self.tensor_data
        time_dim = data.shape[0]
        freqs = self._fftfreq(time_dim)
        min_freq = np.min(freqs[freqs > 0])  # Only consider positive frequencies
        max_freq = np.max(freqs)

        # Create the dialog
        dialog = QDialog(self)
        dialog.setWindowTitle("Select Frequency Band")
        layout = QVBoxLayout()

        # Display min and max frequency
        freq_label = QLabel(f"Frequency Range: {min_freq:.2f} Hz to {max_freq:.2f} Hz")
        layout.addWidget(freq_label)

        # Create SpinBoxes for start and end frequencies
        start_spin_box = QSpinBox()
        start_spin_box.setMinimum(int(min_freq))
        start_spin_box.setMaximum(int(max_freq))
        start_spin_box.setValue(int(min_freq))
        layout.addWidget(QLabel("Start Frequency (Hz):"))
        layout.addWidget(start_spin_box)

        end_spin_box = QSpinBox()
        end_spin_box.setMinimum(int(min_freq))
        end_spin_box.setMaximum(int(max_freq))
        end_spin_box.setValue(int(max_freq))
        layout.addWidget(QLabel("End Frequency (Hz):"))
        layout.addWidget(end_spin_box)

        # Ensure spin boxes are constrained dynamically
        def update_end_spin_box():
            end_spin_box.setMinimum(start_spin_box.value())

        def update_start_spin_box():
            start_spin_box.setMaximum(end_spin_box.value())

        start_spin_box.valueChanged.connect(update_end_spin_box)
        end_spin_box.valueChanged.connect(update_start_spin_box)

        # Add OK button to close the dialog
        button = QPushButton("OK")
        button.clicked.connect(dialog.accept)
        layout.addWidget(button)

        dialog.setLayout(layout)
        dialog.setMinimumWidth(250)
        dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        dialog.adjustSize()

        # Show the dialog and get the result
        if dialog.exec() == QDialog.DialogCode.Accepted:
            start_freq = start_spin_box.value()
            end_freq = end_spin_box.value()
            return (start_freq, end_freq)
        else:
            return None

    @staticmethod
    def spectral_decomposition_task(data, sampling_interval, low_freq, high_freq, window_size):
        """
        Memory-efficient spectral decomposition for large 4D datasets on CPU

        Parameters:
            data: 4D numpy array with shape (time, height, width, channel)
            sampling_interval_ms: sampling interval in milliseconds
            low_freq: lower frequency bound in Hz
            high_freq: upper frequency bound in Hz
            window_size: number of spatial slices to process at once

        Returns:
            Filtered 4D tensor with the same shape as input, containing only the specified frequency band
        """
        time_samples, height, width, channels = data.shape

        window_size = min(height, window_size)

        # Calculate frequencies once (convert ms to seconds)
        freqs = fftfreq(time_samples, d=sampling_interval)

        # Create frequency mask for bandpass filter
        band_mask = (freqs >= low_freq) & (freqs <= high_freq)

        # Initialize output tensor
        output_tensor = np.zeros_like(data, dtype=np.float32)

        # Process data in spatial windows for memory efficiency
        for h_start in range(0, height, window_size):
            h_end = min(h_start + window_size, height)

            # Extract current window
            window_data = data[:, h_start:h_end, :, :]

            # Perform FFT along time dimension
            window_freq_data = fft(window_data, axis=0)

            # Apply bandpass filter (zero out frequencies outside the band)
            filtered_freq_data = np.zeros_like(window_freq_data)
            for t in range(time_samples):
                if band_mask[t]:
                    filtered_freq_data[t, ...] = window_freq_data[t, ...]

            # Inverse FFT to get filtered time-domain signal
            window_filtered = ifft(filtered_freq_data, axis=0).real

            # Store in output tensor
            output_tensor[:, h_start:h_end, :, :] = window_filtered

            # Free memory
            del window_data, window_freq_data, filtered_freq_data, window_filtered

        return output_tensor

    @staticmethod
    def spectral_decomposition_cpu_task(data, sampling_interval, low_freq, high_freq):

        time_dim = data.shape[0]

        # Perform FFT along the time dimension
        freq_data = fft(data, axis=0)

        # Frequency axis
        freqs = fftfreq(time_dim, d=sampling_interval)

        # Extract the relevant frequency components for the selected band
        band_mask = (freqs >= low_freq) & (freqs <= high_freq)

        # Apply the bandpass filter and inverse FFT
        filtered_data = np.zeros_like(freq_data)
        filtered_data[band_mask, ...] = freq_data[band_mask, ...]

        # Inverse FFT to get the time-domain signal for the specific frequency band
        band_data = ifft(filtered_data, axis=0).real

        # Ensure output is 4D with the same shape, especially keeping the channel dimension
        output_tensor = band_data.reshape(data.shape)

        return output_tensor

    @staticmethod
    def spectral_decomposition_gpu_task(data, sampling_interval, low_freq, high_freq):
        tensor_data = cp.asarray(data)
        time_dim = tensor_data.shape[0]

        # Perform FFT along the time dimension
        freq_data = cp.fft.fft(tensor_data, axis=0)

        # Frequency axis
        freqs = cp.fft.fftfreq(time_dim, d=sampling_interval)  # Convert ms to seconds

        # Extract the relevant frequency components for the selected band
        band_mask = (freqs >= low_freq) & (freqs <= high_freq)

        # Apply the bandpass filter and inverse FFT
        filtered_data = cp.zeros_like(freq_data)
        filtered_data[band_mask, ...] = freq_data[band_mask, ...]

        # Inverse FFT to get the time-domain signal for the specific frequency band
        band_data = cp.fft.ifft(filtered_data, axis=0).real

        output_tensor = cp.asnumpy(band_data)  # Convert back to NumPy array if using GPU

        # Ensure output is 4D with the same shape, especially keeping the channel dimension
        output_tensor = output_tensor.reshape(data.shape)

        return output_tensor

    def spectral_decomposition(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return

        # Get the frequency range (start, end) from the automatic_band_selection function
        freq_band = self.automatic_band_selection()

        if freq_band is None:
            return

        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            low_freq, high_freq = freq_band
            data = self.tensor_data.astype(np.float32)
            sampling_interval = self.metadata[self.file_name].get('sampling_interval_ms') / 1000.0
            gpu = False

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = data.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 6:
                    cp.get_default_memory_pool().free_all_blocks()

                    gpu = False
                else:
                    gpu = True

            if gpu:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.spectral_decomposition_gpu_task(data, sampling_interval, low_freq, high_freq)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.spectral_decomposition_gpu_task,
                                                       data, sampling_interval, low_freq, high_freq)
            else:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.spectral_decomposition_cpu_task(data, sampling_interval, low_freq, high_freq)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.spectral_decomposition_task, data,
                                                       sampling_interval, low_freq, high_freq, self.window_size)

            if result is not None:
                self.tensor_data = result
                # Store the modified tensor with an informative key
                range_str = f"{low_freq}-{high_freq} Hz"
                self.metadata[f"{self.file_name}_Spectral Decomposition {range_str}"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Spectral Decomposition {range_str}"][
                    'name'] = f"{self.metadata[f'{self.file_name}_Spectral Decomposition {range_str}'].get('name', '')} Spectral Decomposition {range_str}"
                self.add_tensor(f"{self.file_name}_Spectral Decomposition {range_str}", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def fft_process_in_spatial_windows(data, sampling_interval, window_size):
        """
        Process a 4D tensor using spatial windowing to minimize memory usage

        Parameters:
            data: 4D tensor with shape (time, height, width, channel)
            sampling_interval_ms: sampling interval
            window_size: number of height slices to process at once

        Returns:
            freqs: positive frequency values
            sum_amplitude_spectrum: summed amplitude spectrum
        """
        # Get dimensions
        time_samples, height, width, channels = data.shape

        window_size = min(height, window_size)

        # Calculate frequency values once
        freqs = np.fft.fftfreq(time_samples, d=sampling_interval)
        positive_mask = freqs > 0
        freqs_positive = freqs[positive_mask]

        # Initialize sum of amplitude spectrum (only for positive frequencies)
        sum_amplitude_spectrum = np.zeros(len(freqs_positive), dtype=np.float32)

        # Process data in windows along the height dimension
        for h_start in range(0, height, window_size):
            h_end = min(h_start + window_size, height)

            # Extract current spatial window
            window_data = data[:, h_start:h_end, :, :]

            # Perform FFT on this window along time dimension
            window_fft = np.fft.fft(window_data, axis=0)

            # Calculate amplitude spectrum
            window_amplitude = np.abs(window_fft)

            # Keep only positive frequencies
            window_amplitude_positive = window_amplitude[positive_mask]

            # Reshape and sum spatial dimensions for this window
            window_sum = window_amplitude_positive.reshape(window_amplitude_positive.shape[0], -1).sum(axis=1)

            # Add to total sum
            sum_amplitude_spectrum += window_sum

            # Free memory
            del window_data, window_fft, window_amplitude, window_amplitude_positive, window_sum

        return freqs_positive, sum_amplitude_spectrum

    @staticmethod
    def fft_process_gpu(data, sampling_interval):
        # Convert the data to a CuPy array
        data = cp.asarray(data)
        # Perform FFT along the Time dimension (axis 0) using CuPy
        freq_data = cp.fft.fft(data, axis=0)
        # Calculate the amplitude spectrum (magnitude of the FFT)
        amplitude_spectrum = cp.abs(freq_data)
        # Get the frequency values corresponding to the FFT output
        freqs = cp.asnumpy(
            cp.fft.fftfreq(data.shape[0], d=sampling_interval))
        # Reshape to combine spatial dimensions into one for visualization
        reshaped_spectrum = amplitude_spectrum.reshape(data.shape[0], -1)
        # Sum the amplitude spectrum across the spatial dimensions
        sum_amplitude_spectrum = cp.asnumpy(reshaped_spectrum.sum(axis=1))

        # Keep only the positive frequencies
        positive_freqs = freqs > 0
        freqs = freqs[positive_freqs]
        sum_amplitude_spectrum = sum_amplitude_spectrum[positive_freqs]
        return freqs, sum_amplitude_spectrum

    @staticmethod
    def fft_process_cpu(data, sampling_interval):
        # Perform FFT along the Time dimension (axis 0) using NumPy
        freq_data = np.fft.fft(data, axis=0)
        # Calculate the amplitude spectrum (magnitude of the FFT)
        amplitude_spectrum = np.abs(freq_data)
        # Get the frequency values corresponding to the FFT output
        freqs = np.fft.fftfreq(data.shape[0], d=sampling_interval)
        # Reshape to combine spatial dimensions into one for visualization
        reshaped_spectrum = amplitude_spectrum.reshape(data.shape[0], -1)
        # Sum the amplitude spectrum across the spatial dimensions
        sum_amplitude_spectrum = reshaped_spectrum.sum(axis=1)

        # Keep only the positive frequencies
        positive_freqs = freqs > 0
        freqs = freqs[positive_freqs]
        sum_amplitude_spectrum = sum_amplitude_spectrum[positive_freqs]
        return freqs, sum_amplitude_spectrum

    def calculate_amplitude_spectrum(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            sampling_interval = self.metadata[self.file_name].get('sampling_interval_ms') / 1000.0
            gpu = False

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = data.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 4:
                    cp.get_default_memory_pool().free_all_blocks()

                    gpu = False
                else:
                    gpu = True

            if gpu:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.fft_process_gpu(data, sampling_interval)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.fft_process_gpu,
                                                       data, sampling_interval)
            else:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.fft_process_cpu(data, sampling_interval)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.fft_process_in_spatial_windows, data,
                                                       sampling_interval, self.window_size)

            if result is not None:
                freqs, sum_amplitude_spectrum = result
                # Visualization using PyQtGraph
                self.visualize_amplitude_spectrum(freqs, sum_amplitude_spectrum)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_spec_context_menu_event(self, event):
        # Create the main context menu
        menu = QMenu(self)

        # "Export" menu item
        export_action = QAction("Export", self.plot_spec)
        export_action.triggered.connect(self.show_plot_spec_export_dialog)
        menu.addAction(export_action)

        # Show the context menu at the position of the event
        menu.exec(event.screenPos())

    def show_plot_spec_export_dialog(self):
        # Create and show the export dialog
        self.export_plot_spec_dialog = exportDialog.ExportDialog(self.plot_spec.scene())
        if not self.isDarkTheme:
            stylesheet = """
            QWidget {{
                background-color: {light_color_2};
                color: {light_color_3};
            }}
            QListWidget {{
                background-color: {light_color_5}; /* Light grey background */
                color: {light_color_3}; /* Dark grey text */
            }}
            QListWidget::item:selected {{
                background-color: {light_color_2}; /* Slightly darker grey for selected item */
                color: {light_color_3}; /* Dark grey text for consistency */
            }}
            QTreeWidget {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
            }}
            QTreeWidget::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QTreeWidget QMenu {{
                background-color: {light_color_5}; /* Menu background */
                color: {light_color_3}; /* Menu text */
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {light_color_2}; /* Selection background */
                color: {light_color_3}; /* Selection text color */
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QLineEdit:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                border-radius: 2px; /* Corner rounding */
                height: 20px; /* Height */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QComboBox:focus {{
                border: 1px solid {light_color_8}; /* Focus border */
                border-radius: 2px; /* Corner rounding */
            }}
            QComboBox QAbstractItemView {{
                background-color: {light_color_5}; /* Background color */
                color: {light_color_3}; /* Text color */
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QDialog {{
                background-color: {light_color_1}; /* Background color */
                color: {light_color_3}; /* Selection text color */
            }}
            QAbstractItemView {{
                selection-background-color: {light_color_2}; /* Selection background */
                selection-color: {light_color_3}; /* Selection text color */
            }}
            QScrollArea {{
                background-color: {light_color_5}; /* Scroll area background */
                color: {light_color_3}; /* Scroll area text */
            }}
            QScrollArea QWidget {{
                background-color: {light_color_5}; /* Content background */
                color: {light_color_3}; /* Content text */
            }}
            QScrollBar:vertical {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            QScrollBar:horizontal {{
                background-color: {light_color_5}; /* Scrollbar background */
            }}
            """.format(
                light_color_1=self.light_color_1,
                light_color_2=self.light_color_2,
                light_color_3=self.light_color_3,
                light_color_4=self.light_color_4,
                light_color_5=self.light_color_5,
                light_color_6=self.light_color_6,
                light_color_7=self.light_color_7,
                light_color_8=self.light_color_8,
                light_color_9=self.light_color_9,
                light_color_10=self.light_color_10,
            )
        else:
            stylesheet = """
            QWidget {{
                background-color: {darkColor1};
                color: {darkColor8};
            }}
            QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                background-color: {darkColor1};
                color: {darkColor5};
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QLineEdit:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox {{
                background-color: {darkColor3};
                color: {darkColor5};
                border-radius: 2px;
                height: 20px;
                selection-background-color: {darkColor2};
                selection-color: {darkColor4};
            }}
            QComboBox:focus {{ 
                border: 1px solid {darkColor8}; 
                border-radius: 2px; /* Rounded corners for the progress bar */
            }}
            QComboBox QAbstractItemView {{
                background-color: {darkColor3}; /* Same as the combo box background */
                color: {darkColor5}; /* Text color */
                selection-background-color: {darkColor9}; /* Background color when an item is selected */
                selection-color: {darkColor5}; /* Text color when an item is selected */
            }}
            QTreeWidget {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QTreeWidget QMenu {{
                background-color: {darkColor2};
                color: {darkColor5};
            }}
            QTreeWidget QMenu::item:selected {{
                background-color: {darkColor3};
                color: {darkColor5};
            }}
            QDialog {{
                background-color: {darkColor1};
            }}
            QAbstractItemView {{
                selection-background-color: {darkColor3};
                selection-color: {darkColor5};
            }}
            QScrollArea {{
                background-color: {darkColor2}; /* Dark grey background for the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollArea QWidget {{
                background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                color: {darkColor5}; /* White text for better readability */
            }}
            QScrollBar:vertical {{
                background-color: {darkColor2};
            }}
            QScrollBar:horizontal {{
                background-color: {darkColor2};
            }}
            """.format(
                darkColor1=self.darkColor1,
                darkColor2=self.darkColor2,
                darkColor3=self.darkColor3,
                darkColor4=self.darkColor4,
                darkColor5=self.darkColor5,
                darkColor6=self.darkColor6,
                darkColor7=self.darkColor7,
                darkColor8=self.darkColor8,
                darkColor9=self.darkColor9,
                darkColor10=self.darkColor10,
                darkColor11=self.darkColor11,
                darkColor12=self.darkColor12,
                darkColor13=self.darkColor13,
            )
        self.export_plot_spec_dialog.setStyleSheet(stylesheet)
        self.export_plot_spec_dialog.show(self.plot_spec.plotItem)

    def visualize_amplitude_spectrum(self, freqs, amplitude_spectrum):
        """
        Visualizes the amplitude spectrum using PyQtGraph.

        Args:
            freqs (numpy array): Frequency values corresponding to the FFT output.
            amplitude_spectrum (numpy array): Amplitude spectrum summed over the spatial dimensions.
        """
        # Create a dialog window for the plot
        dialog = QDialog(self)
        dialog.setWindowFlags(
            dialog.windowFlags() | Qt.WindowType.WindowMaximizeButtonHint | Qt.WindowType.WindowMinimizeButtonHint)
        dialog.setWindowTitle("Amplitude Spectrum")
        layout = QVBoxLayout(dialog)

        # Create a PyQtGraph plot widget

        self.plot_spec = pg.PlotWidget()
        self.plot_spec.setMenuEnabled(False)
        self.plot_spec.setBackground(self.last_selected_color)  # Set background color
        self.plot_spec.scene().contextMenuEvent = self.plot_spec_context_menu_event

        # Plot the amplitude spectrum
        self.plot_spec.plot(freqs, amplitude_spectrum, pen=pg.mkPen(color=self.graph_brush, width=2))

        layout.addWidget(self.plot_spec)
        # Set plot labels
        self.plot_spec.setLabel('bottom', 'Frequency (Hz)')
        self.plot_spec.setLabel('left', 'Amplitude')
        self.plot_spec.setTitle('Amplitude Spectrum')

        # Show the dialog
        dialog.show()

    def processing_options(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("Multiprocessing Options")

        layout = QVBoxLayout(dialog)
        form_layout = QFormLayout()

        # Replace QSpinBox with QLineEdit for Int64 support
        self.multiprocessing_threshold_input = QLineEdit(dialog)
        self.multiprocessing_threshold_input.setText(str(self.multiprocessing_threshold))

        # Configure validator for Int64 range
        int64_validator = QIntValidator()
        int64_validator.setBottom(0)
        self.multiprocessing_threshold_input.setValidator(int64_validator)

        # Do the same for window size
        self.window_size_input = QLineEdit(dialog)
        self.window_size_input.setText(str(self.window_size))
        self.window_size_input.setValidator(int64_validator)

        form_layout.addRow("Multiprocessing Threshold:", self.multiprocessing_threshold_input)
        form_layout.addRow("Window Size:", self.window_size_input)

        layout.addLayout(form_layout)

        ok_button = QPushButton("OK", dialog)
        ok_button.clicked.connect(dialog.accept)
        layout.addWidget(ok_button)

        if dialog.exec():
            try:
                self.multiprocessing_threshold = np.int64(self.multiprocessing_threshold_input.text())
                self.window_size = np.int64(self.window_size_input.text())
            except ValueError:
                QMessageBox.warning(self, "Invalid Input", "Please enter valid integer values.")

    @staticmethod
    def instantaneous_phase_task(data, window):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        instantaneous_phase = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            slice_phase = np.degrees(np.angle(slice_analytic))

            # Store results
            instantaneous_phase[:, h_start:h_end, :, :] = slice_phase

            # Free memory
            del slice_analytic, slice_phase

        return instantaneous_phase

    def instantaneous_phase(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                analytic_signal = hilbert(data, axis=0)
                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)
                # Convert the instantaneous phase from radians to degrees
                self.tensor_data = np.degrees(inst_phase_radians)
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Inst. Phase"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Inst. Phase"][
                    'template'] = "Inst. Phase"
                self.add_tensor(f"{self.file_name}_Inst. Phase", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.instantaneous_phase_task, data, self.window_size)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Inst. Phase"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Inst. Phase"][
                        'template'] = "Inst. Phase"
                    self.add_tensor(f"{self.file_name}_Inst. Phase", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def instantaneous_frequency_task(data, sampling_interval_seconds, window):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array for instantaneous frequency
        # Note: freq array is 1 sample shorter in time dimension due to diff operation
        inst_freq_hz = np.zeros((time_samples - 1, height, width, channels), dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform and get analytic signal
            slice_analytic = hilbert(slice_data, axis=0)

            # Calculate phase
            slice_phase = np.angle(slice_analytic)

            # Unwrap the phase to correct for discontinuities
            slice_phase_unwrapped = np.unwrap(slice_phase, axis=0)

            # Calculate the time derivative of the unwrapped phase
            slice_freq = np.diff(slice_phase_unwrapped, axis=0) / sampling_interval_seconds

            # Convert the instantaneous frequency from radians per second to Hertz
            slice_freq_hz = slice_freq / (2 * np.pi)

            # Check for negative frequencies and set them to zero if necessary
            slice_freq_hz = abs(slice_freq_hz)

            # Store results in the output array
            inst_freq_hz[:, h_start:h_end, :, :] = slice_freq_hz

            # Free memory
            del slice_analytic, slice_phase, slice_phase_unwrapped, slice_freq, slice_freq_hz

        # Pad the first time sample to match the original time dimension
        return np.pad(inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)), 'edge')

    def instantaneous_frequency(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                analytic_signal = hilbert(data, axis=0)
                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)

                # Unwrap the phase to correct for discontinuities
                inst_phase_unwrapped = np.unwrap(inst_phase_radians, axis=0)

                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000

                # Calculate the time derivative of the unwrapped phase
                inst_freq = np.diff(inst_phase_unwrapped, axis=0) / sampling_interval_seconds

                # Convert the instantaneous frequency from radians per second to Hertz
                inst_freq_hz = inst_freq / (2 * np.pi)

                # Check for negative frequencies and set them to zero if necessary
                inst_freq_hz = abs(inst_freq_hz)

                self.tensor_data = np.pad(inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)),
                                          'edge')
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Inst. Frequency"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Inst. Frequency"][
                    'template'] = "Inst. Frequency"
                self.add_tensor(f"{self.file_name}_Inst. Frequency", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000

                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.instantaneous_frequency_task, data,
                                                   sampling_interval_seconds, self.window_size)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Inst. Frequency"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Inst. Frequency"][
                        'template'] = "Inst. Frequency"
                    self.add_tensor(f"{self.file_name}_Inst. Frequency", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def cosine_of_phase_task(data, window):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        cos_phase = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately after use

            slice_phase = np.angle(slice_analytic)
            del slice_analytic  # Free memory immediately after use

            slice_phase_unwrapped = np.unwrap(slice_phase, axis=0)
            del slice_phase  # Free memory immediately after use

            slice_cos_phase = np.cos(slice_phase_unwrapped)
            del slice_phase_unwrapped  # Free memory immediately after use

            # Store results
            cos_phase[:, h_start:h_end, :, :] = slice_cos_phase
            del slice_cos_phase  # Free memory immediately after use

        return cos_phase

    def cosine_of_phase(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                analytic_signal = hilbert(data, axis=0)
                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)
                # Unwrap the phase to correct for discontinuities
                inst_phase_unwrapped = np.unwrap(inst_phase_radians, axis=0)
                self.tensor_data = np.cos(inst_phase_unwrapped)
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Cos Phase"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Cos Phase"][
                    'template'] = "Cos Phase"
                self.add_tensor(f"{self.file_name}_Cos Phase", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.cosine_of_phase_task, data, self.window_size)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Cos Phase"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Cos Phase"][
                        'template'] = "Cos Phase"
                    self.add_tensor(f"{self.file_name}_Cos Phase", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def envelope_task(data, window):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        envelope = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately after use

            slice_envelope = np.abs(slice_analytic)  # Corrected variable name
            del slice_analytic  # Free memory immediately after use

            # Store results
            envelope[:, h_start:h_end, :, :] = slice_envelope
            del slice_envelope  # Free memory immediately after use

        return envelope

    def envelope(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                analytic_signal = hilbert(data, axis=0)
                self.tensor_data = np.abs(analytic_signal)
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Envelope"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Envelope"][
                    'template'] = "Envelope"
                self.add_tensor(f"{self.file_name}_Envelope", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.envelope_task, data, self.window_size)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Envelope"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Envelope"][
                        'template'] = "Envelope"
                    self.add_tensor(f"{self.file_name}_Envelope", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def sweetness_task(data, window, sampling_interval_seconds):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        sweetness = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately after use

            # Compute envelope
            slice_envelope = np.abs(slice_analytic)

            # Compute instantaneous phase
            slice_phase = np.angle(slice_analytic)
            del slice_analytic  # Free memory immediately after use

            # Unwrap phase to correct discontinuities
            slice_phase_unwrapped = np.unwrap(slice_phase, axis=0)
            del slice_phase  # Free memory immediately after use

            # Compute instantaneous frequency
            slice_inst_freq = np.diff(slice_phase_unwrapped, axis=0) / sampling_interval_seconds
            del slice_phase_unwrapped  # Free memory immediately after use

            # Convert to Hertz
            slice_inst_freq_hz = slice_inst_freq / (2 * np.pi)

            # Ensure no negative frequencies
            slice_inst_freq_hz = abs(slice_inst_freq_hz)

            # Pad to maintain shape consistency
            slice_inst_freq_hz_padded = np.pad(slice_inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)), mode='edge')

            # Ensure numerical stability in the denominator
            slice_inst_freq_hz_threshold = np.maximum(slice_inst_freq_hz_padded, 1)
            del slice_inst_freq_hz_padded  # Free memory immediately after use

            # Compute Sweetness
            slice_sweetness = slice_envelope / np.sqrt(slice_inst_freq_hz_threshold)
            del slice_envelope, slice_inst_freq_hz_threshold  # Free memory immediately after use

            # Store results
            sweetness[:, h_start:h_end, :, :] = slice_sweetness
            del slice_sweetness  # Free memory immediately after use

        return sweetness

    def sweetness(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                analytic_signal = hilbert(data, axis=0)
                envelope = np.abs(analytic_signal)

                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)

                # Unwrap the phase to correct for discontinuities
                inst_phase_unwrapped = np.unwrap(inst_phase_radians, axis=0)

                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000

                # Compute the time derivative of the unwrapped phase
                inst_freq = np.diff(inst_phase_unwrapped, axis=0) / sampling_interval_seconds

                # Convert from radians per second to Hertz
                inst_freq_hz = inst_freq / (2 * np.pi)

                # Ensure no negative frequencies
                inst_freq_hz = abs(inst_freq_hz)

                # Pad the frequency array to match the original shape
                inst_freq_hz_padded = np.pad(inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)), mode='edge')

                # Ensure no zeros in the denominator
                inst_freq_hz_threshold = np.maximum(inst_freq_hz_padded, 1)

                # Calculate Sweetness attribute
                sweetness = envelope / np.sqrt(inst_freq_hz_threshold)

                self.tensor_data = sweetness

                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Sweetness"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Sweetness"][
                    'template'] = "Sweetness"
                self.add_tensor(f"{self.file_name}_Sweetness", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.sweetness_task, data, self.window_size,
                                                   sampling_interval_seconds)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Sweetness"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Sweetness"][
                        'template'] = "Sweetness"
                    self.add_tensor(f"{self.file_name}_Sweetness", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def inst_bandwidth_task(data, window, sampling_interval_seconds):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        inst_bandwidth = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately after use

            # Compute envelope
            slice_envelope = np.abs(slice_analytic)

            # Compute time derivative of envelope
            slice_time_derivative_envelope = np.diff(slice_envelope, axis=0)
            del slice_analytic  # Free memory immediately after use

            # Pad to maintain shape consistency
            slice_time_derivative_envelope_padded = np.pad(slice_time_derivative_envelope,
                                                           ((1, 0), (0, 0), (0, 0), (0, 0)), mode='edge')

            slice_envelope_safe = np.maximum(slice_envelope, 1)
            del slice_envelope

            # Calculate the Instantaneous Bandwidth
            slice_time_derivative_envelope_per_2pi = slice_time_derivative_envelope_padded / (2 * np.pi)
            slice_time_derivative_envelope_per_hz = slice_time_derivative_envelope_per_2pi / slice_envelope_safe
            del slice_envelope_safe, slice_time_derivative_envelope_padded  # Free memory immediately after use

            # Take absolute value and normalize by sampling interval
            slice_inst_bandwidth = np.abs(slice_time_derivative_envelope_per_hz)
            del slice_time_derivative_envelope_per_hz  # Free memory immediately after use

            slice_inst_bandwidth_final = slice_inst_bandwidth / sampling_interval_seconds

            # Cap values at 125
            slice_inst_bandwidth_final[slice_inst_bandwidth_final > 125] = 125

            # Store results
            inst_bandwidth[:, h_start:h_end, :, :] = slice_inst_bandwidth_final
            del slice_inst_bandwidth_final  # Free memory immediately after use

        return inst_bandwidth

    def inst_bandwidth(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                # Compute the analytic signal
                analytic_signal = hilbert(data, axis=0)
                envelope = np.abs(analytic_signal)
                time_derivative_envelope = np.diff(envelope, axis=0)
                time_derivative_envelope_padded = np.pad(time_derivative_envelope, ((1, 0), (0, 0), (0, 0), (0, 0)),
                                                         'edge')
                # Calculate the Instantaneous Bandwidth
                time_derivative_envelope_per_2pi = time_derivative_envelope_padded / (2 * np.pi)
                envelope_safe = np.maximum(envelope, 1)
                time_derivative_envelope_per_hz = time_derivative_envelope_per_2pi / envelope_safe
                data = np.abs(time_derivative_envelope_per_hz)
                sampling_interval = self.metadata[self.file_name].get('sampling_interval_ms') / 1000
                data_final = data / sampling_interval

                data_final[data_final > 125] = 125

                self.tensor_data = data_final
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Inst. Bandwidth"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Inst. Bandwidth"][
                    'template'] = "Inst. Bandwidth"
                self.add_tensor(f"{self.file_name}_Inst. Bandwidth", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.inst_bandwidth_task, data, self.window_size,
                                                   sampling_interval_seconds)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Inst. Bandwidth"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Inst. Bandwidth"][
                        'template'] = "Inst. Bandwidth"
                    self.add_tensor(f"{self.file_name}_Inst. Bandwidth", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def dominant_frequency_task(data, window, sampling_interval_seconds):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        dominant_frequency = np.zeros_like(data, dtype=np.float32)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately after use

            # Compute envelope
            slice_envelope = np.abs(slice_analytic)

            # Calculate instantaneous bandwidth
            # Compute time derivative of envelope
            slice_time_derivative_envelope = np.diff(slice_envelope, axis=0)

            # Pad to maintain shape consistency
            slice_time_derivative_envelope_padded = np.pad(slice_time_derivative_envelope,
                                                           ((1, 0), (0, 0), (0, 0), (0, 0)), mode='edge')

            # Calculate the Instantaneous Bandwidth
            slice_time_derivative_envelope_padded_time = slice_time_derivative_envelope_padded * sampling_interval_seconds
            slice_inst_bandwidth = np.abs(slice_time_derivative_envelope_padded_time)
            del slice_time_derivative_envelope_padded, slice_time_derivative_envelope  # Free memory

            # Calculate instantaneous phase and frequency
            # Calculate the instantaneous phase in radians
            slice_inst_phase_radians = np.angle(slice_analytic)

            # Unwrap the phase to correct for discontinuities
            slice_inst_phase_unwrapped = np.unwrap(slice_inst_phase_radians, axis=0)
            del slice_inst_phase_radians  # Free memory

            # Calculate the time derivative of the unwrapped phase
            slice_inst_freq = np.diff(slice_inst_phase_unwrapped, axis=0) / sampling_interval_seconds
            del slice_inst_phase_unwrapped  # Free memory

            # Convert the instantaneous frequency from radians per second to Hertz
            slice_inst_freq_hz = slice_inst_freq / (2 * np.pi)
            del slice_inst_freq  # Free memory

            # Take absolute value for instantaneous frequency
            slice_inst_freq_hz = np.abs(slice_inst_freq_hz)

            # Pad instantaneous frequency to maintain shape consistency
            slice_inst_freq_hz_padded = np.pad(slice_inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)), mode='edge')
            del slice_inst_freq_hz  # Free memory

            # Calculate the Dominant Frequency
            slice_dominant_frequency = np.sqrt(slice_inst_freq_hz_padded ** 2 + slice_inst_bandwidth ** 2)
            del slice_inst_freq_hz_padded, slice_inst_bandwidth, slice_analytic, slice_envelope  # Free memory

            # Cap values at 125
            slice_dominant_frequency[slice_dominant_frequency > 125] = 125

            # Store results
            dominant_frequency[:, h_start:h_end, :, :] = slice_dominant_frequency
            del slice_dominant_frequency  # Free memory immediately after use

        return dominant_frequency

    def dominant_frequency(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                # Compute the analytic signal
                analytic_signal = hilbert(data, axis=0)

                envelope = np.abs(analytic_signal)
                time_derivative_envelope = np.diff(envelope, axis=0)
                time_derivative_envelope_padded = np.pad(time_derivative_envelope, ((1, 0), (0, 0), (0, 0), (0, 0)),
                                                         'edge')
                # Calculate the Instantaneous Bandwidth
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000
                time_derivative_envelope_padded_time = time_derivative_envelope_padded * sampling_interval_seconds
                inst_bandwidth = np.abs(time_derivative_envelope_padded_time)

                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)

                # Unwrap the phase to correct for discontinuities
                inst_phase_unwrapped = np.unwrap(inst_phase_radians, axis=0)

                # Calculate the time derivative of the unwrapped phase
                inst_freq = np.diff(inst_phase_unwrapped, axis=0) / sampling_interval_seconds

                # Convert the instantaneous frequency from radians per second to Hertz
                inst_freq_hz = inst_freq / (2 * np.pi)

                # Check for negative frequencies and set them to zero if necessary
                inst_freq_hz = abs(inst_freq_hz)

                inst_freq_hz_padded = np.pad(inst_freq_hz, ((1, 0), (0, 0), (0, 0), (0, 0)),
                                             'edge')
                # Calculate the Dominant Frequency
                dominant_frequency = np.sqrt(inst_freq_hz_padded ** 2 + inst_bandwidth ** 2)

                dominant_frequency[dominant_frequency > 125] = 125

                self.tensor_data = dominant_frequency
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Dom. Frequency"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Dom. Frequency"][
                    'template'] = "Dom. Frequency"
                self.add_tensor(f"{self.file_name}_Dom. Frequency", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                # Calculate the instantaneous frequency
                sampling_interval_seconds = self.metadata[self.file_name].get('sampling_interval_ms') / 1000
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.dominant_frequency_task, data, self.window_size,
                                                   sampling_interval_seconds)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Dom. Frequency"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Dom. Frequency"][
                        'template'] = "Dom. Frequency"
                    self.add_tensor(f"{self.file_name}_Dom. Frequency", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def apparent_polarity_task(data, window, threshold_factor):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Determine slice size dynamically
        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate output array
        apparent_polarity = np.zeros_like(data, dtype=np.float32)

        # Parameters for the sliding window approach
        window_size = 11  # Adjust as needed
        half_window = window_size // 2

        # First pass: calculate global threshold using sampling
        # Instead of processing all data, take samples to estimate mean
        envelope_sum = 0
        total_points = 0

        # Use sampling stride to estimate mean faster
        sampling_stride = max(1, height // 10)  # Sample ~10 slices across the volume

        for h_start in range(0, height, sampling_stride * slice_size):
            h_end = min(h_start + slice_size, height)
            if h_end <= h_start:
                continue

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform for this slice only
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately

            # Calculate envelope
            slice_envelope = np.abs(slice_analytic)
            del slice_analytic  # Free memory immediately

            # Accumulate sum for later threshold calculation
            envelope_sum += np.sum(slice_envelope)
            total_points += slice_envelope.size

            del slice_envelope  # Free memory immediately

        # Calculate global threshold
        global_threshold = threshold_factor * (envelope_sum / total_points)

        # Second pass: actual calculation
        # Pre-compute padding configuration
        pad_width = [(half_window, half_window)] + [(0, 0)] * 3  # Efficient creation of padding config

        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Apply Hilbert transform for this slice
            slice_analytic = hilbert(slice_data, axis=0)
            del slice_data  # Free memory immediately

            # Calculate envelope and phase in one go
            slice_envelope = np.abs(slice_analytic)
            slice_inst_phase = np.angle(slice_analytic)
            del slice_analytic  # Free memory immediately

            # Pad the envelope along time dimension for window operations
            padded_slice_envelope = np.pad(slice_envelope, pad_width, mode='edge')

            # Initialize slice output
            slice_apparent_polarity = np.zeros_like(slice_envelope)

            # Vectorize window operations where possible
            for i in range(time_samples):
                # Extract the window for all positions at this time point
                window_data = padded_slice_envelope[i:i + window_size]

                # Get the value at the center of the window
                center_value = slice_envelope[i]

                # Compare center with all other points in window using vectorized operations
                is_max = np.all(center_value >= np.delete(window_data, half_window, axis=0), axis=0)
                is_min = np.all(center_value <= np.delete(window_data, half_window, axis=0), axis=0)

                # Additional threshold filtering
                is_significant = center_value > global_threshold

                # Combined masks
                is_significant_max = is_max & is_significant
                is_significant_min = is_min & is_significant

                # Apply polarity values using vectorized operations
                # Create a temporary array for this time step
                temp_polarity = np.zeros_like(center_value)

                # Set values for max points
                np.place(temp_polarity, is_significant_max, np.cos(slice_inst_phase[i][is_significant_max]))

                # Set values for min points
                np.place(temp_polarity, is_significant_min, -np.cos(slice_inst_phase[i][is_significant_min]))

                # Assign to output slice
                slice_apparent_polarity[i] = temp_polarity

            # Store results
            apparent_polarity[:, h_start:h_end, :, :] = slice_apparent_polarity

            # Free memory
            del slice_envelope, slice_inst_phase, padded_slice_envelope, slice_apparent_polarity

        return apparent_polarity

    def apparent_polarity(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return

        threshold_factor, ok = QInputDialog.getDouble(
            self, "Set Threshold Factor", "Enter threshold factor:", 0.2, 0.0, 1.0, 2
        )

        if not ok:
            return  # User canceled the input

        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                # Compute the analytic signal
                window_size = 11  # Adjust as needed
                analytic_signal = hilbert(data, axis=0)
                envelope = np.abs(analytic_signal)

                # Calculate the instantaneous phase in radians
                inst_phase_radians = np.angle(analytic_signal)

                global_threshold = threshold_factor * np.mean(envelope)

                # Initialize output array
                apparent_polarity = np.zeros_like(envelope)

                # Get the shape correctly for a 4D tensor
                shape = envelope.shape
                time_points = shape[0]

                # Pad only along the time dimension
                pad_width = [(window_size // 2, window_size // 2)]
                for _ in range(len(shape) - 1):
                    pad_width.append((0, 0))
                padded_envelope = np.pad(envelope, pad_width, mode='edge')

                # Process all points directly without chunking
                for i in range(time_points):
                    # Extract the window for all positions
                    window_start = i
                    window_end = i + window_size
                    window_data = padded_envelope[window_start:window_end]

                    # Find center index in the window
                    center_idx = window_size // 2

                    # Get the value at the center of the window
                    center_value = window_data[center_idx]

                    # Create masks for points that are maxima or minima in their window
                    # Compare center with all other points in the window
                    window_points = list(range(window_size))
                    window_points.remove(center_idx)

                    # Initialize max and min masks
                    is_max = np.ones_like(center_value, dtype=bool)
                    is_min = np.ones_like(center_value, dtype=bool)

                    # Check if center is max/min compared to all other points in window
                    for idx in window_points:
                        is_max = is_max & (center_value >= window_data[idx])
                        is_min = is_min & (center_value <= window_data[idx])

                    # Additional threshold filtering
                    is_significant = center_value > global_threshold

                    # Combined masks
                    is_significant_max = is_max & is_significant
                    is_significant_min = is_min & is_significant

                    # Apply polarity values
                    if np.any(is_significant_max):
                        apparent_polarity[i][is_significant_max] = np.cos(
                            inst_phase_radians[i][is_significant_max])

                    if np.any(is_significant_min):
                        apparent_polarity[i][is_significant_min] = -np.cos(
                            inst_phase_radians[i][is_significant_min])

                self.tensor_data = apparent_polarity
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_App. Polarity"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_App. Polarity"][
                    'template'] = "App. Polarity"
                self.add_tensor(f"{self.file_name}_App. Polarity", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                # Calculate the instantaneous frequency
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.apparent_polarity_task, data, self.window_size,
                                                   threshold_factor)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_App. Polarity"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_App. Polarity"][
                        'template'] = "App. Polarity"
                    self.add_tensor(f"{self.file_name}_App. Polarity", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def rms_amplitude_task(data, window):
        # Get dimensions
        time_samples, height, width, channels = data.shape

        # Define window sizes
        window_size = 9
        half_window = window_size // 2

        max_slice_size = window
        slice_size = min(height, max_slice_size)

        # Pre-allocate the RMS amplitude tensor
        rms_amplitude = np.zeros_like(data)

        # Process data in slices along the height dimension
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)

            # Extract slice
            slice_data = data[:, h_start:h_end, :, :]

            # Pre-allocate the slice RMS amplitude
            slice_rms = np.zeros_like(slice_data)

            # Calculate RMS for each time sample
            for i in range(time_samples):
                # Define adaptive window ranges that handle edges
                start_idx = max(0, i - half_window)
                end_idx = min(time_samples, i + half_window + 1)

                # Extract the window - this processes all positions in the slice at once
                window = slice_data[start_idx:end_idx, :, :, :]

                # Calculate RMS in one vectorized operation for this time sample
                slice_rms[i, :, :, :] = np.sqrt(np.mean(np.square(window), axis=0))

            # Vectorized mirroring for edge regions
            mirror_weights = np.array([(i + 1) / (half_window + 1) for i in range(half_window)])

            # Top edge mirroring (vectorized)
            mirror_indices_top = 2 * half_window - np.arange(half_window)
            valid_mask_top = mirror_indices_top < time_samples
            if np.any(valid_mask_top):
                valid_indices = mirror_indices_top[valid_mask_top]
                for i, (mirror_idx, weight) in enumerate(zip(valid_indices, mirror_weights[valid_mask_top])):
                    slice_rms[i, :, :, :] = weight * slice_rms[i, :, :, :] + (1 - weight) * slice_rms[mirror_idx, :, :,
                                                                                            :]

            # Bottom edge mirroring (vectorized)
            bottom_indices = time_samples - np.arange(1, half_window + 1)
            mirror_indices_bottom = time_samples - 2 * half_window + np.arange(half_window) - 1
            valid_mask_bottom = (mirror_indices_bottom >= 0) & (mirror_indices_bottom < time_samples)
            if np.any(valid_mask_bottom):
                valid_bottom_indices = bottom_indices[valid_mask_bottom]
                valid_mirror_indices = mirror_indices_bottom[valid_mask_bottom]
                for i, (bottom_idx, mirror_idx, weight) in enumerate(zip(valid_bottom_indices,
                                                                         valid_mirror_indices,
                                                                         mirror_weights[valid_mask_bottom])):
                    slice_rms[bottom_idx, :, :, :] = weight * slice_rms[bottom_idx, :, :, :] + (
                            1 - weight) * slice_rms[mirror_idx, :, :, :]

            # Store this slice in the output array
            rms_amplitude[:, h_start:h_end, :, :] = slice_rms

            # Free memory
            del slice_data, slice_rms

        # Post-processing (run on the complete dataset at the end)
        # Take absolute value
        rms_amplitude = np.abs(rms_amplitude)

        # Calculate statistics and threshold
        # Use running statistics to avoid loading everything at once
        sum_rms = 0
        sum_sq_rms = 0
        count = 0

        # Calculate mean and standard deviation in chunks
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)
            slice_rms = rms_amplitude[:, h_start:h_end, :, :]

            # Update running statistics
            sum_rms += np.sum(slice_rms)
            sum_sq_rms += np.sum(np.square(slice_rms))
            count += slice_rms.size

            del slice_rms  # Free memory

        # Calculate final statistics
        mean_rms = sum_rms / count
        std_rms = np.sqrt(sum_sq_rms / count - mean_rms ** 2)  # Corrected formula for standard deviation
        threshold = mean_rms + 5 * std_rms

        # Apply thresholding in chunks
        for h_start in range(0, height, slice_size):
            h_end = min(h_start + slice_size, height)
            rms_amplitude[:, h_start:h_end, :, :] = np.minimum(rms_amplitude[:, h_start:h_end, :, :], threshold)

        return rms_amplitude

    def rms_amplitude(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            data = self.tensor_data.astype(np.float32)
            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                # Pre-allocate the RMS amplitude tensor with the same shape as the input data
                window_size = 9
                rms_amplitude = np.zeros_like(data)
                # Calculate the number of samples along the time axis
                num_samples = data.shape[0]
                # Define the half window size
                half_window = window_size // 2

                # Vectorized calculation for all time samples at once
                for i in range(num_samples):
                    # Define adaptive window ranges that handle edges
                    start_idx = max(0, i - half_window)
                    end_idx = min(num_samples, i + half_window + 1)

                    # Extract the window - this processes all inline/crossline positions at once
                    window = data[start_idx:end_idx, :, :]

                    # Calculate RMS in one vectorized operation for this time sample
                    rms_amplitude[i, :, :] = np.sqrt(np.mean(np.square(window), axis=0))

                # Vectorized mirroring for edge regions
                mirror_weights = np.array([(i + 1) / (half_window + 1) for i in range(half_window)])

                # Top edge mirroring (vectorized)
                mirror_indices_top = 2 * half_window - np.arange(half_window)
                valid_mask_top = mirror_indices_top < num_samples
                if np.any(valid_mask_top):
                    valid_indices = mirror_indices_top[valid_mask_top]
                    for i, (mirror_idx, weight) in enumerate(zip(valid_indices, mirror_weights[valid_mask_top])):
                        rms_amplitude[i, :, :] = weight * rms_amplitude[i, :, :] + (1 - weight) * rms_amplitude[
                                                                                                  mirror_idx,
                                                                                                  :, :]

                # Bottom edge mirroring (vectorized)
                bottom_indices = num_samples - np.arange(1, half_window + 1)
                mirror_indices_bottom = num_samples - 2 * half_window + np.arange(half_window) - 1
                valid_mask_bottom = (mirror_indices_bottom >= 0) & (mirror_indices_bottom < num_samples)
                if np.any(valid_mask_bottom):
                    valid_bottom_indices = bottom_indices[valid_mask_bottom]
                    valid_mirror_indices = mirror_indices_bottom[valid_mask_bottom]
                    for i, (bottom_idx, mirror_idx, weight) in enumerate(zip(valid_bottom_indices,
                                                                             valid_mirror_indices,
                                                                             mirror_weights[valid_mask_bottom])):
                        rms_amplitude[bottom_idx, :, :] = weight * rms_amplitude[bottom_idx, :, :] + (
                                1 - weight) * rms_amplitude[mirror_idx, :, :]

                # Post-processing (already vectorized)
                rms_amplitude = np.abs(rms_amplitude)
                std_rms = np.std(rms_amplitude)
                mean_rms = np.mean(rms_amplitude)
                threshold = mean_rms + 5 * std_rms
                rms_amplitude = np.minimum(rms_amplitude, threshold)

                self.tensor_data = rms_amplitude
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_RMS Amplitude"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_RMS Amplitude"][
                    'template'] = "RMS Amplitude"
                self.add_tensor(f"{self.file_name}_RMS Amplitude", self.tensor_data)
                QApplication.restoreOverrideCursor()
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.rms_amplitude_task, data, self.window_size)

                if result is not None:
                    self.tensor_data = result
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_RMS Amplitude"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_RMS Amplitude"][
                        'template'] = "RMS Amplitude"
                    self.add_tensor(f"{self.file_name}_RMS Amplitude", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def numpy_to_segy_task(data, metadata, output_path, Format, choice):
        # Assume metadata and data (NumPy array) are already defined
        data_min = np.min(data)
        data_max = np.max(data)

        # Get dimensions from the tensor shape
        time_samples, num_inlines, num_crosslines = data.shape

        # Extract the three corner points from metadata
        origin = metadata.get('origin', (0, 0))  # (x, y) for origin (inline=0, crossline=0)
        xline_end = metadata.get('xline_end', (num_crosslines - 1, 0))  # (x, y) for end of crossline axis
        inline_end = metadata.get('inline_end', (0, num_inlines - 1))  # (x, y) for end of inline axis

        # Calculate unit vectors for crossline and inline directions
        crossline_vector = (
            (xline_end[0] - origin[0]) / (num_crosslines - 1),
            (xline_end[1] - origin[1]) / (num_crosslines - 1)
        )
        inline_vector = (
            (inline_end[0] - origin[0]) / (num_inlines - 1),
            (inline_end[1] - origin[1]) / (num_inlines - 1)
        )

        # Create parameter arrays
        i_indices = np.arange(num_inlines)[:, np.newaxis]
        j_indices = np.arange(num_crosslines)

        # Calculate coordinates using vectorized operations
        x_coords = origin[0] + i_indices * inline_vector[0] + j_indices * crossline_vector[0]
        y_coords = origin[1] + i_indices * inline_vector[1] + j_indices * crossline_vector[1]

        x_min, x_max = x_coords.min(), x_coords.max()
        y_min, y_max = y_coords.min(), y_coords.max()

        # Extract inline and xline ranges from metadata
        inline_start, inline_end = metadata.get('inline_range', (0, num_inlines - 1))
        xline_start, xline_end = metadata.get('xline_range', (0, num_crosslines - 1))

        # Generate inline and crossline numbers based on metadata
        inline_numbers = np.linspace(inline_start, inline_end, num_inlines, dtype=np.int32)
        crossline_numbers = np.linspace(xline_start, xline_end, num_crosslines, dtype=np.int32)

        sample_interval = int(metadata['sampling_interval_ms'] * 1000)
        textual_header = (
                f"C 1 SEGY OUTPUT FROM SeismicFlow {datetime.now().strftime('%A, %B %d %Y %H:%M:%S')}".ljust(80) +
                f"C 2 Name: {metadata.get('name')} Type: {metadata.get('survey_type')}".ljust(80) +
                f"C 3".ljust(80) +
                f"C 4 First inline: {inline_start}    Last inline: {inline_end}".ljust(80) +
                f"C 5 First xline:  {xline_start}    Last xline: {xline_end}".ljust(80) +
                f"C 6 CRS: {metadata.get('crs', 'Undefined')}".ljust(80) +
                f"C 7 X min: {x_min} max: {x_max} delta: {x_max - x_min}".ljust(
                    80) +
                f"C 8 Y min: {y_min} max: {y_max} delta: {y_max - y_min}".ljust(
                    80) +
                f"C 9 Time min: {metadata.get('time_range', [None, None, None])[0]} "
                f"max: {metadata.get('time_range', [None, None, None])[1]} "
                f"delta: {metadata.get('time_range', [None, None, None])[2]}".ljust(80) +
                f"C10 Lat min: {metadata.get('lat_range', [None, None, None])[0] or '-'} "
                f"max: {metadata.get('lat_range', [None, None, None])[1] or '-'} "
                f"delta: {metadata.get('lat_range', [None, None, None])[2] or '-'}".ljust(80) +
                f"C11 Long min: {metadata.get('long_range', [None, None, None])[0] or '-'} "
                f"max: {metadata.get('long_range', [None, None, None])[1] or '-'} "
                f"delta: {metadata.get('long_range', [None, None, None])[2] or '-'}".ljust(80) +
                f"C12 Trace min: {metadata.get('time_range', [None, None, None])[0]} "
                f"max: {metadata.get('time_range', [None, None, None])[1]} "
                f"delta: {metadata.get('time_range', [None, None, None])[2]}".ljust(80) +
                f"C13 {metadata.get('template', None)} (template) min: {data_min:.2f} max: {data_max:.2f} delta: {(data_max - data_min):.2f}".ljust(
                    80) +
                f"C14 Amplitude (data) min: {data_min:.2f} max: {data_max:.2f} delta: {(data_max - data_min):.2f}".ljust(
                    80) +
                f"C15 Trace sample format: {choice}".ljust(80) +
                f"C16 Coordinate scale factor: 1.00000".ljust(80) +
                f"C17".ljust(80) +
                f"C18 Binary header locations:".ljust(80) +
                f"C19 Sample interval             : bytes 17-18".ljust(80) +
                f"C20 Number of samples per trace : bytes 21-22".ljust(80) +
                f"C21 Trace date format           : bytes 25-26".ljust(80) +
                f"C22".ljust(80) +
                f"C23 Trace header locations:".ljust(80) +
                f"C24 Inline number               : bytes 5-8".ljust(80) +
                f"C25 Xline number                : bytes 21-24".ljust(80) +
                f"C26 Coordinate scale factor     : bytes 71-72".ljust(80) +
                f"C27 X coordinate                : bytes 73-76".ljust(80) +
                f"C28 Y coordinate                : bytes 77-80".ljust(80) +
                f"C29 Trace start time/depth      : bytes 109-110".ljust(80) +
                f"C30 Number of samples per trace : bytes 115-116".ljust(80) +
                f"C31 Sample interval             : bytes 117-118".ljust(80) +
                f"C32".ljust(80) +
                f"C33".ljust(80) +
                f"C34".ljust(80) +
                f"C35".ljust(80) +
                f"C36".ljust(80) +
                f"C37".ljust(80) +
                f"C38".ljust(80) +
                f"C39".ljust(80) +
                f"C40 END EBCDIC".ljust(80)
        )

        # Remove test double dimension & transpose to (inline, crossline, time) in one operation
        data = np.ascontiguousarray(data.transpose((1, 2, 0)))

        # --- Infer dimensions once ---
        num_inlines, num_crosslines, num_samples = data.shape
        total_traces = num_inlines * num_crosslines

        # --- Pre-validate and prepare coordinate arrays ---
        inline_numbers = np.ascontiguousarray(inline_numbers, dtype=np.int32)
        crossline_numbers = np.ascontiguousarray(crossline_numbers, dtype=np.int32)
        x_coords = np.ascontiguousarray(x_coords)
        y_coords = np.ascontiguousarray(y_coords)

        # --- SEGY specification setup ---
        spec = segyio.spec()
        spec.sorting = 2
        spec.format = Format
        spec.samples = range(num_samples)
        spec.ilines = inline_numbers
        spec.xlines = crossline_numbers

        # --- Pre-compute trace indices ---
        il_indices = np.repeat(np.arange(num_inlines), num_crosslines)
        xl_indices = np.tile(np.arange(num_crosslines), num_inlines)

        # --- Flatten coordinate arrays ---
        x_coords_flat = x_coords.ravel()
        y_coords_flat = y_coords.ravel()

        # --- Create flattened trace data ---
        data_flat = data.reshape(-1, num_samples)

        # Get the starting time from metadata
        start_value = metadata.get('time_range', [0, 0, 0])[1]
        t_start = int(max(0, -start_value))

        with segyio.create(output_path, spec) as segyfile:
            # Write textual header
            segyfile.text[0] = textual_header.ljust(3200)[:3200]

            # Set binary header once
            segyfile.bin.update({
                segyio.BinField.Samples: num_samples,
                segyio.BinField.Interval: sample_interval,
                segyio.BinField.Format: Format
            })

            # Prepare headers for all traces at once
            for trace_idx in range(total_traces):
                il_idx = il_indices[trace_idx]
                xl_idx = xl_indices[trace_idx]

                # Set trace header
                segyfile.header[trace_idx] = {
                    segyio.TraceField.INLINE_3D: int(inline_numbers[il_idx]),
                    segyio.TraceField.CROSSLINE_3D: int(crossline_numbers[xl_idx]),
                    segyio.TraceField.SourceX: int(x_coords_flat[trace_idx]),
                    segyio.TraceField.SourceY: int(y_coords_flat[trace_idx]),
                    segyio.TraceField.TRACE_SAMPLE_COUNT: num_samples,
                    segyio.TraceField.TRACE_SAMPLE_INTERVAL: sample_interval,
                    segyio.TraceField.DelayRecordingTime: t_start
                }

                # Write trace data
                segyfile.trace[trace_idx] = data_flat[trace_idx]

        return True

    def numpy_to_segy(self):
        try:
            # Step 1: Label connected components and analyze their characteristics
            if not isinstance(self.tensor_data, np.ndarray):
                return

            result = None

            # Get the file path from the user using a file dialog window
            output_path, _ = QFileDialog.getSaveFileName(self, "Save File", f"{self.file_name}.sgy",
                                                         'SEGY Files (*.segy *.sgy);;All Files (*)')

            if not output_path:
                return

            formats = {
                "IEEE floating-point": 5,
                "IBM floating-point": 1,
                "4-byte signed integer": 2,
            }

            choice, ok = QInputDialog.getItem(
                self, "Select SEG-Y Format", "Choose format:", list(formats.keys()), 0, False
            )

            if not ok:
                return

            Format = formats[choice]  # Assign selected value to Format

            # Assuming self.tensor_data is your 4D tensor with shape (depth, height, width, channels)
            original_shape = self.tensor_data.shape

            # Check if the last dimension (channel dimension) has more than one channel
            if original_shape[-1] > 1:
                # Create an integer input dialog to select the channel
                channel, ok = QInputDialog.getInt(
                    self, "Select Channel",
                    f"Enter the channel (1 to {original_shape[-1]}):",
                    min=1, max=original_shape[-1]
                )

                # Check if the user clicked OK
                if not ok:
                    return

                # Adjust the channel index (subtract 1 because QInputDialog returns 1-based index)
                channel -= 1
            else:
                # If there's only one channel, default to that single channel
                channel = 0

            data = self.tensor_data[..., channel]
            metadata = self.metadata[self.file_name]

            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                QApplication.setOverrideCursor(self.custom_cursor)
                result = TensorVisualizer.numpy_to_segy_task(data, metadata, output_path, Format,
                                                             choice)
                QApplication.restoreOverrideCursor()
            else:
                result = self.task_runner.run_task(TensorVisualizer.numpy_to_segy_task, data, metadata, output_path, Format,
                                                   choice)

            if result is not None:
                QMessageBox.information(self, "Save Confirmation",
                                        f"SEG-Y file saved successfully as {choice}")

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "An error occurred", f"Error: {e}")

    @staticmethod
    def seismic_loading_filter_task(segy_file_path, scale, save_path, compress):
        potential_columns = [
            'SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D', 'TRACE_SEQUENCE_FILE',
            'CDP', 'EnergySourcePoint', 'FieldRecord', 'SourceEnergyDirectionExponent',
            'CDP_X', 'CDP_Y', 'ShotPoint', 'TraceValueMeasurementUnit', 'TransductionConstantMantissa'
        ]

        with segyio.open(segy_file_path, "r", ignore_geometry=True) as f:
            # Decode EBCDIC header if it's in byte format
            EBCDIC = f.text[0]
            time_samples = f.samples
            t_start_value = time_samples[0]
            # access headers
            header_keys = segyio.tracefield.keys
            data = {k: f.attributes(header_keys[k])[:] for k in potential_columns if k in header_keys}
            # Extract amplitude data
            amplitudes = np.array([f.trace[i] for i in range(f.tracecount)])
            # Access the Sample Interval (in ms) from the Binary Header
            sample_interval = f.bin[segyio.BinField.Interval] / 1000

        ebcdic_header = EBCDIC.decode('ascii', errors='ignore')
        if not ebcdic_header.startswith('C'):
            ebcdic_header = EBCDIC.translate(bytearray(a2e)).decode('ascii')

        parsed_header_metadata, ebcdic_header_metadata = parse_ebcdic_header(ebcdic_header)

        trace_headers = pd.DataFrame(data)
        columns_to_keep = ['SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D']

        # Renaming and adjusting columns based on conditions
        if (trace_headers[['SourceX', 'SourceY']] == 0).all().all():
            trace_headers = trace_headers.rename(
                columns={'SourceX': 'tmp1', 'SourceY': 'tmp2', 'CDP_X': 'SourceX', 'CDP_Y': 'SourceY'})

        if (trace_headers[['INLINE_3D', 'CROSSLINE_3D']] == 0).all().all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp3', 'CROSSLINE_3D': 'tmp4',
                         'TRACE_SEQUENCE_FILE': 'INLINE_3D',
                         'CDP': 'CROSSLINE_3D'})
            if trace_headers['CROSSLINE_3D'].nunique() == len(trace_headers) and trace_headers[
                'INLINE_3D'].nunique() > 1:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp5', 'EnergySourcePoint': 'CROSSLINE_3D'})

        if (trace_headers['INLINE_3D'] == 0).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp6', 'FieldRecord': 'INLINE_3D'})

        if (trace_headers['SourceEnergyDirectionExponent'].nunique() > 1) and (
                trace_headers['INLINE_3D'].nunique() * trace_headers['CROSSLINE_3D'].nunique() != len(
            trace_headers)):
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp7', 'SourceEnergyDirectionExponent': 'INLINE_3D'})

        if (trace_headers['INLINE_3D'] == 0).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp8', 'CROSSLINE_3D': 'tmp9', 'CDP_X': 'INLINE_3D',
                         'CDP_Y': 'CROSSLINE_3D'})

        if ('CDP_X' in trace_headers.columns) and ('CDP_Y' in trace_headers.columns) and (
                trace_headers[['INLINE_3D', 'CROSSLINE_3D']].max() > 10000).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp10', 'CROSSLINE_3D': 'tmp11', 'CDP_X': 'INLINE_3D',
                         'CDP_Y': 'CROSSLINE_3D'})

        if trace_headers['INLINE_3D'].max() > 15000:
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp12', 'CROSSLINE_3D': 'tmp13', 'ShotPoint': 'INLINE_3D',
                         'TraceValueMeasurementUnit': 'CROSSLINE_3D'})

        if (trace_headers['CROSSLINE_3D'] == 0).all():
            if trace_headers['SourceY'].max() < 10000:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp14', 'SourceY': 'CROSSLINE_3D', 'SourceX': 'tmp15',
                             'TraceValueMeasurementUnit': 'SourceX',
                             'TransductionConstantMantissa': 'SourceY'})
            else:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp16', 'CDP_Y': 'CROSSLINE_3D'})

        final_headers = trace_headers[columns_to_keep]

        del trace_headers

        sourcex = final_headers['SourceX'].to_numpy(copy=False).astype(np.float64)
        sourcey = final_headers['SourceY'].to_numpy(copy=False).astype(np.float64)
        INLINE_3D = final_headers['INLINE_3D'].to_numpy(copy=False)
        CROSSLINE_3D = final_headers['CROSSLINE_3D'].to_numpy(copy=False)

        del final_headers

        # Ensure unique values for INLINE_3D and CROSSLINE_3D using NumPy arrays
        unique_inline3d_values = np.unique(INLINE_3D)  # Sorted unique values
        unique_crossline3d_values = np.unique(CROSSLINE_3D)  # Sorted unique values

        # Get values as contiguous arrays (ensure int32 type)
        inline_values = INLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed
        crossline_values = CROSSLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed

        inline3d_index = {value: index for index, value in enumerate(unique_inline3d_values)}
        crossline3d_index = {value: index for index, value in enumerate(unique_crossline3d_values)}

        # Calculate dimensions of the 3D array
        inline3d_max = len(unique_inline3d_values)
        crossline3d_max = len(unique_crossline3d_values)

        i1, i2 = parsed_header_metadata.inline_range
        c1, c2 = parsed_header_metadata.xline_range

        if None not in (i1, i2, c1, c2):
            # Calculate fixed step sizes
            inline3d_step_size = unique_inline3d_values[1] - unique_inline3d_values[0]
            crossline3d_step_size = unique_crossline3d_values[1] - unique_crossline3d_values[0]
            num_unique_inline3d = (i2 - i1) // inline3d_step_size + 1
            num_unique_crossline3d = (c2 - c1) // crossline3d_step_size + 1
            if not inline3d_max == num_unique_inline3d:
                raise Exception("Invalid inline column.Please retry and choose the correct column manually.")
            elif not crossline3d_max == num_unique_crossline3d:
                raise Exception("Invalid crossline column.Please retry and choose the correct column manually.")

        del unique_inline3d_values, unique_crossline3d_values  # No longer needed

        if parsed_header_metadata.coord_scale_factor is None:
            scale = scale
        else:
            scale = parsed_header_metadata.coord_scale_factor

        x1, x2 = parsed_header_metadata.x_range[0], parsed_header_metadata.x_range[1]
        y1, y2 = parsed_header_metadata.y_range[0], parsed_header_metadata.y_range[1]

        # Compute existing min/max from arrays
        sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
        sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)

        if None in (x1, x2, y1, y2):
            # Apply predefined scale if we don't have reference min/max values
            sourcex = sourcex / scale
            sourcey = sourcey / scale
        else:
            # Check if scaling is needed
            scale_x = (x2 - x1) / (sourcex_max - sourcex_min) if sourcex_max != sourcex_min else 1.0
            scale_y = (y2 - y1) / (sourcey_max - sourcey_min) if sourcey_max != sourcey_min else 1.0

            # Apply scaling to align sourceX and sourceY
            sourcex = (sourcex - sourcex_min) * scale_x + x1
            sourcey = (sourcey - sourcey_min) * scale_y + y1

        # Find min and max values
        inline_min, inline_max = int(np.min(INLINE_3D)), int(np.max(INLINE_3D))
        crossline_min, crossline_max = int(np.min(CROSSLINE_3D)), int(np.max(CROSSLINE_3D))

        # Use least squares regression to compute corner coordinates
        # This is more robust than exact lookups
        INLINE_3D_f32 = INLINE_3D.astype(np.float32, copy=False)
        CROSSLINE_3D_f32 = CROSSLINE_3D.astype(np.float32, copy=False)

        # Build linear system: [INLINE, CROSSLINE, 1] * [a, b, c] = X (or Y)
        # This fits a plane: X = a*INLINE + b*CROSSLINE + c
        A = np.column_stack([INLINE_3D_f32, CROSSLINE_3D_f32, np.ones(len(INLINE_3D_f32), dtype=np.float32)])
        coeff_x, _, _, _ = np.linalg.lstsq(A, sourcex, rcond=None)
        coeff_y, _, _, _ = np.linalg.lstsq(A, sourcey, rcond=None)

        # Define corner coordinates in inline/crossline space
        corners = np.array([
            [inline_min, crossline_min],  # origin
            [inline_min, crossline_max],  # xline_end
            [inline_max, crossline_min]  # inline_end
        ], dtype=np.float32)

        # Compute X and Y coordinates for corners using the fitted coefficients
        corner_x = coeff_x[0] * corners[:, 0] + coeff_x[1] * corners[:, 1] + coeff_x[2]
        corner_y = coeff_y[0] * corners[:, 0] + coeff_y[1] * corners[:, 1] + coeff_y[2]

        # Extract individual corner tuples
        origin = (float(corner_x[0]), float(corner_y[0]))
        xline_end = (float(corner_x[1]), float(corner_y[1]))
        inline_end = (float(corner_x[2]), float(corner_y[2]))

        num_time_intervals = amplitudes.shape[1]

        seismic_cube = seismic_processing.py_populate_seismic_cube(
            amplitudes,
            inline_values,
            crossline_values,
            inline3d_index,
            crossline3d_index,
            inline3d_max,
            crossline3d_max,
            num_time_intervals
        )

        del amplitudes, inline_values, crossline_values, inline3d_index, crossline3d_index  # No longer needed

        seismic_cube = np.expand_dims(seismic_cube, axis=-1)  # Adding channel dimension

        # Transpose the tensor to match the desired orientation
        seismic_cube = np.transpose(seismic_cube, (2, 0, 1, 3))

        if parsed_header_metadata.template is None:
            parsed_header_metadata.template = 'Seismic'

        if parsed_header_metadata.template == 'phase':
            parsed_header_metadata.template = 'Cos Phase'

        if any(x is None for x in parsed_header_metadata.inline_range):
            parsed_header_metadata.inline_range = (inline_min, inline_max)

        if any(x is None for x in parsed_header_metadata.xline_range):
            parsed_header_metadata.xline_range = (crossline_min, crossline_max)

        if any(x is None for x in parsed_header_metadata.x_range):
            sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
            parsed_header_metadata.x_range = (sourcex_min, sourcex_max, sourcex_max - sourcex_min)

        if any(x is None for x in parsed_header_metadata.y_range):
            sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)
            parsed_header_metadata.y_range = (sourcey_min, sourcey_max, sourcey_max - sourcey_min)

        if any(x is None for x in parsed_header_metadata.time_range):
            parsed_header_metadata.time_range = ((-(seismic_cube.shape[0] - 1) * sample_interval) - t_start_value,
                                                 -t_start_value, (seismic_cube.shape[0] - 1) * sample_interval)

        metadata_dict = {
            key: value for key, value in vars(parsed_header_metadata).items()
            if value is not None and (not isinstance(value, (tuple, list)) or all(elem is not None for elem in value))
        }

        # Metadata you want to save
        metadata = {
            'origin': origin,
            'xline_end': xline_end,
            'inline_end': inline_end,
            "sampling_interval_ms": sample_interval,
            "ebcdic_header_metadata": ebcdic_header_metadata
        }

        metadata.update(metadata_dict)

        # If the user cancels the dialog, save_path will be empty, so we check it
        if save_path:

            if compress:

                if compress == 'Compressed':
                    np.savez_compressed(save_path, tensor=seismic_cube, metadata=metadata)
                else:
                    np.savez(save_path, tensor=seismic_cube, metadata=metadata)

        return seismic_cube, metadata

    @staticmethod
    def seismic_loading_task(INLINE_3D, CROSSLINE_3D, sourcex, sourcey, segy_file_path, scale, save_path, compress):

        # Convert coordinate arrays to float64 to handle division operations
        sourcex = sourcex.astype(np.float64, copy=False)
        sourcey = sourcey.astype(np.float64, copy=False)

        with segyio.open(segy_file_path, "r", ignore_geometry=True) as f:
            # Decode EBCDIC header if it's in byte format
            EBCDIC = f.text[0]
            # Extract time range (in milliseconds)
            time_samples = f.samples
            t_start_value = time_samples[0]
            # Extract amplitude data
            amplitudes = np.array([f.trace[i] for i in range(f.tracecount)])
            # Access the Sample Interval (in ms) from the Binary Header
            sample_interval = f.bin[segyio.BinField.Interval] / 1000

        ebcdic_header = EBCDIC.decode('ascii', errors='ignore')
        if not ebcdic_header.startswith('C'):
            ebcdic_header = EBCDIC.translate(bytearray(a2e)).decode('ascii')

        parsed_header_metadata, ebcdic_header_metadata = parse_ebcdic_header(ebcdic_header)

        # Ensure unique values for INLINE_3D and CROSSLINE_3D using NumPy arrays
        unique_inline3d_values = np.unique(INLINE_3D)  # Sorted unique values
        unique_crossline3d_values = np.unique(CROSSLINE_3D)  # Sorted unique values

        # Get values as contiguous arrays (ensure int32 type)
        inline_values = INLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed
        crossline_values = CROSSLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed

        inline3d_index = {value: index for index, value in enumerate(unique_inline3d_values)}
        crossline3d_index = {value: index for index, value in enumerate(unique_crossline3d_values)}

        # Calculate dimensions of the 3D array
        inline3d_max = len(unique_inline3d_values)
        crossline3d_max = len(unique_crossline3d_values)

        i1, i2 = parsed_header_metadata.inline_range
        c1, c2 = parsed_header_metadata.xline_range

        if None not in (i1, i2, c1, c2):
            # Calculate fixed step sizes
            inline3d_step_size = unique_inline3d_values[1] - unique_inline3d_values[0]
            crossline3d_step_size = unique_crossline3d_values[1] - unique_crossline3d_values[0]
            num_unique_inline3d = (i2 - i1) // inline3d_step_size + 1
            num_unique_crossline3d = (c2 - c1) // crossline3d_step_size + 1
            if not inline3d_max == num_unique_inline3d:
                raise Exception("Invalid inline column.Please retry and choose the correct column manually.")
            elif not crossline3d_max == num_unique_crossline3d:
                raise Exception("Invalid crossline column.Please retry and choose the correct column manually.")

        del unique_inline3d_values, unique_crossline3d_values  # No longer needed

        if parsed_header_metadata.coord_scale_factor is None:
            scale = scale
        else:
            scale = parsed_header_metadata.coord_scale_factor

        x1, x2 = parsed_header_metadata.x_range[0], parsed_header_metadata.x_range[1]
        y1, y2 = parsed_header_metadata.y_range[0], parsed_header_metadata.y_range[1]

        # Compute existing min/max from arrays
        sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
        sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)

        if None in (x1, x2, y1, y2):
            # Apply predefined scale if we don't have reference min/max values
            sourcex = sourcex / scale
            sourcey = sourcey / scale
        else:
            # Check if scaling is needed
            scale_x = (x2 - x1) / (sourcex_max - sourcex_min) if sourcex_max != sourcex_min else 1.0
            scale_y = (y2 - y1) / (sourcey_max - sourcey_min) if sourcey_max != sourcey_min else 1.0

            # Apply scaling to align sourceX and sourceY
            sourcex = (sourcex - sourcex_min) * scale_x + x1
            sourcey = (sourcey - sourcey_min) * scale_y + y1

        # Find min and max values
        inline_min, inline_max = int(np.min(INLINE_3D)), int(np.max(INLINE_3D))
        crossline_min, crossline_max = int(np.min(CROSSLINE_3D)), int(np.max(CROSSLINE_3D))

        # Use least squares regression to compute corner coordinates
        # This is more robust than exact lookups
        INLINE_3D_f32 = INLINE_3D.astype(np.float32, copy=False)
        CROSSLINE_3D_f32 = CROSSLINE_3D.astype(np.float32, copy=False)

        # Build linear system: [INLINE, CROSSLINE, 1] * [a, b, c] = X (or Y)
        # This fits a plane: X = a*INLINE + b*CROSSLINE + c
        A = np.column_stack([INLINE_3D_f32, CROSSLINE_3D_f32, np.ones(len(INLINE_3D_f32), dtype=np.float32)])
        coeff_x, _, _, _ = np.linalg.lstsq(A, sourcex, rcond=None)
        coeff_y, _, _, _ = np.linalg.lstsq(A, sourcey, rcond=None)

        # Define corner coordinates in inline/crossline space
        corners = np.array([
            [inline_min, crossline_min],  # origin
            [inline_min, crossline_max],  # xline_end
            [inline_max, crossline_min]  # inline_end
        ], dtype=np.float32)

        # Compute X and Y coordinates for corners using the fitted coefficients
        corner_x = coeff_x[0] * corners[:, 0] + coeff_x[1] * corners[:, 1] + coeff_x[2]
        corner_y = coeff_y[0] * corners[:, 0] + coeff_y[1] * corners[:, 1] + coeff_y[2]

        # Extract individual corner tuples
        origin = (float(corner_x[0]), float(corner_y[0]))
        xline_end = (float(corner_x[1]), float(corner_y[1]))
        inline_end = (float(corner_x[2]), float(corner_y[2]))

        num_time_intervals = amplitudes.shape[1]

        seismic_cube = seismic_processing.py_populate_seismic_cube(
            amplitudes,
            inline_values,
            crossline_values,
            inline3d_index,
            crossline3d_index,
            inline3d_max,
            crossline3d_max,
            num_time_intervals
        )

        del amplitudes, inline_values, crossline_values, inline3d_index, crossline3d_index  # No longer needed

        seismic_cube = np.expand_dims(seismic_cube, axis=-1)  # Adding channel dimension

        # Transpose the tensor to match the desired orientation
        seismic_cube = np.transpose(seismic_cube, (2, 0, 1, 3))

        if parsed_header_metadata.template is None:
            parsed_header_metadata.template = 'Seismic'

        if parsed_header_metadata.template == 'phase':
            parsed_header_metadata.template = 'Cos Phase'

        if any(x is None for x in parsed_header_metadata.inline_range):
            parsed_header_metadata.inline_range = (inline_min, inline_max)

        if any(x is None for x in parsed_header_metadata.xline_range):
            parsed_header_metadata.xline_range = (crossline_min, crossline_max)

        if any(x is None for x in parsed_header_metadata.x_range):
            sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
            parsed_header_metadata.x_range = (sourcex_min, sourcex_max, sourcex_max - sourcex_min)

        if any(x is None for x in parsed_header_metadata.y_range):
            sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)
            parsed_header_metadata.y_range = (sourcey_min, sourcey_max, sourcey_max - sourcey_min)

        if any(x is None for x in parsed_header_metadata.time_range):
            parsed_header_metadata.time_range = ((-(seismic_cube.shape[0] - 1) * sample_interval) - t_start_value,
                                                 -t_start_value, (seismic_cube.shape[0] - 1) * sample_interval)

        metadata_dict = {
            key: value for key, value in vars(parsed_header_metadata).items()
            if value is not None and (not isinstance(value, (tuple, list)) or all(elem is not None for elem in value))
        }

        # Metadata you want to save
        metadata = {
            'origin': origin,
            'xline_end': xline_end,
            'inline_end': inline_end,
            "sampling_interval_ms": sample_interval,
            "ebcdic_header_metadata": ebcdic_header_metadata
        }

        metadata.update(metadata_dict)

        # If the user cancels the dialog, save_path will be empty, so we check it
        if save_path:

            if compress:

                if compress == 'Compressed':
                    np.savez_compressed(save_path, tensor=seismic_cube, metadata=metadata)
                else:
                    np.savez(save_path, tensor=seismic_cube, metadata=metadata)

        return seismic_cube, metadata

    @staticmethod
    def seismic_loading_filter_huge_task(segy_file_path, scale, save_path, compress):
        potential_columns = [
            'SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D', 'TRACE_SEQUENCE_FILE',
            'CDP', 'EnergySourcePoint', 'FieldRecord', 'SourceEnergyDirectionExponent',
            'CDP_X', 'CDP_Y', 'ShotPoint', 'TraceValueMeasurementUnit', 'TransductionConstantMantissa'
        ]

        with segyio.open(segy_file_path, "r", ignore_geometry=True) as f:
            # Decode EBCDIC header if it's in byte format
            EBCDIC = f.text[0]
            # Extract time range (in milliseconds)
            time_samples = f.samples
            t_start_value = time_samples[0]
            # access headers
            header_keys = segyio.tracefield.keys
            data = {k: f.attributes(header_keys[k])[:] for k in potential_columns if k in header_keys}
            # Extract amplitude data
            amplitudes = np.array([f.trace[i] for i in range(f.tracecount)])
            # Access the Sample Interval (in ms) from the Binary Header
            sample_interval = f.bin[segyio.BinField.Interval] / 1000

        ebcdic_header = EBCDIC.decode('ascii', errors='ignore')
        if not ebcdic_header.startswith('C'):
            ebcdic_header = EBCDIC.translate(bytearray(a2e)).decode('ascii')

        parsed_header_metadata, ebcdic_header_metadata = parse_ebcdic_header(ebcdic_header)

        trace_headers = pd.DataFrame(data)
        columns_to_keep = ['SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D']

        # Renaming and adjusting columns based on conditions
        if (trace_headers[['SourceX', 'SourceY']] == 0).all().all():
            trace_headers = trace_headers.rename(
                columns={'SourceX': 'tmp1', 'SourceY': 'tmp2', 'CDP_X': 'SourceX', 'CDP_Y': 'SourceY'})

        if (trace_headers[['INLINE_3D', 'CROSSLINE_3D']] == 0).all().all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp3', 'CROSSLINE_3D': 'tmp4',
                         'TRACE_SEQUENCE_FILE': 'INLINE_3D',
                         'CDP': 'CROSSLINE_3D'})
            if trace_headers['CROSSLINE_3D'].nunique() == len(trace_headers) and trace_headers[
                'INLINE_3D'].nunique() > 1:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp5', 'EnergySourcePoint': 'CROSSLINE_3D'})

        if (trace_headers['INLINE_3D'] == 0).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp6', 'FieldRecord': 'INLINE_3D'})

        if (trace_headers['SourceEnergyDirectionExponent'].nunique() > 1) and (
                trace_headers['INLINE_3D'].nunique() * trace_headers['CROSSLINE_3D'].nunique() != len(
            trace_headers)):
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp7', 'SourceEnergyDirectionExponent': 'INLINE_3D'})

        if (trace_headers['INLINE_3D'] == 0).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp8', 'CROSSLINE_3D': 'tmp9', 'CDP_X': 'INLINE_3D',
                         'CDP_Y': 'CROSSLINE_3D'})

        if ('CDP_X' in trace_headers.columns) and ('CDP_Y' in trace_headers.columns) and (
                trace_headers[['INLINE_3D', 'CROSSLINE_3D']].max() > 10000).all():
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp10', 'CROSSLINE_3D': 'tmp11', 'CDP_X': 'INLINE_3D',
                         'CDP_Y': 'CROSSLINE_3D'})

        if trace_headers['INLINE_3D'].max() > 15000:
            trace_headers = trace_headers.rename(
                columns={'INLINE_3D': 'tmp12', 'CROSSLINE_3D': 'tmp13', 'ShotPoint': 'INLINE_3D',
                         'TraceValueMeasurementUnit': 'CROSSLINE_3D'})

        if (trace_headers['CROSSLINE_3D'] == 0).all():
            if trace_headers['SourceY'].max() < 10000:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp14', 'SourceY': 'CROSSLINE_3D', 'SourceX': 'tmp15',
                             'TraceValueMeasurementUnit': 'SourceX',
                             'TransductionConstantMantissa': 'SourceY'})
            else:
                trace_headers = trace_headers.rename(
                    columns={'CROSSLINE_3D': 'tmp16', 'CDP_Y': 'CROSSLINE_3D'})

        final_headers = trace_headers[columns_to_keep]

        del trace_headers

        sourcex = final_headers['SourceX'].to_numpy(copy=False).astype(np.float64)
        sourcey = final_headers['SourceY'].to_numpy(copy=False).astype(np.float64)
        INLINE_3D = final_headers['INLINE_3D'].to_numpy(copy=False)
        CROSSLINE_3D = final_headers['CROSSLINE_3D'].to_numpy(copy=False)

        del final_headers

        # Ensure unique values for INLINE_3D and CROSSLINE_3D using NumPy arrays
        unique_inline3d_values = np.unique(INLINE_3D)  # Sorted unique values
        unique_crossline3d_values = np.unique(CROSSLINE_3D)  # Sorted unique values

        # Get values as contiguous arrays (ensure int32 type)
        inline_values = INLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed
        crossline_values = CROSSLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed

        inline3d_index = {value: index for index, value in enumerate(unique_inline3d_values)}
        crossline3d_index = {value: index for index, value in enumerate(unique_crossline3d_values)}

        # Calculate dimensions of the 3D array
        inline3d_max = len(unique_inline3d_values)
        crossline3d_max = len(unique_crossline3d_values)

        i1, i2 = parsed_header_metadata.inline_range
        c1, c2 = parsed_header_metadata.xline_range

        if None not in (i1, i2, c1, c2):
            # Calculate fixed step sizes
            inline3d_step_size = unique_inline3d_values[1] - unique_inline3d_values[0]
            crossline3d_step_size = unique_crossline3d_values[1] - unique_crossline3d_values[0]
            num_unique_inline3d = (i2 - i1) // inline3d_step_size + 1
            num_unique_crossline3d = (c2 - c1) // crossline3d_step_size + 1
            if not inline3d_max == num_unique_inline3d:
                raise Exception("Invalid inline column.Please retry and choose the correct column manually.")
            elif not crossline3d_max == num_unique_crossline3d:
                raise Exception("Invalid crossline column.Please retry and choose the correct column manually.")

        del unique_inline3d_values, unique_crossline3d_values  # No longer needed

        if parsed_header_metadata.coord_scale_factor is None:
            scale = scale
        else:
            scale = parsed_header_metadata.coord_scale_factor

        x1, x2 = parsed_header_metadata.x_range[0], parsed_header_metadata.x_range[1]
        y1, y2 = parsed_header_metadata.y_range[0], parsed_header_metadata.y_range[1]

        # Compute existing min/max from arrays
        sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
        sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)

        if None in (x1, x2, y1, y2):
            # Apply predefined scale if we don't have reference min/max values
            sourcex = sourcex / scale
            sourcey = sourcey / scale
        else:
            # Check if scaling is needed
            scale_x = (x2 - x1) / (sourcex_max - sourcex_min) if sourcex_max != sourcex_min else 1.0
            scale_y = (y2 - y1) / (sourcey_max - sourcey_min) if sourcey_max != sourcey_min else 1.0

            # Apply scaling to align sourceX and sourceY
            sourcex = (sourcex - sourcex_min) * scale_x + x1
            sourcey = (sourcey - sourcey_min) * scale_y + y1

        # Find min and max values
        inline_min, inline_max = int(np.min(INLINE_3D)), int(np.max(INLINE_3D))
        crossline_min, crossline_max = int(np.min(CROSSLINE_3D)), int(np.max(CROSSLINE_3D))

        # Use least squares regression to compute corner coordinates
        # This is more robust than exact lookups
        INLINE_3D_f32 = INLINE_3D.astype(np.float32, copy=False)
        CROSSLINE_3D_f32 = CROSSLINE_3D.astype(np.float32, copy=False)

        # Build linear system: [INLINE, CROSSLINE, 1] * [a, b, c] = X (or Y)
        # This fits a plane: X = a*INLINE + b*CROSSLINE + c
        A = np.column_stack([INLINE_3D_f32, CROSSLINE_3D_f32, np.ones(len(INLINE_3D_f32), dtype=np.float32)])
        coeff_x, _, _, _ = np.linalg.lstsq(A, sourcex, rcond=None)
        coeff_y, _, _, _ = np.linalg.lstsq(A, sourcey, rcond=None)

        # Define corner coordinates in inline/crossline space
        corners = np.array([
            [inline_min, crossline_min],  # origin
            [inline_min, crossline_max],  # xline_end
            [inline_max, crossline_min]  # inline_end
        ], dtype=np.float32)

        # Compute X and Y coordinates for corners using the fitted coefficients
        corner_x = coeff_x[0] * corners[:, 0] + coeff_x[1] * corners[:, 1] + coeff_x[2]
        corner_y = coeff_y[0] * corners[:, 0] + coeff_y[1] * corners[:, 1] + coeff_y[2]

        # Extract individual corner tuples
        origin = (float(corner_x[0]), float(corner_y[0]))
        xline_end = (float(corner_x[1]), float(corner_y[1]))
        inline_end = (float(corner_x[2]), float(corner_y[2]))

        num_time_intervals = amplitudes.shape[1]

        # Initialize 3D array with zeros, setting dtype to float32
        seismic_cube = np.zeros((inline3d_max, crossline3d_max, num_time_intervals), dtype=np.float32)

        # Populate the 3D array based on unique values
        for idx in range(len(inline_values)):
            inline_idx = inline3d_index[inline_values[idx]]
            crossline_idx = crossline3d_index[crossline_values[idx]]
            seismic_cube[inline_idx, crossline_idx, :] = amplitudes[idx]

        del amplitudes, inline_values, crossline_values, inline3d_index, crossline3d_index  # No longer needed

        seismic_cube = np.expand_dims(seismic_cube, axis=-1)  # Adding channel dimension

        # Transpose the tensor to match the desired orientation
        seismic_cube = np.transpose(seismic_cube, (2, 0, 1, 3))

        if parsed_header_metadata.template is None:
            parsed_header_metadata.template = 'Seismic'

        if parsed_header_metadata.template == 'phase':
            parsed_header_metadata.template = 'Cos Phase'

        if any(x is None for x in parsed_header_metadata.inline_range):
            parsed_header_metadata.inline_range = (inline_min, inline_max)

        if any(x is None for x in parsed_header_metadata.xline_range):
            parsed_header_metadata.xline_range = (crossline_min, crossline_max)

        if any(x is None for x in parsed_header_metadata.x_range):
            sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
            parsed_header_metadata.x_range = (sourcex_min, sourcex_max, sourcex_max - sourcex_min)

        if any(x is None for x in parsed_header_metadata.y_range):
            sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)
            parsed_header_metadata.y_range = (sourcey_min, sourcey_max, sourcey_max - sourcey_min)

        if any(x is None for x in parsed_header_metadata.time_range):
            parsed_header_metadata.time_range = ((-(seismic_cube.shape[0] - 1) * sample_interval) - t_start_value,
                                                 -t_start_value, (seismic_cube.shape[0] - 1) * sample_interval)

        metadata_dict = {
            key: value for key, value in vars(parsed_header_metadata).items()
            if value is not None and (not isinstance(value, (tuple, list)) or all(elem is not None for elem in value))
        }

        # Metadata you want to save
        metadata = {
            'origin': origin,
            'xline_end': xline_end,
            'inline_end': inline_end,
            "sampling_interval_ms": sample_interval,
            "ebcdic_header_metadata": ebcdic_header_metadata
        }

        metadata.update(metadata_dict)

        # If the user cancels the dialog, save_path will be empty, so we check it
        if save_path:

            if compress:

                if compress == 'Compressed':
                    np.savez_compressed(save_path, tensor=seismic_cube, metadata=metadata)
                else:
                    np.savez(save_path, tensor=seismic_cube, metadata=metadata)

        return seismic_cube, metadata

    @staticmethod
    def seismic_loading_huge_task(INLINE_3D, CROSSLINE_3D, sourcex, sourcey, segy_file_path, scale, save_path,
                                  compress):

        # Convert coordinate arrays to float64 to handle division operations
        sourcex = sourcex.astype(np.float64, copy=False)
        sourcey = sourcey.astype(np.float64, copy=False)

        with segyio.open(segy_file_path, "r", ignore_geometry=True) as f:
            # Decode EBCDIC header if it's in byte format
            EBCDIC = f.text[0]
            # Extract time range (in milliseconds)
            time_samples = f.samples
            t_start_value = time_samples[0]
            # Extract amplitude data
            amplitudes = np.array([f.trace[i] for i in range(f.tracecount)])
            # Access the Sample Interval (in ms) from the Binary Header
            sample_interval = f.bin[segyio.BinField.Interval] / 1000

        ebcdic_header = EBCDIC.decode('ascii', errors='ignore')
        if not ebcdic_header.startswith('C'):
            ebcdic_header = EBCDIC.translate(bytearray(a2e)).decode('ascii')

        parsed_header_metadata, ebcdic_header_metadata = parse_ebcdic_header(ebcdic_header)

        # Ensure unique values for INLINE_3D and CROSSLINE_3D using NumPy arrays
        unique_inline3d_values = np.unique(INLINE_3D)  # Sorted unique values
        unique_crossline3d_values = np.unique(CROSSLINE_3D)  # Sorted unique values

        # Get values as contiguous arrays (ensure int32 type)
        inline_values = INLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed
        crossline_values = CROSSLINE_3D.astype(np.int32, copy=False)  # Use existing array, convert if needed

        inline3d_index = {value: index for index, value in enumerate(unique_inline3d_values)}
        crossline3d_index = {value: index for index, value in enumerate(unique_crossline3d_values)}

        # Calculate dimensions of the 3D array
        inline3d_max = len(unique_inline3d_values)
        crossline3d_max = len(unique_crossline3d_values)

        i1, i2 = parsed_header_metadata.inline_range
        c1, c2 = parsed_header_metadata.xline_range

        if None not in (i1, i2, c1, c2):
            # Calculate fixed step sizes
            inline3d_step_size = unique_inline3d_values[1] - unique_inline3d_values[0]
            crossline3d_step_size = unique_crossline3d_values[1] - unique_crossline3d_values[0]
            num_unique_inline3d = (i2 - i1) // inline3d_step_size + 1
            num_unique_crossline3d = (c2 - c1) // crossline3d_step_size + 1
            if not inline3d_max == num_unique_inline3d:
                raise Exception("Invalid inline column.Please retry and choose the correct column manually.")
            elif not crossline3d_max == num_unique_crossline3d:
                raise Exception("Invalid crossline column.Please retry and choose the correct column manually.")

        del unique_inline3d_values, unique_crossline3d_values  # No longer needed

        if parsed_header_metadata.coord_scale_factor is None:
            scale = scale
        else:
            scale = parsed_header_metadata.coord_scale_factor

        x1, x2 = parsed_header_metadata.x_range[0], parsed_header_metadata.x_range[1]
        y1, y2 = parsed_header_metadata.y_range[0], parsed_header_metadata.y_range[1]

        # Compute existing min/max from arrays
        sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
        sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)

        if None in (x1, x2, y1, y2):
            # Apply predefined scale if we don't have reference min/max values
            sourcex = sourcex / scale
            sourcey = sourcey / scale
        else:
            # Check if scaling is needed
            scale_x = (x2 - x1) / (sourcex_max - sourcex_min) if sourcex_max != sourcex_min else 1.0
            scale_y = (y2 - y1) / (sourcey_max - sourcey_min) if sourcey_max != sourcey_min else 1.0

            # Apply scaling to align sourceX and sourceY
            sourcex = (sourcex - sourcex_min) * scale_x + x1
            sourcey = (sourcey - sourcey_min) * scale_y + y1

        # Find min and max values
        inline_min, inline_max = int(np.min(INLINE_3D)), int(np.max(INLINE_3D))
        crossline_min, crossline_max = int(np.min(CROSSLINE_3D)), int(np.max(CROSSLINE_3D))

        # Use least squares regression to compute corner coordinates
        # This is more robust than exact lookups
        INLINE_3D_f32 = INLINE_3D.astype(np.float32, copy=False)
        CROSSLINE_3D_f32 = CROSSLINE_3D.astype(np.float32, copy=False)

        # Build linear system: [INLINE, CROSSLINE, 1] * [a, b, c] = X (or Y)
        # This fits a plane: X = a*INLINE + b*CROSSLINE + c
        A = np.column_stack([INLINE_3D_f32, CROSSLINE_3D_f32, np.ones(len(INLINE_3D_f32), dtype=np.float32)])
        coeff_x, _, _, _ = np.linalg.lstsq(A, sourcex, rcond=None)
        coeff_y, _, _, _ = np.linalg.lstsq(A, sourcey, rcond=None)

        # Define corner coordinates in inline/crossline space
        corners = np.array([
            [inline_min, crossline_min],  # origin
            [inline_min, crossline_max],  # xline_end
            [inline_max, crossline_min]  # inline_end
        ], dtype=np.float32)

        # Compute X and Y coordinates for corners using the fitted coefficients
        corner_x = coeff_x[0] * corners[:, 0] + coeff_x[1] * corners[:, 1] + coeff_x[2]
        corner_y = coeff_y[0] * corners[:, 0] + coeff_y[1] * corners[:, 1] + coeff_y[2]

        # Extract individual corner tuples
        origin = (float(corner_x[0]), float(corner_y[0]))
        xline_end = (float(corner_x[1]), float(corner_y[1]))
        inline_end = (float(corner_x[2]), float(corner_y[2]))

        num_time_intervals = amplitudes.shape[1]

        # Initialize 3D array with zeros, setting dtype to float32
        seismic_cube = np.zeros((inline3d_max, crossline3d_max, num_time_intervals), dtype=np.float32)

        # Populate the 3D array based on unique values
        for idx in range(len(inline_values)):
            inline_idx = inline3d_index[inline_values[idx]]
            crossline_idx = crossline3d_index[crossline_values[idx]]
            seismic_cube[inline_idx, crossline_idx, :] = amplitudes[idx]

        del amplitudes, inline_values, crossline_values, inline3d_index, crossline3d_index  # No longer needed

        seismic_cube = np.expand_dims(seismic_cube, axis=-1)  # Adding channel dimension

        # Transpose the tensor to match the desired orientation
        seismic_cube = np.transpose(seismic_cube, (2, 0, 1, 3))

        if parsed_header_metadata.template is None:
            parsed_header_metadata.template = 'Seismic'

        if parsed_header_metadata.template == 'phase':
            parsed_header_metadata.template = 'Cos Phase'

        if any(x is None for x in parsed_header_metadata.inline_range):
            parsed_header_metadata.inline_range = (inline_min, inline_max)

        if any(x is None for x in parsed_header_metadata.xline_range):
            parsed_header_metadata.xline_range = (crossline_min, crossline_max)

        if any(x is None for x in parsed_header_metadata.x_range):
            sourcex_min, sourcex_max = np.min(sourcex), np.max(sourcex)
            parsed_header_metadata.x_range = (sourcex_min, sourcex_max, sourcex_max - sourcex_min)

        if any(x is None for x in parsed_header_metadata.y_range):
            sourcey_min, sourcey_max = np.min(sourcey), np.max(sourcey)
            parsed_header_metadata.y_range = (sourcey_min, sourcey_max, sourcey_max - sourcey_min)

        if any(x is None for x in parsed_header_metadata.time_range):
            parsed_header_metadata.time_range = ((-(seismic_cube.shape[0] - 1) * sample_interval) - t_start_value,
                                                 -t_start_value, (seismic_cube.shape[0] - 1) * sample_interval)

        metadata_dict = {
            key: value for key, value in vars(parsed_header_metadata).items()
            if value is not None and (not isinstance(value, (tuple, list)) or all(elem is not None for elem in value))
        }

        # Metadata you want to save
        metadata = {
            'origin': origin,
            'xline_end': xline_end,
            'inline_end': inline_end,
            "sampling_interval_ms": sample_interval,
            "ebcdic_header_metadata": ebcdic_header_metadata
        }

        metadata.update(metadata_dict)

        # If the user cancels the dialog, save_path will be empty, so we check it
        if save_path:

            if compress:

                if compress == 'Compressed':
                    np.savez_compressed(save_path, tensor=seismic_cube, metadata=metadata)
                else:
                    np.savez(save_path, tensor=seismic_cube, metadata=metadata)

        return seismic_cube, metadata

    def process_seismic_data(self):

        dialog = QFileDialog()
        options = dialog.options()
        self.segy_file_path, _ = QFileDialog.getOpenFileName(self, 'Open SEGY File', ''
                                                             , 'SEGY Files (*.segy *.sgy);;All Files (*)',
                                                             options=options)

        # Check if a file was selected
        if not self.segy_file_path:
            return  # Return if no file was selected

        self.file_name = os.path.splitext(os.path.basename(self.segy_file_path))[0].strip()

        compress = None

        settings = QDialog(self)
        settings.setWindowTitle("Settings")
        layout = QVBoxLayout()

        # Header handling selection
        selection_label = QLabel("Header Handling:")
        layout.addWidget(selection_label)

        selection_box = QComboBox()
        selection_box.addItems(["Automatic", "Manual"])
        layout.addWidget(selection_box)

        # Scaling factor input
        input_field_label = QLabel("Default Coordinate Scaling Factor:")
        layout.addWidget(input_field_label)

        input_field = QLineEdit()
        input_field.setText('1')
        input_field.setValidator(QDoubleValidator())
        layout.addWidget(input_field)

        # OK button
        button = QPushButton("OK")
        button.clicked.connect(settings.accept)
        layout.addWidget(button)

        settings.setLayout(layout)
        settings.setMinimumWidth(300)
        settings.setWindowFlags(settings.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
        settings.adjustSize()

        if settings.exec() == QDialog.DialogCode.Accepted:
            filter_headers = selection_box.currentIndex() == 0  # Automatic -> True, Manual -> False
            scale = float(input_field.text())
            if scale.is_integer():
                scale = int(scale)
        else:
            return

        # Open a QFileDialog to select the save location and name
        save_path, _ = QFileDialog.getSaveFileName(
            self, "Save NPZ File", f"{self.file_name}.npz", "NPZ Files (*.npz)"
        )

        # If the user cancels the dialog, save_path will be empty, so we check it
        if save_path:

            dialog = QDialog(self)
            dialog.setWindowTitle("Save Options")
            layout = QVBoxLayout()

            label = QLabel("Select Save Option:")
            layout.addWidget(label)

            combo = QComboBox()
            combo.addItems(['Standard', 'Compressed'])
            layout.addWidget(combo)

            button = QPushButton("OK")
            button.clicked.connect(dialog.accept)
            layout.addWidget(button)

            dialog.setLayout(layout)
            dialog.setMinimumWidth(225)
            dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)
            dialog.adjustSize()

            if dialog.exec() == QDialog.DialogCode.Accepted:
                compress = combo.currentText()

        try:
            if not filter_headers:

                QApplication.setOverrideCursor(self.custom_cursor)

                potential_columns = [
                    'SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D', 'TRACE_SEQUENCE_FILE',
                    'CDP', 'EnergySourcePoint', 'FieldRecord', 'SourceEnergyDirectionExponent',
                    'CDP_X', 'CDP_Y', 'ShotPoint', 'TraceValueMeasurementUnit', 'TransductionConstantMantissa'
                ]

                with segyio.open(self.segy_file_path, "r", ignore_geometry=True) as f:
                    # access headers
                    header_keys = segyio.tracefield.keys
                    data = {k: f.attributes(header_keys[k])[:] for k in potential_columns if k in header_keys}

                trace_headers = pd.DataFrame(data)

                columns_to_keep = ['SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D']

                # Show the CSV viewer and allow user to select columns
                QApplication.restoreOverrideCursor()
                selected_columns = show_csv_in_dialog_segy(trace_headers.iloc[:10000], self)
                QApplication.setOverrideCursor(self.custom_cursor)

                if len(selected_columns) != 4:
                    QApplication.restoreOverrideCursor()
                    QMessageBox.critical(self, "Selection Error", "You must select exactly 4 columns.")
                    return None  # Or handle the error appropriately

                # Determine which of the chosen columns are SourceX, SourceY, INLINE_3D, CROSSLINE_3D
                chosen_columns = [
                    trace_headers.columns[selected_columns[0]],
                    trace_headers.columns[selected_columns[1]],
                    trace_headers.columns[selected_columns[2]],
                    trace_headers.columns[selected_columns[3]]
                ]

                # Identify if any of the chosen columns are already SourceX, SourceY, INLINE_3D, CROSSLINE_3D
                temp_mapping = {}
                for idx, col in enumerate(['SourceX', 'SourceY', 'INLINE_3D', 'CROSSLINE_3D']):
                    if col in trace_headers.columns and col not in chosen_columns:
                        temp_mapping[col] = f'TMP{idx + 1}'

                # Rename existing SourceX, SourceY, INLINE_3D, CROSSLINE_3D to temporary names if necessary
                if temp_mapping:
                    trace_headers.rename(columns=temp_mapping, inplace=True)

                # Map the selected columns to SourceX, SourceY, INLINE_3D, CROSSLINE_3D
                chosen_mapping = {
                    trace_headers.columns[selected_columns[0]]: 'SourceX',
                    trace_headers.columns[selected_columns[1]]: 'SourceY',
                    trace_headers.columns[selected_columns[2]]: 'INLINE_3D',
                    trace_headers.columns[selected_columns[3]]: 'CROSSLINE_3D'
                }

                # Show confirmation dialog
                QApplication.restoreOverrideCursor()
                result = show_confirmation_dialog(chosen_mapping, self)
                QApplication.setOverrideCursor(self.custom_cursor)
                if result != QDialog.DialogCode.Accepted:
                    QApplication.restoreOverrideCursor()
                    QMessageBox.information(self, "Cancelled", "Column mapping was cancelled.")
                    return None

                trace_headers.rename(columns=chosen_mapping, inplace=True)
                final_headers = trace_headers[columns_to_keep]

                del trace_headers

                sourcex = final_headers['SourceX'].to_numpy(copy=False)
                sourcey = final_headers['SourceY'].to_numpy(copy=False)
                INLINE_3D = final_headers['INLINE_3D'].to_numpy(copy=False)
                CROSSLINE_3D = final_headers['CROSSLINE_3D'].to_numpy(copy=False)

                del final_headers

                # Compute total sample count
                file_size_bytes = np.int64(os.path.getsize(self.segy_file_path))

                if file_size_bytes <= self.multiprocessing_threshold * 4:
                    result = TensorVisualizer.seismic_loading_task(INLINE_3D, CROSSLINE_3D,
                                                                   sourcex, sourcey, self.segy_file_path, scale,
                                                                   save_path, compress)
                    QApplication.restoreOverrideCursor()
                elif file_size_bytes >= self.multiprocessing_threshold * 40:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.seismic_loading_huge_task, INLINE_3D, CROSSLINE_3D,
                                                       sourcex, sourcey, self.segy_file_path, scale, save_path,
                                                       compress)
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.seismic_loading_task, INLINE_3D, CROSSLINE_3D,
                                                       sourcex, sourcey, self.segy_file_path, scale, save_path,
                                                       compress)
            else:
                # Compute total sample count
                file_size_bytes = np.int64(os.path.getsize(self.segy_file_path))

                if file_size_bytes <= self.multiprocessing_threshold * 4:
                    QApplication.setOverrideCursor(self.custom_cursor)
                    result = TensorVisualizer.seismic_loading_filter_task(self.segy_file_path,
                                                                          scale, save_path, compress)
                    QApplication.restoreOverrideCursor()
                elif file_size_bytes >= self.multiprocessing_threshold * 40:
                    result = self.task_runner.run_task(TensorVisualizer.seismic_loading_filter_huge_task,
                                                       self.segy_file_path,
                                                       scale, save_path, compress)
                else:
                    result = self.task_runner.run_task(TensorVisualizer.seismic_loading_filter_task,
                                                       self.segy_file_path,
                                                       scale, save_path, compress)

            if result is not None:

                transposed_tensor, metadata = result

                # If the user cancels the dialog, save_path will be empty, so we check it
                if save_path:

                    if compress:
                        self.loaded_file_paths[self.file_name] = save_path
                        self.update_recent_files_submenu()  # Update the recent files submenu

                self.tensor_data = transposed_tensor
                self.metadata[self.file_name] = metadata
                self.add_tensor(self.file_name, self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "An error occurred", f"Error: {e}")

    def tensor_cutter(self):
        if not isinstance(self.tensor_data, np.ndarray):
            return
        # Create a QDialog instance
        dialog = QDialog(self)
        dialog.setWindowTitle('Seismic Image Slicer')

        layout = QVBoxLayout(dialog)

        # Get the shape of the tensor data
        metadata = self.metadata[self.file_name]

        sampling_interval = metadata.get('sampling_interval_ms')
        time_max_slider_value = int((self.tensor_data.shape[0] - 1) * sampling_interval) + max(0, -(
            metadata.get('time_range', [0, 0, 0])[1]))
        time_min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
        time_step = sampling_interval

        inline_range = metadata.get('inline_range', (0, self.tensor_data.shape[1] - 1))

        xline_range = metadata.get('xline_range', (0, self.tensor_data.shape[2] - 1))

        inline_max_slider_value = inline_range[1]
        inline_min_slider_value = inline_range[0]
        inline_step = ((inline_range[1] - inline_range[0]) / (self.tensor_data.shape[1] - 1) if self.tensor_data.shape[
                                                                                                    1] > 1 else 1)

        xline_max_slider_value = xline_range[1]
        xline_min_slider_value = xline_range[0]
        xline_step = (
            (xline_range[1] - xline_range[0]) / (self.tensor_data.shape[2] - 1) if self.tensor_data.shape[2] > 1 else 1)

        # Depth slice (1st dimension)
        self.depth_min, self.depth_max = self.create_slider_and_spinbox('Time', layout, time_max_slider_value,
                                                                        time_min_slider_value, time_step)
        # Height slice (2nd dimension)
        self.height_min, self.height_max = self.create_slider_and_spinbox('Inline', layout, inline_max_slider_value,
                                                                          inline_min_slider_value, inline_step)
        # Width slice (3rd dimension)
        self.width_min, self.width_max = self.create_slider_and_spinbox('Crossline', layout, xline_max_slider_value,
                                                                        xline_min_slider_value, xline_step)

        # Cut Button
        cut_button = QPushButton('Cut', dialog)
        cut_button.clicked.connect(lambda: self.slice_tensor(dialog))
        layout.addWidget(cut_button)

        dialog.setLayout(layout)

        dialog.setWindowFlags(
            dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint | Qt.WindowType.WindowMinimizeButtonHint)
        dialog.exec()

    def create_slider_and_spinbox(self, label, layout, max_slider_value, min_slider_value, step):
        """ Helper method to create a label, a slider and a spinbox for each dimension. """
        hbox = QHBoxLayout()
        hbox.addWidget(QLabel(label))

        min_spinbox = StepSpinBox(self)
        min_spinbox.setMaximum(int(max_slider_value))
        min_spinbox.setMinimum(int(min_slider_value))
        min_spinbox.setSingleStep(int(step))
        min_spinbox.setValue(int(min_slider_value))

        max_spinbox = StepSpinBox(self)
        max_spinbox.setMaximum(int(max_slider_value))
        max_spinbox.setMinimum(int(min_slider_value))
        max_spinbox.setSingleStep(int(step))
        max_spinbox.setValue(int(max_slider_value))

        min_slider = StepSlider(self)
        # Set the maximum value of the slider
        min_slider.setMaximum(int(max_slider_value))
        min_slider.setMinimum(int(min_slider_value))
        min_slider.setSingleStep(int(step))
        min_slider.setValue(int(min_slider_value))
        min_slider.setOrientation(Qt.Orientation.Horizontal)

        max_slider = StepSlider(self)
        # Set the maximum value of the slider
        max_slider.setMaximum(int(max_slider_value))
        max_slider.setMinimum(int(min_slider_value))
        max_slider.setSingleStep(int(step))
        max_slider.setValue(int(max_slider_value))
        max_slider.setOrientation(Qt.Orientation.Horizontal)

        # Connect spinboxes and sliders
        min_spinbox.valueChanged.connect(min_slider.setValue)
        min_slider.valueChanged.connect(min_spinbox.setValue)
        max_spinbox.valueChanged.connect(max_slider.setValue)
        max_slider.valueChanged.connect(max_spinbox.setValue)

        hbox.addWidget(min_slider)
        hbox.addWidget(min_spinbox)
        hbox.addWidget(QLabel('to'))
        hbox.addWidget(max_spinbox)
        hbox.addWidget(max_slider)

        layout.addLayout(hbox)

        return min_spinbox, max_spinbox

    def slice_tensor(self, dialog):
        try:
            dialog.accept()
            QApplication.setOverrideCursor(self.custom_cursor)
            """ Perform the slicing of the tensor based on the selected ranges. """
            time_min = self.depth_min.value()
            time_max = self.depth_max.value()
            iline_min = self.height_min.value()
            iline_max = self.height_max.value()
            crossline_min = self.width_min.value()
            crossline_max = self.width_max.value()

            # Extract the metadata from the filename
            metadata = self.metadata[self.file_name]

            # Extract tensor dimensions
            n_inlines = self.tensor_data.shape[1]
            n_xlines = self.tensor_data.shape[2]

            sampling_interval = metadata.get('sampling_interval_ms')
            d_min = int(((time_min - max(0, -(metadata.get('time_range', [0, 0, 0])[1]))) / sampling_interval))
            d_max = int(((time_max - max(0, -(metadata.get('time_range', [0, 0, 0])[1]))) / sampling_interval) + 1)

            delta_time = time_max - time_min + sampling_interval

            # Retrieve original inline range
            inline_start, inline_end = metadata.get('inline_range', (0, self.tensor_data.shape[1] - 1))
            inline_step = ((inline_end - inline_start) / (n_inlines - 1) if n_inlines > 1 else 0) \
                if (inline_end - inline_start) != 0 else 1

            # Calculate new inline range
            h_min = int((iline_min - inline_start) / inline_step)
            h_max = int(((iline_max - inline_start) / inline_step) + 1)

            # Retrieve original crossline range
            crossline_start, crossline_end = metadata.get('xline_range', (0, self.tensor_data.shape[2] - 1))
            crossline_step = ((crossline_end - crossline_start) / (n_xlines - 1) if n_xlines > 1 else 0) \
                if (crossline_end - crossline_start) != 0 else 1

            # Calculate new crossline range
            w_min = int((crossline_min - crossline_start) / crossline_step)
            w_max = int(((crossline_max - crossline_start) / crossline_step) + 1)

            # Perform the slicing
            output = self.tensor_data[d_min:d_max, h_min:h_max, w_min:w_max, :]

            # create new metadata for the sliced tensor
            self.metadata[f"{self.file_name} {output.shape}"] = self.metadata[
                self.file_name].copy()
            self.metadata[f"{self.file_name} {output.shape}"][
                'name'] = f"{self.metadata[f'{self.file_name} {output.shape}'].get('name', '')} {output.shape}"
            self.metadata[f"{self.file_name} {output.shape}"]['time_range'] = (-time_max, -time_min, delta_time)
            self.metadata[f"{self.file_name} {output.shape}"]['inline_range'] = (iline_min, iline_max)
            self.metadata[f"{self.file_name} {output.shape}"]['xline_range'] = (crossline_min, crossline_max)

            # get the cordinates for the input data
            origin = metadata.get('origin', (0, 0))  # (x, y) for origin (inline=0, crossline=0)
            xline_end = metadata.get('xline_end', (self.tensor_data.shape[2] - 1, 0))
            inline_end = metadata.get('inline_end', (0, self.tensor_data.shape[1] - 1))

            # Calculate the new coordinates for the sliced tensor
            H = self.tensor_data.shape[1]  # Original number of inlines
            W = self.tensor_data.shape[2]  # Original number of crosslines

            # Convert coordinates to NumPy arrays for vector operations
            origin = np.array(origin, dtype=float)
            xline_end = np.array(xline_end, dtype=float)
            inline_end = np.array(inline_end, dtype=float)

            # Compute step vectors, handling cases where H or W is 1
            V_inline = (inline_end - origin) / (H - 1) if H > 1 else np.array([0, 0], dtype=float)
            V_crossline = (xline_end - origin) / (W - 1) if W > 1 else np.array([0, 0], dtype=float)

            # Compute new coordinates
            origin_new = origin + h_min * V_inline + w_min * V_crossline
            xline_end_new = origin + h_min * V_inline + (w_max - 1) * V_crossline
            inline_end_new = origin + (h_max - 1) * V_inline + w_min * V_crossline
            corner4 = origin + (h_max - 1) * V_inline + (w_max - 1) * V_crossline  # Fourth corner

            # Calculate x and y ranges from all four corners
            corners = [origin_new, xline_end_new, inline_end_new, corner4]
            x_coords = [corner[0] for corner in corners]
            y_coords = [corner[1] for corner in corners]
            min_x_coords, max_x_coords = min(x_coords), max(x_coords)
            x_range = (min_x_coords, max_x_coords, max_x_coords - min_x_coords)
            min_y_coords, max_y_coords = min(y_coords), max(y_coords)
            y_range = (min_y_coords, max_y_coords, max_y_coords - min_y_coords)

            # Convert back to tuples for consistency with metadata format
            origin_new = tuple(origin_new)
            xline_end_new = tuple(xline_end_new)
            inline_end_new = tuple(inline_end_new)

            # Metadata you want to save
            metadata = {
                "origin": origin_new,
                "xline_end": xline_end_new,
                "inline_end": inline_end_new,
                "x_range": x_range,
                "y_range": y_range
            }

            self.metadata[f"{self.file_name} {output.shape}"].update(metadata)

            self.tensor_data = output
            self.add_tensor(f"{self.file_name} {output.shape}", self.tensor_data)
            QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def rotate_tensor_ui(self):
        """
        This function sets up the UI for rotating a 4D tensor representing a 3D cube.
        It is assumed that `self.tensor_data` is a 4D NumPy array of shape (depth, height, width, channel).
        """
        try:
            if not isinstance(self.tensor_data, np.ndarray):
                return
            data = np.copy(self.tensor_data)

            def rotate_tensor(axis, direction, degrees, axis_rotation_label):
                """
                Rotates the tensor around the specified axis.

                Parameters:
                - axis: The axis to rotate around ('X', 'Y', or 'Z').
                - direction: The rotation direction ('Clockwise' or 'Counterclockwise').
                - degrees: The rotation in degrees (90, 180, or 270).
                """
                # Determine which axes to use for rotation
                if axis == 'Z':
                    axes = (1, 2)  # Rotate around z (height, width)
                elif axis == 'X':
                    axes = (0, 2)  # Rotate around x (depth, width)
                elif axis == 'Y':
                    axes = (1, 0)  # Rotate around y (depth, height)

                # Calculate the number of 90-degree rotations
                k = degrees // 90
                if direction == 'Counterclockwise':
                    axes = axes[::-1]

                # Perform the rotation using np.rot90
                self.tensor_data = np.rot90(data, k=k, axes=axes)

                # Create a meaningful key for the new rotated tensor
                new_key = f"{self.file_name}_Rotated {degrees}° {direction} around {axis_rotation_label}"

                # Create a copy of the original metadata for the new rotated tensor
                parent_meta = self.metadata[self.file_name]
                self.metadata[new_key] = parent_meta.copy()

                # Update the name in the metadata
                self.metadata[new_key][
                    'name'] = f"{parent_meta.get('name', '')}_Rotated {degrees}° {direction} around {axis_rotation_label}"

                # Get original metadata values
                inline_range = parent_meta['inline_range']
                xline_range = parent_meta['xline_range']
                time_range = parent_meta['time_range']
                x_range = parent_meta['x_range']
                y_range = parent_meta['y_range']
                origin = parent_meta['origin']
                inline_end = parent_meta['inline_end']
                xline_end = parent_meta['xline_end']

                # Handle metadata differently based on rotation axis
                if axis == 'Z':
                    # Z-axis rotation affects X-Y plane (inline and xline)
                    if k == 1:  # 90° clockwise
                        # Swap inline and xline ranges
                        self.metadata[new_key]['inline_range'] = xline_range
                        self.metadata[new_key]['xline_range'] = inline_range
                        # Swap x and y ranges
                        self.metadata[new_key]['x_range'] = y_range
                        self.metadata[new_key]['y_range'] = x_range
                        # Adjust origin and ends
                        self.metadata[new_key]['origin'] = (origin[1], origin[0])
                        self.metadata[new_key]['inline_end'] = xline_end
                        self.metadata[new_key]['xline_end'] = inline_end

                    elif k == 2:  # 180° clockwise
                        # Ranges stay the same but coordinates are inverted
                        self.metadata[new_key]['origin'] = (origin[0], origin[1])
                        self.metadata[new_key]['inline_end'] = inline_end
                        self.metadata[new_key]['xline_end'] = xline_end

                    elif k == 3:  # 270° clockwise
                        # Swap inline and xline ranges (similar to 90° but different orientation)
                        self.metadata[new_key]['inline_range'] = xline_range
                        self.metadata[new_key]['xline_range'] = inline_range
                        # Swap x and y ranges
                        self.metadata[new_key]['x_range'] = y_range
                        self.metadata[new_key]['y_range'] = x_range
                        # Adjust origin and ends
                        self.metadata[new_key]['origin'] = (origin[1], origin[0])
                        self.metadata[new_key]['inline_end'] = xline_end
                        self.metadata[new_key]['xline_end'] = inline_end

                elif axis == 'X':
                    # X-axis rotation affects Y-Z plane (inline and time)
                    if k == 1:  # 90° clockwise
                        # Time becomes inline, inline becomes negative time
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['inline_range'] = (abs(t_max), abs(t_min))
                        i_min, i_max = inline_range
                        self.metadata[new_key]['time_range'] = (-i_max, -i_min, i_max - i_min)
                        # Y range becomes time range values and vice versa
                        self.metadata[new_key]['y_range'] = (abs(t_max), abs(t_min), abs(t_max) - abs(t_min))

                    elif k == 2:  # 180° clockwise
                        # Ranges invert but stay in the same dimension
                        # For time range, we keep the same structure but negate the values
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['time_range'] = (-t_min, -t_max, t_delta)

                    elif k == 3:  # 270° clockwise
                        # Similar to 90° but in reverse direction
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['inline_range'] = (abs(t_min), abs(t_max))
                        i_min, i_max = inline_range
                        self.metadata[new_key]['time_range'] = (-i_min, -i_max, i_max - i_min)
                        # Y range becomes time range values and vice versa
                        self.metadata[new_key]['y_range'] = (abs(t_min), abs(t_max), abs(t_max) - abs(t_min))

                elif axis == 'Y':
                    # Y-axis rotation affects X-Z plane (xline and time)
                    if k == 1:  # 90° clockwise
                        # Time becomes xline, xline becomes negative time
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['xline_range'] = (abs(t_max), abs(t_min))
                        x_min, x_max = xline_range
                        self.metadata[new_key]['time_range'] = (-x_max, -x_min, x_max - x_min)
                        # X range becomes time range values and vice versa
                        self.metadata[new_key]['x_range'] = (abs(t_max), abs(t_min), abs(t_max) - abs(t_min))

                    elif k == 2:  # 180° clockwise
                        # Ranges invert but stay in the same dimension
                        # For time range, we keep the same structure but negate the values
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['time_range'] = (-t_min, -t_max, t_delta)

                    elif k == 3:  # 270° clockwise
                        # Similar to 90° but in reverse direction
                        t_min, t_max, t_delta = time_range
                        self.metadata[new_key]['xline_range'] = (abs(t_min), abs(t_max))
                        x_min, x_max = xline_range
                        self.metadata[new_key]['time_range'] = (-x_min, -x_max, x_max - x_min)
                        # X range becomes time range values and vice versa
                        self.metadata[new_key]['x_range'] = (abs(t_min), abs(t_max), abs(t_max) - abs(t_min))

                # Add rotated tensor to the collection
                self.add_tensor(new_key, self.tensor_data)

            def transpose_tensor(dim1, dim2, dim3):
                """
                Transposes the tensor according to the specified dimension order.
                Parameters:
                - dim1, dim2, dim3: Indices representing the new order of the first three dimensions.
                """
                # Map indices to dimension names for better readability in metadata
                index_to_name = {
                    0: 'X',
                    1: 'Y',
                    2: 'Z'
                }

                # Get dimension names for metadata
                dim1_name = index_to_name[dim1]
                dim2_name = index_to_name[dim2]
                dim3_name = index_to_name[dim3]

                # Create permutation list: first 3 dimensions in new order + channel dimension (always last)
                permutation = [dim1, dim2, dim3, 3]

                # Perform the transposition
                self.tensor_data = np.transpose(data, permutation)

                # Create meaningful key using dimension names (rather than just numbers)
                new_order_key = f"{self.file_name}_Transposed_{dim1_name}_{dim2_name}_{dim3_name}"

                # Update metadata with meaningful dimension order
                parent_meta = self.metadata[self.file_name]
                self.metadata[new_order_key] = parent_meta.copy()

                # Determine original dimensions for new dim0, dim1, dim2
                original_dims = [dim1, dim2, dim3]

                # Process inline_range
                original_dim0 = original_dims[0]
                if original_dim0 == 0:
                    inline_range_val = parent_meta['inline_range']
                elif original_dim0 == 1:
                    inline_range_val = parent_meta['xline_range']
                elif original_dim0 == 2:
                    t_min, t_max, t_delta = parent_meta['time_range']
                    inline_range_val = (abs(t_max), abs(t_min))

                # Process xline_range
                original_dim1 = original_dims[1]
                if original_dim1 == 0:
                    xline_range_val = parent_meta['inline_range']
                elif original_dim1 == 1:
                    xline_range_val = parent_meta['xline_range']
                elif original_dim1 == 2:
                    t_min, t_max, t_delta = parent_meta['time_range']
                    xline_range_val = (abs(t_max), abs(t_min))

                # Process time_range
                original_dim2 = original_dims[2]
                if original_dim2 == 0:
                    i_min, i_max = parent_meta['inline_range']
                    time_range_val = (-i_max, -i_min, i_max - i_min)
                elif original_dim2 == 1:
                    x_min, x_max = parent_meta['xline_range']
                    time_range_val = (-x_max, -x_min, x_max - x_min)
                elif original_dim2 == 2:
                    time_range_val = parent_meta['time_range']

                # Process x_range and y_range
                if original_dim0 == 0:
                    x_range_val = parent_meta['x_range']
                elif original_dim0 == 1:
                    x_range_val = parent_meta['y_range']
                elif original_dim0 == 2:
                    t_min, t_max, t_delta = parent_meta['time_range']
                    x_range_val = (abs(t_max), abs(t_min), abs(t_max) - abs(t_min))

                if original_dim1 == 0:
                    y_range_val = parent_meta['x_range']
                elif original_dim1 == 1:
                    y_range_val = parent_meta['y_range']
                elif original_dim1 == 2:
                    t_min, t_max, t_delta = parent_meta['time_range']
                    y_range_val = (abs(t_max), abs(t_min), abs(t_max) - abs(t_min))

                # Process origin, inline_end, xline_end
                swap_xy = (original_dims[0] == 1 and original_dims[1] == 0)
                if swap_xy:
                    origin_val = (parent_meta['origin'][1], parent_meta['origin'][0])
                    inline_end_val = parent_meta['xline_end']
                    xline_end_val = parent_meta['inline_end']
                else:
                    origin_val = parent_meta['origin']
                    inline_end_val = parent_meta['inline_end']
                    xline_end_val = parent_meta['xline_end']

                # Update metadata entries
                self.metadata[new_order_key]['inline_range'] = inline_range_val
                self.metadata[new_order_key]['xline_range'] = xline_range_val
                self.metadata[new_order_key]['x_range'] = x_range_val
                self.metadata[new_order_key]['y_range'] = y_range_val
                self.metadata[new_order_key]['time_range'] = time_range_val
                self.metadata[new_order_key]['origin'] = origin_val
                self.metadata[new_order_key]['inline_end'] = inline_end_val
                self.metadata[new_order_key]['xline_end'] = xline_end_val

                # Update the name to reflect the transposition
                self.metadata[new_order_key][
                    'name'] = f"{parent_meta.get('name', '')}_Transposed_{dim1_name}_{dim2_name}_{dim3_name}"

                # Add transposed tensor to the collection
                self.add_tensor(new_order_key, self.tensor_data)

            # Initialize the Qt application
            window = QDialog(self)
            layout = QVBoxLayout()

            # Group for Transposition
            transpose_group = QGroupBox("Transposition", self)
            transpose_layout = QVBoxLayout()

            transpose_label = QLabel("Reorder the dimensions of your image:")
            transpose_layout.addWidget(transpose_label)

            # Add info about current shape
            shape_info = QLabel(f"Current image shape: {self.tensor_data.shape}")
            transpose_layout.addWidget(shape_info)

            # Create dimension selection combo boxes
            combo_layout = QHBoxLayout()

            # First dimension combo
            dim1_combo = QComboBox()
            dim1_combo.addItems(['X', 'Y', 'Z'])
            dim1_combo.setCurrentIndex(0)  # Default: Depth
            combo_layout.addWidget(QLabel("Time:"))
            combo_layout.addWidget(dim1_combo)

            # Second dimension combo
            dim2_combo = QComboBox()
            dim2_combo.addItems(['X', 'Y', 'Z'])
            dim2_combo.setCurrentIndex(1)  # Default: Height
            combo_layout.addWidget(QLabel("InLine:"))
            combo_layout.addWidget(dim2_combo)

            # Third dimension combo
            dim3_combo = QComboBox()
            dim3_combo.addItems(['X', 'Y', 'Z'])
            dim3_combo.setCurrentIndex(2)  # Default: Width
            combo_layout.addWidget(QLabel("CrossLine:"))
            combo_layout.addWidget(dim3_combo)

            transpose_layout.addLayout(combo_layout)

            # Validation label (hidden initially)
            validation_label = QLabel("")
            validation_label.setStyleSheet("color: red;")
            validation_label.setVisible(False)
            transpose_layout.addWidget(validation_label)

            # Apply transpose button
            apply_transpose_button = QPushButton("Apply Transposition")
            transpose_layout.addWidget(apply_transpose_button)

            transpose_group.setLayout(transpose_layout)
            layout.addWidget(transpose_group)

            # Group for Rotation
            rotation_group = QGroupBox("Rotation", self)
            rotation_layout = QVBoxLayout()

            # Axis selection dropdown
            axis_label = QLabel("Select Axis (InLine, CrossLine, Time):")
            axis_combo = QComboBox()
            axis_combo.addItem('InLine')
            axis_combo.addItem('CrossLine')
            axis_combo.addItem('Time')
            rotation_layout.addWidget(axis_label)
            rotation_layout.addWidget(axis_combo)

            # Direction selection dropdown
            direction_label = QLabel("Select Rotation Direction:")
            direction_combo = QComboBox()
            direction_combo.addItem('Clockwise')
            direction_combo.addItem('Counterclockwise')
            rotation_layout.addWidget(direction_label)
            rotation_layout.addWidget(direction_combo)

            # Rotation degree input with slider
            degrees_label = QLabel("Select Degrees of Rotation:")
            degrees_edit = QLineEdit()
            degrees_slider = QSlider(Qt.Orientation.Horizontal)
            degrees_slider.setRange(1, 3)  # Slider range represents positions 1, 2, 3
            rotation_layout.addWidget(degrees_label)

            # Mapping of slider positions to degrees
            slider_to_degrees = {1: 90, 2: 180, 3: 270}

            def update_edit_from_slider(value):
                degrees_edit.setText(str(slider_to_degrees[value]))

            def validate_and_update_edit():
                try:
                    value = int(degrees_edit.text())
                except ValueError:
                    value = 90  # Default to 90 if input is not a valid number

                # Find the closest valid degree (90, 180, 270)
                closest_value = min(slider_to_degrees.values(), key=lambda x: abs(x - value))
                degrees_edit.setText(str(closest_value))

                # Update the slider position based on the closest valid degree
                for k, v in slider_to_degrees.items():
                    if v == closest_value:
                        degrees_slider.setValue(k)
                        break

            degrees_slider.valueChanged.connect(update_edit_from_slider)
            degrees_edit.editingFinished.connect(validate_and_update_edit)

            # Initialize with 90 degrees
            degrees_slider.setValue(1)
            degrees_edit.setText('90')

            slider_layout = QHBoxLayout()
            slider_layout.addWidget(degrees_edit)
            slider_layout.addWidget(degrees_slider)
            rotation_layout.addLayout(slider_layout)

            # Apply rotation button
            apply_rotation_button = QPushButton("Apply Rotation")
            rotation_layout.addWidget(apply_rotation_button)

            rotation_group.setLayout(rotation_layout)
            layout.addWidget(rotation_group)

            # Map user-friendly dimension names to axis indices
            dim_map = {
                'X': 0,
                'Y': 1,
                'Z': 2
            }

            # Define what happens when the apply transpose button is clicked
            def apply_transposition():
                QApplication.setOverrideCursor(self.custom_cursor)
                dim1 = dim_map[dim1_combo.currentText()]
                dim2 = dim_map[dim2_combo.currentText()]
                dim3 = dim_map[dim3_combo.currentText()]

                # Check for duplicates
                dims = [dim1, dim2, dim3]
                if len(set(dims)) != 3:
                    validation_label.setText("Error: Each dimension must be selected exactly once")
                    validation_label.setVisible(True)
                    QApplication.restoreOverrideCursor()
                    return

                # If valid, perform transposition
                validation_label.setVisible(False)
                transpose_tensor(dim1, dim2, dim3)
                QApplication.restoreOverrideCursor()
                window.accept()

            apply_transpose_button.clicked.connect(apply_transposition)

            dim_rotation_map = {
                'InLine': 'X',
                'CrossLine': 'Y',
                'Time': 'Z'
            }

            # Define what happens when the apply button is clicked
            def apply_rotation():
                QApplication.setOverrideCursor(self.custom_cursor)
                axis = dim_rotation_map[axis_combo.currentText()]
                direction = direction_combo.currentText()
                degrees = int(degrees_edit.text())

                # Perform the rotation
                rotate_tensor(axis, direction, degrees, axis_combo.currentText())
                QApplication.restoreOverrideCursor()
                window.accept()

            apply_rotation_button.clicked.connect(apply_rotation)

            # Set up the dialog
            window.setLayout(layout)
            window.setWindowTitle("Rotation Operations")
            window.setWindowFlags(
                window.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint | Qt.WindowType.WindowMinimizeButtonHint)
            window.exec()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def connected_labels_gpu(binary_volume):
        binary_volume = cp.array(binary_volume)
        # Create 26-connectivity structure element for GPU
        structure = cp.ones((3, 3, 3), dtype=cp.int16)
        labeled, num_features = gpu_ndimage.label(binary_volume, structure=structure)
        sizes = cp.bincount(labeled.ravel())[1:]
        return labeled.get(), num_features, sizes.get()

    @staticmethod
    def connected_labels_cpu(binary_volume):
        # Create 26-connectivity structure element for CPU
        structure = np.ones((3, 3, 3), dtype=np.int16)
        labeled, num_features = ndimage.label(binary_volume, structure=structure)
        sizes = np.bincount(labeled.ravel())[1:]
        return labeled, num_features, sizes

    @staticmethod
    def compute_component_connectivity_gpu(binary_volume, labeled, start_component, end_component, sizes):

        binary_volume = cp.array(binary_volume)
        labeled = cp.array(labeled)
        sizes = cp.array(sizes)

        original_shape = binary_volume.shape + (1,)

        # Compute component connectivity (volume-weighted significance)
        component_connectivity = gpu_ndimage.sum(
            binary_volume,
            labeled,
            cp.arange(1, len(sizes) + 1)
        )

        # Compute composite score: size * connectivity
        component_scores = sizes * component_connectivity

        # Select components within the specified range
        sorted_indices = cp.argsort(component_scores)[::-1] + 1
        top_component_indices = sorted_indices[start_component - 1:end_component]

        # Create mask for top components
        fault_plane = cp.isin(labeled, top_component_indices)

        # Step 2: Process slices orthogonal to the dominant direction
        z_projection = cp.sum(fault_plane, axis=(1, 2))  # Sum along x and y axes
        x_projection = cp.sum(fault_plane, axis=(0, 2))  # Sum along y and z axes
        y_projection = cp.sum(fault_plane, axis=(0, 1))  # Sum along x and z axes

        # Find the dominant axis based on the maximum projection sum
        dominant_axis = cp.argmax(cp.array([z_projection.sum(), x_projection.sum(), y_projection.sum()]))

        # Return the significant components, maintaining the original volume shape
        if dominant_axis == 0:
            result = fault_plane
        elif dominant_axis == 1:
            result = cp.transpose(fault_plane, (1, 0, 2))
        else:  # dominant_axis == 2
            result = cp.transpose(fault_plane, (2, 0, 1))

        return cp.asnumpy(cp.reshape(result, original_shape).astype(cp.int16))

    @staticmethod
    def compute_component_connectivity_cpu(binary_volume, labeled, start_component, end_component, sizes):
        original_shape = binary_volume.shape + (1,)

        # Compute component connectivity (volume-weighted significance)
        component_connectivity = ndimage.sum(
            binary_volume,
            labeled,
            range(1, len(sizes) + 1)
        )

        # Compute composite score: size * connectivity
        component_scores = sizes * component_connectivity

        # Select components within the specified range
        sorted_indices = np.argsort(component_scores)[::-1] + 1
        top_component_indices = sorted_indices[start_component - 1:end_component]

        # Create mask for top components
        fault_plane = np.isin(labeled, top_component_indices)

        # Step 2: Process slices orthogonal to the dominant direction
        z_projection = np.sum(fault_plane, axis=(1, 2))  # Sum along x and y axes
        x_projection = np.sum(fault_plane, axis=(0, 2))  # Sum along y and z axes
        y_projection = np.sum(fault_plane, axis=(0, 1))  # Sum along x and z axes

        # Find the dominant axis based on the maximum projection sum
        dominant_axis = np.argmax([z_projection.sum(), x_projection.sum(), y_projection.sum()])

        # Return the significant components, maintaining the original volume shape
        if dominant_axis == 0:
            result = fault_plane
        elif dominant_axis == 1:
            result = np.transpose(fault_plane, (1, 0, 2))
        else:  # dominant_axis == 2
            result = np.transpose(fault_plane, (2, 0, 1))

        return np.reshape(result, original_shape).astype(np.int16)

    def connected_component_analysis_filtering(self):
        """
            Refined reconstruction of fault planes using advanced connected component analysis.

            Identifies top significant components based on size and connectivity,
            then determines the dominant plane orientation.

            Parameters:
                binary_volume (np.ndarray): 3D binary array (1s = fault points).

            Returns:
                np.ndarray: Reconstructed binary volume with significant components.
            """
        try:
            # Step 1: Label connected components and analyze their characteristics
            if not isinstance(self.tensor_data, np.ndarray):
                return

            # Assuming self.tensor_data is your 4D tensor with shape (depth, height, width, channels)
            original_shape = self.tensor_data.shape

            # Check if the last dimension (channel dimension) has more than one channel
            if original_shape[-1] > 1:
                # Create an integer input dialog to select the channel
                channel, ok = QInputDialog.getInt(
                    self, "Select Channel",
                    f"Enter the channel (1 to {original_shape[-1]}):",
                    min=1, max=original_shape[-1]
                )

                # Check if the user clicked OK
                if not ok:
                    return

                # Adjust the channel index (subtract 1 because QInputDialog returns 1-based index)
                channel -= 1
            else:
                # If there's only one channel, default to that single channel
                channel = 0

            QApplication.setOverrideCursor(self.custom_cursor)

            # Extract the 3D tensor for the chosen channel
            binary_volume = np.squeeze(self.tensor_data[..., channel].astype(np.int32))

            gpu = False

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = binary_volume.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 2:
                    cp.get_default_memory_pool().free_all_blocks()

                    gpu = False
                else:
                    gpu = True

            if gpu:
                # Compute total sample count
                num_samples = np.prod(binary_volume.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.connected_labels_gpu(binary_volume)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.connected_labels_gpu,
                                                       binary_volume)
            else:
                # Compute total sample count
                num_samples = np.prod(binary_volume.shape, dtype=np.int64)

                if num_samples <= self.multiprocessing_threshold:
                    result = TensorVisualizer.connected_labels_cpu(binary_volume)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.connected_labels_cpu, binary_volume)

            if result is not None:
                labeled, num_features, sizes = result

                QApplication.restoreOverrideCursor()

                # Use the custom dialog
                dialog = RangeDialog(self, num_features)
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    start_component, end_component = dialog.get_values()
                else:
                    return

                QApplication.setOverrideCursor(self.custom_cursor)

                gpu = False

                if gpu_available:
                    # Check available GPU memory before allocating
                    free_memory, total_memory = cp.cuda.Device().mem_info
                    required_memory = binary_volume.nbytes + labeled.nbytes

                    # Safety margin (e.g., 80% of free memory)
                    safety_margin = 0.8
                    if required_memory > free_memory * safety_margin / 4:
                        cp.get_default_memory_pool().free_all_blocks()

                        gpu = False
                    else:
                        gpu = True

                if gpu:
                    # Compute total sample count
                    num_samples = np.prod(binary_volume.shape, dtype=np.int64)

                    if num_samples <= self.multiprocessing_threshold:
                        result = TensorVisualizer.compute_component_connectivity_gpu(binary_volume, labeled,
                                                                                     start_component, end_component,
                                                                                     sizes)
                        QApplication.restoreOverrideCursor()
                    else:
                        QApplication.restoreOverrideCursor()
                        result = self.task_runner.run_task(TensorVisualizer.compute_component_connectivity_gpu,
                                                           binary_volume, labeled, start_component, end_component,
                                                           sizes)
                else:
                    # Compute total sample count
                    num_samples = np.prod(binary_volume.shape, dtype=np.int64)

                    if num_samples <= self.multiprocessing_threshold:
                        result = TensorVisualizer.compute_component_connectivity_cpu(binary_volume, labeled,
                                                                                     start_component, end_component,
                                                                                     sizes)
                        QApplication.restoreOverrideCursor()
                    else:
                        QApplication.restoreOverrideCursor()
                        result = self.task_runner.run_task(TensorVisualizer.compute_component_connectivity_cpu,
                                                           binary_volume, labeled, start_component, end_component,
                                                           sizes)

                if result is not None:
                    self.tensor_data = result
                    self.metadata[
                        f"{self.file_name}_Connected Component {self.metadata[self.file_name]['template']} ({start_component},{end_component})"] = \
                        self.metadata[
                            self.file_name].copy()
                    self.metadata[
                        f"{self.file_name}_Connected Component {self.metadata[self.file_name]['template']} ({start_component},{end_component})"][
                        'template'] = f"Connected Component {self.metadata[self.file_name]['template']} ({start_component},{end_component})"
                    self.add_tensor(
                        f"{self.file_name}_Connected Component {self.metadata[self.file_name]['template']} ({start_component},{end_component})",
                        self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def fault_prediction(self):
        if not isinstance(self.tensor_data, np.ndarray):
            return

        batch_size = 6
        depth = 128
        height = 128
        width = 128
        input_data = self.tensor_data.astype(np.float32)  # The data you need to pass to the process

        # Function to open a dialog and return the selected model path
        def get_model_path():
            dialog = QFileDialog(self)
            dialog.setFileMode(QFileDialog.FileMode.ExistingFile)
            dialog.setNameFilter("*.h5 *.pth")
            dialog.setWindowTitle("Select Geobody Segmentation Model")
            if dialog.exec():
                model_path = dialog.selectedFiles()
                return model_path[0]  # Return the first selected file
            return None  # Return None if no file is selected

        # Use the function to get the model path
        model_path = get_model_path()
        if model_path:
            # Check the file extension and load the model accordingly
            if model_path.endswith('.h5'):

                batch_size, ok = QInputDialog.getInt(
                    self, "Batch Size",
                    f"Enter the batch size:",
                    value=6, min=1, max=100
                )
                if not ok:
                    return

            elif model_path.endswith('.pth'):

                # Step 2: Get the depth, height, and width from the user using styled input dialogs
                dialog = SizeInputDialog(self)
                if dialog.exec():
                    depth, height, width = dialog.getValues()
                else:
                    return

            result = self.task_runner.run_task(TensorVisualizer.run_inference, input_data, model_path,
                                               batch_size, depth, height, width)

            if result is not None:

                # Determine the Otsu threshold
                threshold = threshold_otsu(result)

                QApplication.restoreOverrideCursor()

                # Ask the user to modify the threshold percentage
                threshold_percentage, ok = QInputDialog.getInt(
                    self, "Input Threshold",
                    f"Otsu Threshold is {threshold:.2f} (scaled to {threshold * 100:.1f}%).\n"
                    "Enter the threshold percentage (0-100):",
                    value=int(threshold * 100), min=0, max=100)

                if not ok:
                    return  # User canceled the input

                # Convert the percentage back to a threshold value
                threshold_value = threshold_percentage / 100

                # Apply the threshold to create a binary mask
                binary_masks = (result > threshold_value)

                self.tensor_data = binary_masks.astype(np.int16)
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Geobodies"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Geobodies"][
                    'template'] = "Geobodies"
                self.add_tensor(f"{self.file_name}_Geobodies", self.tensor_data)

    @staticmethod
    def run_inference(data, model_path, batch_size, depth, height, width):

        # Set up logging
        TensorVisualizer.setup_logging('Geobodies_Inference')

        # Function to pad the image to the target size
        def pad_image(image, target_height, target_width):
            height, width = image.shape[:2]  # Ensure image has at least 2 dimensions
            channels = 1 if len(image.shape) == 2 else image.shape[2]
            pad_height = max(target_height - height, 0)
            pad_width = max(target_width - width, 0)

            pad_top = pad_height // 2
            pad_bottom = pad_height - pad_top
            pad_left = pad_width // 2
            pad_right = pad_width - pad_left

            padded_image = cv2.copyMakeBorder(image, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_REFLECT)
            if channels == 1:
                padded_image = np.expand_dims(padded_image, axis=-1)
            return padded_image, pad_top, pad_left

        # Function to generate 2D sliding windows
        def generate_windows_2d(data, window_size, stride):
            height, width = data.shape[:2]  # Ensure data has at least 2 dimensions

            # Calculate end points to avoid repeated calculations
            max_h = height - window_size[0]
            max_w = width - window_size[1]

            # Regular grid points - main loop
            h_positions = list(range(0, max_h + 1, stride[0]))
            w_positions = list(range(0, max_w + 1, stride[1]))

            # Make sure we include the right edge if it's not in the regular grid
            if max_h % stride[0] != 0 and max_h not in h_positions:
                h_positions.append(max_h)
            if max_w % stride[1] != 0 and max_w not in w_positions:
                w_positions.append(max_w)

            # Generate all windows in a more efficient loop
            for h in h_positions:
                for w in w_positions:
                    yield h, w, data[h:h + window_size[0], w:w + window_size[1], :]

        # Function to create a 2D Gaussian window
        def create_gaussian_window(height, width, sigma=0.3):
            # Create 1D Gaussian distributions
            h_window = gaussian(height, std=(height - 1) * sigma / 2)[:, np.newaxis]
            w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, :]

            # Multiply to get 2D Gaussian window
            window = h_window * w_window

            # Add channel dimension to match image format
            return window[:, :, np.newaxis]

        # Function to predict using sliding windows and handle smaller images with Gaussian weighting
        def predict_with_sliding_window_2d(model, data, window_size, stride, sigma=0.3):
            if len(data.shape) == 2:
                data = np.expand_dims(data, axis=-1)  # Add channel dimension if not present

            height, width, channels = data.shape
            padded_data, pad_top, pad_left = pad_image(data, window_size[0], window_size[1])

            # Initialize arrays to accumulate output and weights
            output_data = np.zeros(padded_data.shape)
            weight_sum = np.zeros(padded_data.shape)

            # Create Gaussian window weights
            window_weights = create_gaussian_window(window_size[0], window_size[1], sigma)

            for h, w, window in generate_windows_2d(padded_data, window_size, stride):
                window = np.expand_dims(window, axis=0)  # Add batch dimension
                output = model.predict(window, verbose=0)
                output = np.squeeze(output, axis=0)  # Remove batch dimension

                # Apply Gaussian window weights to the output
                weighted_output = output * window_weights

                # Accumulate weighted output and weights
                output_data[h:h + window_size[0], w:w + window_size[1], :] += weighted_output
                weight_sum[h:h + window_size[0], w:w + window_size[1], :] += window_weights

            # Normalize by weight sum - use where to avoid division by zero
            mask = weight_sum > 0
            output_data[mask] /= weight_sum[mask]

            # Remove padding if it was added
            if pad_top > 0 or pad_left > 0:
                output_data = output_data[pad_top:pad_top + height, pad_left:pad_left + width, :]

            return output_data

        if model_path:

            if model_path.endswith('.h5'):
                model = load_model(model_path, compile=False)
                input_shape = model.input_shape

                if len(input_shape) == 5:  # For 3D CNNs
                    _, target_depth, target_height, target_width, _ = input_shape

                    if target_depth is None or target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target depth, height, and width.")

                    # Normalize data once
                    X_train = data
                    mean_X = np.mean(X_train)
                    std_X = np.std(X_train)
                    X_train_normalized = (X_train - mean_X) / std_X

                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Handle case where input dimensions are smaller than target dimensions
                    need_padding = False
                    pad_depth, pad_height, pad_width = 0, 0, 0

                    if depth_size < target_depth:
                        pad_depth = target_depth - depth_size
                        need_padding = True

                    if height_size < target_height:
                        pad_height = target_height - height_size
                        need_padding = True

                    if width_size < target_width:
                        pad_width = target_width - width_size
                        need_padding = True

                    if need_padding:
                        # Calculate padding for each dimension (distribute evenly on both sides)
                        pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                        pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                        pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                        # Apply padding with reflection to avoid edge artifacts
                        X_train_normalized = np.pad(
                            X_train_normalized,
                            ((pad_depth_before, pad_depth_after),
                             (pad_height_before, pad_height_after),
                             (pad_width_before, pad_width_after),
                             (0, 0)),
                            mode='reflect'
                        )

                        # Update dimensions after padding
                        depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Initialize arrays to accumulate output and weights
                    output_data = np.zeros((depth_size, height_size, width_size, channels))
                    weight_sum = np.zeros((depth_size, height_size, width_size, channels))

                    # Create gaussian window only once - more efficient implementation
                    def create_gaussian_window(depth, height, width, sigma=0.3):
                        # Compute the 3D Gaussian window
                        d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                        h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                        w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                        window = d_window * h_window * w_window

                        return window[:, :, :, np.newaxis]

                    # Generate window weights once
                    window_weights = create_gaussian_window(target_depth, target_height, target_width)

                    # Use 50% overlap - works well with the cosine window
                    stride = (target_depth // 2, target_height // 2, target_width // 2)

                    # More efficient window generation
                    def generate_windows_optimized(data, window_size, stride):
                        depth, height, width, _ = data.shape

                        # Calculate end points to avoid repeated calculations
                        max_d = depth - window_size[0]
                        max_h = height - window_size[1]
                        max_w = width - window_size[2]

                        # Regular grid points - main loop
                        d_positions = list(range(0, max_d + 1, stride[0]))
                        h_positions = list(range(0, max_h + 1, stride[1]))
                        w_positions = list(range(0, max_w + 1, stride[2]))

                        # Make sure we include the right edge if it's not in the regular grid
                        if max_d % stride[0] != 0 and max_d not in d_positions:
                            d_positions.append(max_d)
                        if max_h % stride[1] != 0 and max_h not in h_positions:
                            h_positions.append(max_h)
                        if max_w % stride[2] != 0 and max_w not in w_positions:
                            w_positions.append(max_w)

                        # Generate all windows in a single loop - more efficient for branch prediction
                        for d in d_positions:
                            for h in h_positions:
                                for w in w_positions:
                                    yield d, h, w, data[d:d + window_size[0],
                                                   h:h + window_size[1],
                                                   w:w + window_size[2], :]

                    # Process windows with batching if possible
                    batch_size = 1  # Adjust based on your model and memory constraints
                    window_list = []
                    positions = []

                    for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                      (target_depth, target_height, target_width),
                                                                      stride):
                        window_list.append(window)
                        positions.append((d, h, w))

                        # Process batch when it reaches batch_size or at the end
                        if len(window_list) == batch_size or (
                                d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):
                            # Convert to batch
                            batch_windows = np.stack(window_list, axis=0)

                            # Run prediction on the batch
                            batch_outputs = model.predict(batch_windows, verbose=0)

                            # Process each output in the batch
                            for i, (d, h, w) in enumerate(positions):
                                output = batch_outputs[i]

                                # Apply window weights
                                weighted_output = output * window_weights

                                # Accumulate weighted output
                                output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += weighted_output
                                weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += window_weights

                            # Reset for next batch
                            window_list = []
                            positions = []

                    # Normalize by weight sum - use where to avoid division by zero
                    mask = weight_sum > 0
                    output_data[mask] /= weight_sum[mask]

                    # Remove padding if we added it
                    if need_padding:
                        # Crop back to original size
                        output_data = output_data[
                                      pad_depth_before:depth_size - pad_depth_after,
                                      pad_height_before:height_size - pad_height_after,
                                      pad_width_before:width_size - pad_width_after,
                                      :
                                      ]

                    return output_data.astype(np.float32)

                elif len(input_shape) == 4:  # For 2D CNNs

                    _, target_height, target_width, _ = input_shape

                    # Ensure target_height and target_width are initialized properly
                    if target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target height and width.")

                        # Transpose the data to feed the model from the side
                    X_train_transposed = np.transpose(data, (2, 0, 1, 3))  # Inline (1, 0, 2, 3)

                    # Z-score normalize the data
                    mean_X = np.mean(X_train_transposed)
                    std_X = np.std(X_train_transposed)
                    X_train_normalized = (X_train_transposed - mean_X) / std_X

                    stride = (target_height // 2, target_width // 2)  # Overlap by 50%

                    predicted_output_batches = []
                    num_samples = X_train_normalized.shape[0]

                    for i in range(0, num_samples, batch_size):
                        batch_data = X_train_normalized[i:i + batch_size]
                        batch_output = []

                        for j in range(batch_data.shape[0]):
                            output = predict_with_sliding_window_2d(model, batch_data[j],
                                                                    (target_height, target_width), stride,
                                                                    sigma=0.3)  # Gaussian sigma parameter
                            batch_output.append(output)

                        batch_output = np.array(batch_output)
                        predicted_output_batches.append(batch_output)

                        # Concatenate predicted output batches
                    predicted_output = np.concatenate(predicted_output_batches, axis=0)
                    final_predicted_output = np.transpose(predicted_output, (1, 2, 0, 3))

                    # Return the final result
                    return final_predicted_output.astype(np.float32)
                else:
                    raise ValueError("Unsupported input shape dimensions. Expected 4D or 5D input shape.")

            elif model_path.endswith('.pth'):

                target_depth, target_height, target_width = depth, height, width

                # Step 3: Load the model
                model = torch.load(model_path)
                model.eval()  # Set the model to evaluation mode

                # Move the model to the appropriate device (GPU if available, otherwise CPU)
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                model = model.to(device)

                # Normalize data once
                X_train = data
                mean_X = np.mean(X_train)
                std_X = np.std(X_train)
                X_train_normalized = (X_train - mean_X) / std_X

                depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Handle case where input dimensions are smaller than target dimensions
                need_padding = False
                pad_depth, pad_height, pad_width = 0, 0, 0

                if depth_size < target_depth:
                    pad_depth = target_depth - depth_size
                    need_padding = True

                if height_size < target_height:
                    pad_height = target_height - height_size
                    need_padding = True

                if width_size < target_width:
                    pad_width = target_width - width_size
                    need_padding = True

                if need_padding:
                    # Calculate padding for each dimension (distribute evenly on both sides)
                    pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                    pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                    pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                    # Apply padding with reflection to avoid edge artifacts
                    X_train_normalized = np.pad(
                        X_train_normalized,
                        ((pad_depth_before, pad_depth_after),
                         (pad_height_before, pad_height_after),
                         (pad_width_before, pad_width_after),
                         (0, 0)),
                        mode='reflect'
                    )

                    # Update dimensions after padding
                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Initialize arrays to accumulate output and weights
                output_data = np.zeros((depth_size, height_size, width_size, channels))
                weight_sum = np.zeros((depth_size, height_size, width_size, channels))

                # Create gaussian window only once - more efficient implementation
                def create_gaussian_window(depth, height, width, sigma=0.3):
                    # Compute the 3D Gaussian window
                    d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                    h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                    w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                    window = d_window * h_window * w_window

                    return window[:, :, :, np.newaxis]

                # Generate window weights once
                window_weights = create_gaussian_window(target_depth, target_height, target_width)

                # Use 50% overlap - works well with the cosine window
                stride = (target_depth // 2, target_height // 2, target_width // 2)

                # More efficient window generation
                def generate_windows_optimized(data, window_size, stride):
                    depth, height, width, _ = data.shape

                    # Calculate end points to avoid repeated calculations
                    max_d = depth - window_size[0]
                    max_h = height - window_size[1]
                    max_w = width - window_size[2]

                    # Regular grid points - main loop
                    d_positions = list(range(0, max_d + 1, stride[0]))
                    h_positions = list(range(0, max_h + 1, stride[1]))
                    w_positions = list(range(0, max_w + 1, stride[2]))

                    # Make sure we include the right edge if it's not in the regular grid
                    if max_d % stride[0] != 0 and max_d not in d_positions:
                        d_positions.append(max_d)
                    if max_h % stride[1] != 0 and max_h not in h_positions:
                        h_positions.append(max_h)
                    if max_w % stride[2] != 0 and max_w not in w_positions:
                        w_positions.append(max_w)

                    # Generate all windows in a single loop - more efficient for branch prediction
                    for d in d_positions:
                        for h in h_positions:
                            for w in w_positions:
                                yield d, h, w, data[d:d + window_size[0],
                                               h:h + window_size[1],
                                               w:w + window_size[2], :]

                # Process windows with batching if possible
                batch_size = 1  # Adjust based on your model and memory constraints
                window_list = []
                positions = []

                for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                  (target_depth, target_height, target_width),
                                                                  stride):
                    # Add window to batch
                    window_list.append(window)
                    positions.append((d, h, w))

                    # Process batch when it reaches batch_size or at the end
                    if len(window_list) == batch_size or (
                            d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):

                        # Convert windows to batch tensor
                        batch_windows = np.stack(window_list, axis=0)
                        batch_tensor = torch.from_numpy(batch_windows).permute(0, 4, 1, 2, 3).float().to(device)

                        # Run prediction on the batch
                        with torch.no_grad():
                            batch_outputs = model(batch_tensor)

                        # Process each output in the batch
                        for i, (d, h, w) in enumerate(positions):
                            # Get individual output and convert back to numpy
                            output = batch_outputs[i].permute(1, 2, 3, 0).cpu().numpy()

                            # Apply window weights
                            weighted_output = output * window_weights

                            # Accumulate weighted output
                            output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += weighted_output
                            weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += window_weights

                        # Reset for next batch
                        window_list = []
                        positions = []

                # Normalize by weight sum - use where to avoid division by zero
                mask = weight_sum > 0
                output_data[mask] /= weight_sum[mask]

                # Remove padding if we added it
                if need_padding:
                    # Crop back to original size
                    output_data = output_data[
                                  pad_depth_before:depth_size - pad_depth_after,
                                  pad_height_before:height_size - pad_height_after,
                                  pad_width_before:width_size - pad_width_after,
                                  :
                                  ]

                return output_data.astype(np.float32)

    def denoise_unet_prediction(self):
        if not isinstance(self.tensor_data, np.ndarray):
            return
        batch_size = 6
        depth = 128
        height = 128
        width = 128
        input_data = self.tensor_data.astype(np.float32)  # The data you need to pass to the process

        # Function to open a dialog and return the selected model path
        def get_model_path():
            dialog = QFileDialog(self)
            dialog.setFileMode(QFileDialog.FileMode.ExistingFile)
            dialog.setNameFilter("*.h5 *.pth")
            dialog.setWindowTitle("Select Denoise Model")
            if dialog.exec():
                model_path = dialog.selectedFiles()
                return model_path[0]  # Return the first selected file
            return None  # Return None if no file is selected

        # Use the function to get the model path
        model_path = get_model_path()
        if model_path:
            # Check the file extension and load the model accordingly
            if model_path.endswith('.h5'):

                batch_size, ok = QInputDialog.getInt(
                    self, "Batch Size",
                    f"Enter the batch size:",
                    value=6, min=1, max=100
                )
                if not ok:
                    return

            elif model_path.endswith('.pth'):

                # Step 2: Get the depth, height, and width from the user using styled input dialogs
                dialog = SizeInputDialog(self)
                if dialog.exec():
                    depth, height, width = dialog.getValues()
                else:
                    return

            result = self.task_runner.run_task(TensorVisualizer.run_denoise_unet_prediction, input_data, model_path,
                                               batch_size, depth, height, width)

            if result is not None:
                self.tensor_data = result
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_Neural Network Denoise"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_Neural Network Denoise"][
                    'name'] = f"{self.metadata[f'{self.file_name}_Neural Network Denoise'].get('name', '')} Neural Network Denoise"
                self.add_tensor(f"{self.file_name}_Neural Network Denoise", self.tensor_data)

    @staticmethod
    def run_denoise_unet_prediction(data, model_path, batch_size, depth, height, width):

        # Set up logging
        TensorVisualizer.setup_logging('Denoise_Inference')

        # Function to pad the image to the target size
        def pad_image(image, target_height, target_width):
            height, width = image.shape[:2]  # Ensure image has at least 2 dimensions
            channels = 1 if len(image.shape) == 2 else image.shape[2]
            pad_height = max(target_height - height, 0)
            pad_width = max(target_width - width, 0)

            pad_top = pad_height // 2
            pad_bottom = pad_height - pad_top
            pad_left = pad_width // 2
            pad_right = pad_width - pad_left

            padded_image = cv2.copyMakeBorder(image, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_REFLECT)
            if channels == 1:
                padded_image = np.expand_dims(padded_image, axis=-1)
            return padded_image, pad_top, pad_left

        # Function to generate 2D sliding windows
        def generate_windows_2d(data, window_size, stride):
            height, width = data.shape[:2]  # Ensure data has at least 2 dimensions

            # Calculate end points to avoid repeated calculations
            max_h = height - window_size[0]
            max_w = width - window_size[1]

            # Regular grid points - main loop
            h_positions = list(range(0, max_h + 1, stride[0]))
            w_positions = list(range(0, max_w + 1, stride[1]))

            # Make sure we include the right edge if it's not in the regular grid
            if max_h % stride[0] != 0 and max_h not in h_positions:
                h_positions.append(max_h)
            if max_w % stride[1] != 0 and max_w not in w_positions:
                w_positions.append(max_w)

            # Generate all windows in a more efficient loop
            for h in h_positions:
                for w in w_positions:
                    yield h, w, data[h:h + window_size[0], w:w + window_size[1], :]

        # Function to create a 2D Gaussian window
        def create_gaussian_window(height, width, sigma=0.3):
            # Create 1D Gaussian distributions
            h_window = gaussian(height, std=(height - 1) * sigma / 2)[:, np.newaxis]
            w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, :]

            # Multiply to get 2D Gaussian window
            window = h_window * w_window

            # Add channel dimension to match image format
            return window[:, :, np.newaxis]

        # Function to predict using sliding windows and handle smaller images with Gaussian weighting
        def predict_with_sliding_window_2d(model, data, window_size, stride, sigma=0.3):
            if len(data.shape) == 2:
                data = np.expand_dims(data, axis=-1)  # Add channel dimension if not present

            height, width, channels = data.shape
            padded_data, pad_top, pad_left = pad_image(data, window_size[0], window_size[1])

            # Initialize arrays to accumulate output and weights
            output_data = np.zeros(padded_data.shape)
            weight_sum = np.zeros(padded_data.shape)

            # Create Gaussian window weights
            window_weights = create_gaussian_window(window_size[0], window_size[1], sigma)

            for h, w, window in generate_windows_2d(padded_data, window_size, stride):
                window = np.expand_dims(window, axis=0)  # Add batch dimension
                output = model.predict(window, verbose=0)
                output = np.squeeze(output, axis=0)  # Remove batch dimension

                # Apply Gaussian window weights to the output
                weighted_output = output * window_weights

                # Accumulate weighted output and weights
                output_data[h:h + window_size[0], w:w + window_size[1], :] += weighted_output
                weight_sum[h:h + window_size[0], w:w + window_size[1], :] += window_weights

            # Normalize by weight sum - use where to avoid division by zero
            mask = weight_sum > 0
            output_data[mask] /= weight_sum[mask]

            # Remove padding if it was added
            if pad_top > 0 or pad_left > 0:
                output_data = output_data[pad_top:pad_top + height, pad_left:pad_left + width, :]

            return output_data

        if model_path:
            if model_path.endswith('.h5'):
                model = load_model(model_path, compile=False)
                input_shape = model.input_shape

                if len(input_shape) == 5:  # For 3D CNNs
                    _, target_depth, target_height, target_width, _ = input_shape

                    if target_depth is None or target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target depth, height, and width.")

                    # Normalize data once
                    X_train = data
                    mean_X = np.mean(X_train)
                    std_X = np.std(X_train)
                    X_train_normalized = (X_train - mean_X) / std_X

                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Handle case where input dimensions are smaller than target dimensions
                    need_padding = False
                    pad_depth, pad_height, pad_width = 0, 0, 0

                    if depth_size < target_depth:
                        pad_depth = target_depth - depth_size
                        need_padding = True

                    if height_size < target_height:
                        pad_height = target_height - height_size
                        need_padding = True

                    if width_size < target_width:
                        pad_width = target_width - width_size
                        need_padding = True

                    if need_padding:
                        # Calculate padding for each dimension (distribute evenly on both sides)
                        pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                        pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                        pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                        # Apply padding with reflection to avoid edge artifacts
                        X_train_normalized = np.pad(
                            X_train_normalized,
                            ((pad_depth_before, pad_depth_after),
                             (pad_height_before, pad_height_after),
                             (pad_width_before, pad_width_after),
                             (0, 0)),
                            mode='reflect'
                        )

                        # Update dimensions after padding
                        depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Initialize arrays to accumulate output and weights
                    output_data = np.zeros((depth_size, height_size, width_size, channels))
                    weight_sum = np.zeros((depth_size, height_size, width_size, channels))

                    # Create gaussian window only once - more efficient implementation
                    def create_gaussian_window(depth, height, width, sigma=0.3):
                        # Compute the 3D Gaussian window
                        d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                        h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                        w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                        window = d_window * h_window * w_window

                        return window[:, :, :, np.newaxis]

                    # Generate window weights once
                    window_weights = create_gaussian_window(target_depth, target_height, target_width)

                    # Use 50% overlap - works well with the cosine window
                    stride = (target_depth // 2, target_height // 2, target_width // 2)

                    # More efficient window generation
                    def generate_windows_optimized(data, window_size, stride):
                        depth, height, width, _ = data.shape

                        # Calculate end points to avoid repeated calculations
                        max_d = depth - window_size[0]
                        max_h = height - window_size[1]
                        max_w = width - window_size[2]

                        # Regular grid points - main loop
                        d_positions = list(range(0, max_d + 1, stride[0]))
                        h_positions = list(range(0, max_h + 1, stride[1]))
                        w_positions = list(range(0, max_w + 1, stride[2]))

                        # Make sure we include the right edge if it's not in the regular grid
                        if max_d % stride[0] != 0 and max_d not in d_positions:
                            d_positions.append(max_d)
                        if max_h % stride[1] != 0 and max_h not in h_positions:
                            h_positions.append(max_h)
                        if max_w % stride[2] != 0 and max_w not in w_positions:
                            w_positions.append(max_w)

                        # Generate all windows in a single loop - more efficient for branch prediction
                        for d in d_positions:
                            for h in h_positions:
                                for w in w_positions:
                                    yield d, h, w, data[d:d + window_size[0],
                                                   h:h + window_size[1],
                                                   w:w + window_size[2], :]

                    # Process windows with batching if possible
                    batch_size = 1  # Adjust based on your model and memory constraints
                    window_list = []
                    positions = []

                    for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                      (target_depth, target_height, target_width),
                                                                      stride):
                        window_list.append(window)
                        positions.append((d, h, w))

                        # Process batch when it reaches batch_size or at the end
                        if len(window_list) == batch_size or (
                                d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):
                            # Convert to batch
                            batch_windows = np.stack(window_list, axis=0)

                            # Run prediction on the batch
                            batch_outputs = model.predict(batch_windows, verbose=0)

                            # Process each output in the batch
                            for i, (d, h, w) in enumerate(positions):
                                output = batch_outputs[i]

                                # Apply window weights
                                weighted_output = output * window_weights

                                # Accumulate weighted output
                                output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += weighted_output
                                weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += window_weights

                            # Reset for next batch
                            window_list = []
                            positions = []

                    # Normalize by weight sum - use where to avoid division by zero
                    mask = weight_sum > 0
                    output_data[mask] /= weight_sum[mask]

                    # Remove padding if we added it
                    if need_padding:
                        # Crop back to original size
                        output_data = output_data[
                                      pad_depth_before:depth_size - pad_depth_after,
                                      pad_height_before:height_size - pad_height_after,
                                      pad_width_before:width_size - pad_width_after,
                                      :
                                      ]

                    # Reverse Z-score normalize the predicted output
                    return ((output_data * std_X) + mean_X).astype(np.float32)

                elif len(input_shape) == 4:  # For 2D CNNs

                    _, target_height, target_width, _ = input_shape

                    # Ensure target_height and target_width are initialized properly
                    if target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target height and width.")

                        # Transpose the data to feed the model from the side
                    X_train_transposed = np.transpose(data, (2, 0, 1, 3))  # Inline (1, 0, 2, 3)

                    # Z-score normalize the data
                    mean_X = np.mean(X_train_transposed)
                    std_X = np.std(X_train_transposed)
                    X_train_normalized = (X_train_transposed - mean_X) / std_X

                    stride = (target_height // 2, target_width // 2)  # Overlap by 50%

                    predicted_output_batches = []
                    num_samples = X_train_normalized.shape[0]

                    for i in range(0, num_samples, batch_size):
                        batch_data = X_train_normalized[i:i + batch_size]
                        batch_output = []

                        for j in range(batch_data.shape[0]):
                            output = predict_with_sliding_window_2d(model, batch_data[j],
                                                                    (target_height, target_width), stride,
                                                                    sigma=0.3)  # Gaussian sigma parameter
                            batch_output.append(output)

                        batch_output = np.array(batch_output)
                        predicted_output_batches.append(batch_output)

                        # Concatenate predicted output batches
                    predicted_output = np.concatenate(predicted_output_batches, axis=0)
                    final_predicted_output = np.transpose(predicted_output, (1, 2, 0, 3))

                    # Reverse Z-score normalize the predicted output
                    return ((final_predicted_output * std_X) + mean_X).astype(np.float32)

                else:
                    raise ValueError("Unsupported input shape dimensions. Expected 4D or 5D input shape.")

            elif model_path.endswith('.pth'):

                target_depth, target_height, target_width = depth, height, width

                # Step 3: Load the model
                model = torch.load(model_path)
                model.eval()  # Set the model to evaluation mode

                # Move the model to the appropriate device (GPU if available, otherwise CPU)
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                model = model.to(device)

                # Normalize data once
                X_train = data
                mean_X = np.mean(X_train)
                std_X = np.std(X_train)
                X_train_normalized = (X_train - mean_X) / std_X

                depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Handle case where input dimensions are smaller than target dimensions
                need_padding = False
                pad_depth, pad_height, pad_width = 0, 0, 0

                if depth_size < target_depth:
                    pad_depth = target_depth - depth_size
                    need_padding = True

                if height_size < target_height:
                    pad_height = target_height - height_size
                    need_padding = True

                if width_size < target_width:
                    pad_width = target_width - width_size
                    need_padding = True

                if need_padding:
                    # Calculate padding for each dimension (distribute evenly on both sides)
                    pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                    pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                    pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                    # Apply padding with reflection to avoid edge artifacts
                    X_train_normalized = np.pad(
                        X_train_normalized,
                        ((pad_depth_before, pad_depth_after),
                         (pad_height_before, pad_height_after),
                         (pad_width_before, pad_width_after),
                         (0, 0)),
                        mode='reflect'
                    )

                    # Update dimensions after padding
                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Initialize arrays to accumulate output and weights
                output_data = np.zeros((depth_size, height_size, width_size, channels))
                weight_sum = np.zeros((depth_size, height_size, width_size, channels))

                # Create gaussian window only once - more efficient implementation
                def create_gaussian_window(depth, height, width, sigma=0.3):
                    # Compute the 3D Gaussian window
                    d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                    h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                    w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                    window = d_window * h_window * w_window

                    return window[:, :, :, np.newaxis]

                # Generate window weights once
                window_weights = create_gaussian_window(target_depth, target_height, target_width)

                # Use 50% overlap - works well with the cosine window
                stride = (target_depth // 2, target_height // 2, target_width // 2)

                # More efficient window generation
                def generate_windows_optimized(data, window_size, stride):
                    depth, height, width, _ = data.shape

                    # Calculate end points to avoid repeated calculations
                    max_d = depth - window_size[0]
                    max_h = height - window_size[1]
                    max_w = width - window_size[2]

                    # Regular grid points - main loop
                    d_positions = list(range(0, max_d + 1, stride[0]))
                    h_positions = list(range(0, max_h + 1, stride[1]))
                    w_positions = list(range(0, max_w + 1, stride[2]))

                    # Make sure we include the right edge if it's not in the regular grid
                    if max_d % stride[0] != 0 and max_d not in d_positions:
                        d_positions.append(max_d)
                    if max_h % stride[1] != 0 and max_h not in h_positions:
                        h_positions.append(max_h)
                    if max_w % stride[2] != 0 and max_w not in w_positions:
                        w_positions.append(max_w)

                    # Generate all windows in a single loop - more efficient for branch prediction
                    for d in d_positions:
                        for h in h_positions:
                            for w in w_positions:
                                yield d, h, w, data[d:d + window_size[0],
                                               h:h + window_size[1],
                                               w:w + window_size[2], :]

                # Process windows with batching if possible
                batch_size = 1  # Adjust based on your model and memory constraints
                window_list = []
                positions = []

                for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                  (target_depth, target_height, target_width),
                                                                  stride):
                    # Add window to batch
                    window_list.append(window)
                    positions.append((d, h, w))

                    # Process batch when it reaches batch_size or at the end
                    if len(window_list) == batch_size or (
                            d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):

                        # Convert windows to batch tensor
                        batch_windows = np.stack(window_list, axis=0)
                        batch_tensor = torch.from_numpy(batch_windows).permute(0, 4, 1, 2, 3).float().to(device)

                        # Run prediction on the batch
                        with torch.no_grad():
                            batch_outputs = model(batch_tensor)

                        # Process each output in the batch
                        for i, (d, h, w) in enumerate(positions):
                            # Get individual output and convert back to numpy
                            output = batch_outputs[i].permute(1, 2, 3, 0).cpu().numpy()

                            # Apply window weights
                            weighted_output = output * window_weights

                            # Accumulate weighted output
                            output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += weighted_output
                            weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += window_weights

                        # Reset for next batch
                        window_list = []
                        positions = []

                # Normalize by weight sum - use where to avoid division by zero
                mask = weight_sum > 0
                output_data[mask] /= weight_sum[mask]

                # Remove padding if we added it
                if need_padding:
                    # Crop back to original size
                    output_data = output_data[
                                  pad_depth_before:depth_size - pad_depth_after,
                                  pad_height_before:height_size - pad_height_after,
                                  pad_width_before:width_size - pad_width_after,
                                  :
                                  ]

                # Reverse Z-score normalize the predicted output
                return ((output_data * std_X) + mean_X).astype(np.float32)

    # Define the task as a local function that only does computation
    @staticmethod
    def nlm_denoising_task(data, strength):
        # Store the original shape of the tensor data
        original_shape = data.shape
        data = np.squeeze(data)  # Remove single channel dimension for 3D processing
        sigma_est = np.mean(estimate_sigma(data, channel_axis=None)) * strength
        patch_kw = dict(patch_size=5, patch_distance=6, channel_axis=None)
        result = denoise_nl_means(data, h=1.15 * sigma_est, fast_mode=True, **patch_kw)
        # Reshape result to the original shape
        return np.reshape(result, original_shape).astype(np.float32)

    def non_local_means_denoising_3d(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return

        strength, ok = QInputDialog.getInt(
            self, "Input Strength",
            f"Enter the denoising strength:",
            value=4, min=1, max=100
        )
        if not ok:
            return

        data = self.tensor_data.astype(np.float32)

        result = self.task_runner.run_task(TensorVisualizer.nlm_denoising_task, data, strength)

        if result is not None:
            self.tensor_data = result
            self.metadata[f"{self.file_name}_Non Local Means Denoise"] = self.metadata[
                self.file_name].copy()
            self.metadata[f"{self.file_name}_Non Local Means Denoise"][
                'name'] = f"{self.metadata[f'{self.file_name}_Non Local Means Denoise'].get('name', '')} Non Local Means Denoise"
            self.add_tensor(f"{self.file_name}_Non Local Means Denoise", self.tensor_data)

    @staticmethod
    def median_filter_cpu_task(data, strength):

        original_shape = data.shape + (1,)

        if len(data) == 2:
            # Create binary mask if there are only two unique values
            data = np.where(data != 0, 1, 0)

        smoothed = ndimage.median_filter(data, size=strength)

        return np.reshape(smoothed, original_shape).astype(np.int16)

    @staticmethod
    def median_filter_gpu_task(data, strength):

        original_shape = data.shape + (1,)

        if len(data) == 2:
            # Create binary mask if there are only two unique values
            data = np.where(data != 0, 1, 0)

        data = cp.array(data)

        smoothed = gpu_ndimage.median_filter(data, size=strength)

        return cp.asnumpy(cp.reshape(smoothed, original_shape).astype(cp.int16))

    def median_filter(self):
        try:
            if not isinstance(self.tensor_data, np.ndarray):
                return

            # Create an integer input dialog
            strength, ok = QInputDialog.getInt(
                self, "Kernel Size",
                f"Enter the kernel size",
                min=1)

            if not ok:
                return

            # Assuming self.tensor_data is your 4D tensor with shape (depth, height, width, channels)
            original_shape = self.tensor_data.shape

            # Check if the last dimension (channel dimension) has more than one channel
            if original_shape[-1] > 1:
                # Create an integer input dialog to select the channel
                channel, ok = QInputDialog.getInt(
                    self, "Select Channel",
                    f"Enter the channel (1 to {original_shape[-1]}):",
                    min=1, max=original_shape[-1]
                )

                # Check if the user clicked OK
                if not ok:
                    return

                # Adjust the channel index (subtract 1 because QInputDialog returns 1-based index)
                channel -= 1
            else:
                # If there's only one channel, default to that single channel
                channel = 0

            QApplication.setOverrideCursor(self.custom_cursor)

            data = self.tensor_data.astype(np.int16)

            # Extract the 3D tensor for the chosen channel
            data = np.squeeze(data[..., channel])

            gpu = False

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = data.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 3:
                    cp.get_default_memory_pool().free_all_blocks()

                    gpu = False
                else:
                    gpu = True

            if gpu:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples * strength / 5 <= self.multiprocessing_threshold:
                    result = TensorVisualizer.median_filter_gpu_task(data, strength)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.median_filter_gpu_task,
                                                       data, strength)
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.median_filter_cpu_task,
                                                   data, strength)

            if result is not None:
                self.tensor_data = result
                self.metadata[f"{self.file_name}_Median {self.metadata[self.file_name]['template']} ({strength})"] = \
                    self.metadata[
                        self.file_name].copy()
                self.metadata[
                    f"{self.file_name}_Median {self.metadata[self.file_name]['template']} ({strength})"][
                    'template'] = f"Median {self.metadata[self.file_name]['template']} ({strength})"
                self.add_tensor(f"{self.file_name}_Median {self.metadata[self.file_name]['template']} ({strength})",
                                self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def binary_closing_gpu_task(data, strength):
        original_shape = data.shape[:3] + (1,)

        structure = ball(strength)  # Use ball for isotropic behavior

        data = cp.array(data)
        structure_cp = cp.array(structure)  # Convert structure to CuPy array

        # Step 6: Selective gap filling
        filled_gaps = gpu_ndimage.binary_closing(data, structure=structure_cp)

        return cp.asnumpy(cp.reshape(filled_gaps, original_shape).astype(cp.int16))

    @staticmethod
    def binary_closing_cpu_task(data, strength):
        original_shape = data.shape[:3] + (1,)

        structure = ball(strength)  # Use ball for isotropic behavior

        # Step 6: Selective gap filling
        filled_gaps = ndimage.binary_closing(data, structure=structure)

        return np.reshape(filled_gaps, original_shape).astype(np.int16)

    def binary_closing(self):
        try:
            if not isinstance(self.tensor_data, np.ndarray):
                return

            # Create an integer input dialog
            strength, ok = QInputDialog.getInt(
                self, "Structuring Element Size",
                f"Enter the structuring element size",
                min=1)

            if not ok:
                return

            # Assuming self.tensor_data is your 4D tensor with shape (depth, height, width, channels)
            original_shape = self.tensor_data.shape

            # Check if the last dimension (channel dimension) has more than one channel
            if original_shape[-1] > 1:
                # Create an integer input dialog to select the channel
                channel, ok = QInputDialog.getInt(
                    self, "Select Channel",
                    f"Enter the channel (1 to {original_shape[-1]}):",
                    min=1, max=original_shape[-1]
                )

                # Check if the user clicked OK
                if not ok:
                    return

                # Adjust the channel index (subtract 1 because QInputDialog returns 1-based index)
                channel -= 1
            else:
                # If there's only one channel, default to that single channel
                channel = 0

            QApplication.setOverrideCursor(self.custom_cursor)

            data = self.tensor_data.astype(np.int16)

            # Extract the 3D tensor for the chosen channel
            data = np.squeeze(data[..., channel])

            gpu = False

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = data.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 2:
                    cp.get_default_memory_pool().free_all_blocks()

                    gpu = False
                else:
                    gpu = True

            if gpu:
                # Compute total sample count
                num_samples = np.prod(data.shape, dtype=np.int64)

                if num_samples * strength / 5 <= self.multiprocessing_threshold:
                    result = TensorVisualizer.binary_closing_gpu_task(data, strength)
                    QApplication.restoreOverrideCursor()
                else:
                    QApplication.restoreOverrideCursor()
                    result = self.task_runner.run_task(TensorVisualizer.binary_closing_gpu_task,
                                                       data, strength)
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.binary_closing_cpu_task,
                                                   data, strength)

            if result is not None:
                self.tensor_data = result
                self.metadata[
                    f"{self.file_name}_Binary Closing {self.metadata[self.file_name]['template']} ({strength})"] = \
                    self.metadata[
                        self.file_name].copy()
                self.metadata[
                    f"{self.file_name}_Binary Closing {self.metadata[self.file_name]['template']} ({strength})"][
                    'template'] = f"Binary Closing {self.metadata[self.file_name]['template']} ({strength})"
                self.add_tensor(
                    f"{self.file_name}_Binary Closing {self.metadata[self.file_name]['template']} ({strength})",
                    self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def bm3d_denoising_task(data, strength):
        original_shape = data.shape
        dtype = data.dtype
        data = np.squeeze(data)  # Remove single channel dimension for 3D processing
        sigma_est = np.mean(estimate_sigma(data, channel_axis=None)) * strength
        profile = BM3DProfile()
        result = bm3d(data, sigma_psd=sigma_est, profile=profile)
        return np.reshape(result.astype(dtype), original_shape).astype(np.float32)

    def bm3d_denoising_3d(self):
        if not isinstance(self.tensor_data, np.ndarray):
            return

        strength, ok = QInputDialog.getInt(
            self, "Input Strength",
            f"Enter the denoising strength:",
            value=4, min=1, max=100
        )
        if not ok:
            return

        data = self.tensor_data.astype(np.float32)

        result = self.task_runner.run_task(TensorVisualizer.bm3d_denoising_task, data, strength)

        if result is not None:
            self.tensor_data = result
            self.metadata[f"{self.file_name}_BM3D Denoise"] = self.metadata[
                self.file_name].copy()
            self.metadata[f"{self.file_name}_BM3D Denoise"][
                'name'] = f"{self.metadata[f'{self.file_name}_BM3D Denoise'].get('name', '')} BM3D Denoise"
            self.add_tensor(f"{self.file_name}_BM3D Denoise", self.tensor_data)

    def facies_prediction(self):
        try:
            if not isinstance(self.tensor_data, np.ndarray):
                return
            batch_size = 6
            depth = 128
            height = 128
            width = 128
            input_data = self.tensor_data.astype(np.float32)

            # Function to open a dialog and return the selected model path
            def get_model_path():
                dialog = QFileDialog(self)
                dialog.setFileMode(QFileDialog.FileMode.ExistingFile)
                dialog.setNameFilter("*.h5 *.pth")
                dialog.setWindowTitle("Select Zone Segmentation Model")
                if dialog.exec():
                    model_path = dialog.selectedFiles()
                    return model_path[0]  # Return the first selected file
                return None  # Return None if no file is selected

            # Use the function to get the model path
            model_path = get_model_path()
            if model_path:
                # Check the file extension and load the model accordingly
                if model_path.endswith('.h5'):

                    batch_size, ok = QInputDialog.getInt(
                        self, "Batch Size",
                        f"Enter the batch size:",
                        value=6, min=1, max=100
                    )
                    if not ok:
                        return

                elif model_path.endswith('.pth'):

                    # Step 2: Get the depth, height, and width from the user using styled input dialogs
                    dialog = SizeInputDialog(self)
                    if dialog.exec():
                        depth, height, width = dialog.getValues()
                    else:
                        return

                options = ["Multichannel", "Single Channel"]
                choice, ok = QInputDialog.getItem(
                    self,  # Parent widget
                    "Select Processing Option",  # Dialog title
                    "Choose an option:",  # Prompt message
                    options,  # List of choices
                    0,  # Default index
                    False  # Editable (False = dropdown only)
                )

                if not ok:
                    return

                # Reconstruct the tensor from shared memory using the metadata
                result = self.task_runner.run_task(TensorVisualizer.run_facies_prediction_inference, input_data,
                                                   model_path, batch_size, depth, height, width)

                if result is not None:
                    if choice == "Multichannel":
                        classes = np.unique(result)
                        class_to_index = {c: i for i, c in enumerate(classes)}

                        # Convert result values to indices in the range 0...n
                        indices = np.vectorize(class_to_index.get)(result)

                        # Use np.eye for one-hot encoding (very efficient)
                        multi_channel = np.eye(len(classes), dtype=np.int16)[indices]

                        # Store the final result in self.tensor_data
                        self.tensor_data = multi_channel
                        # Store the modified tensor with an informative key
                        self.metadata[f"{self.file_name}_Zones Multichannel"] = self.metadata[
                            self.file_name].copy()
                        self.metadata[f"{self.file_name}_Zones Multichannel"][
                            'template'] = "Zones Multichannel"
                        self.add_tensor(f"{self.file_name}_Zones Multichannel", self.tensor_data)

                    elif choice == "Single Channel":
                        single_channel = result + 1  # Start from 1 instead of 0
                        # Store the final result in self.tensor_data
                        self.tensor_data = np.expand_dims(single_channel, axis=-1)
                        # Store the modified tensor with an informative key
                        self.metadata[f"{self.file_name}_Zones"] = self.metadata[
                            self.file_name].copy()
                        self.metadata[f"{self.file_name}_Zones"][
                            'template'] = "Zones"
                        self.add_tensor(f"{self.file_name}_Zones", self.tensor_data)

        except Exception as e:
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def run_facies_prediction_inference(data, model_path, batch_size, depth, height, width):

        if model_path:

            # Set up logging
            TensorVisualizer.setup_logging('Zone_Prediction')

            if model_path.endswith('.h5'):
                model = load_model(model_path, compile=False)
                input_shape = model.input_shape
                model_output_shape = model.output_shape  # Get the model's output shape
                output_channels = model_output_shape[
                    -1]  # Assuming the last dimension represents the number of channels

                if len(input_shape) == 5:  # For 3D CNNs
                    _, target_depth, target_height, target_width, _ = input_shape

                    if target_depth is None or target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target depth, height, and width."
                        )

                    # Normalize data once
                    X_train = data
                    mean_X = np.mean(X_train)
                    std_X = np.std(X_train)
                    X_train_normalized = (X_train - mean_X) / std_X

                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Handle case where input dimensions are smaller than target dimensions
                    need_padding = False
                    pad_depth, pad_height, pad_width = 0, 0, 0

                    if depth_size < target_depth:
                        pad_depth = target_depth - depth_size
                        need_padding = True

                    if height_size < target_height:
                        pad_height = target_height - height_size
                        need_padding = True

                    if width_size < target_width:
                        pad_width = target_width - width_size
                        need_padding = True

                    if need_padding:
                        # Calculate padding for each dimension (distribute evenly on both sides)
                        pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                        pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                        pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                        # Apply padding with reflection to avoid edge artifacts
                        X_train_normalized = np.pad(
                            X_train_normalized,
                            ((pad_depth_before, pad_depth_after),
                             (pad_height_before, pad_height_after),
                             (pad_width_before, pad_width_after),
                             (0, 0)),
                            mode='reflect'
                        )

                        # Update dimensions after padding
                        depth_size, height_size, width_size, channels = X_train_normalized.shape

                    # Initialize arrays to accumulate output and weights
                    output_data = np.zeros((depth_size, height_size, width_size, output_channels))
                    weight_sum = np.zeros((depth_size, height_size, width_size, output_channels))

                    # Create gaussian window only once - more efficient implementation
                    def create_gaussian_window(depth, height, width, sigma=0.3):
                        # Compute the 3D Gaussian window
                        d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                        h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                        w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                        window = d_window * h_window * w_window

                        return window[:, :, :, np.newaxis]

                    # Generate window weights once
                    window_weights = create_gaussian_window(target_depth, target_height, target_width)

                    # Use 50% overlap - works well with the cosine window
                    stride = (target_depth // 2, target_height // 2, target_width // 2)

                    # More efficient window generation
                    def generate_windows_optimized(data, window_size, stride):
                        depth, height, width, _ = data.shape

                        # Calculate end points to avoid repeated calculations
                        max_d = depth - window_size[0]
                        max_h = height - window_size[1]
                        max_w = width - window_size[2]

                        # Regular grid points - main loop
                        d_positions = list(range(0, max_d + 1, stride[0]))
                        h_positions = list(range(0, max_h + 1, stride[1]))
                        w_positions = list(range(0, max_w + 1, stride[2]))

                        # Make sure we include the right edge if it's not in the regular grid
                        if max_d % stride[0] != 0 and max_d not in d_positions:
                            d_positions.append(max_d)
                        if max_h % stride[1] != 0 and max_h not in h_positions:
                            h_positions.append(max_h)
                        if max_w % stride[2] != 0 and max_w not in w_positions:
                            w_positions.append(max_w)

                        # Generate all windows in a single loop - more efficient for branch prediction
                        for d in d_positions:
                            for h in h_positions:
                                for w in w_positions:
                                    yield d, h, w, data[d:d + window_size[0],
                                                   h:h + window_size[1],
                                                   w:w + window_size[2], :]

                    # Process windows with batching if possible
                    batch_size = 1  # Adjust based on your model and memory constraints
                    window_list = []
                    positions = []

                    for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                      (target_depth, target_height, target_width),
                                                                      stride):
                        window_list.append(window)
                        positions.append((d, h, w))

                        # Process batch when it reaches batch_size or at the end
                        if len(window_list) == batch_size or (
                                d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):
                            # Convert to batch
                            batch_windows = np.stack(window_list, axis=0)

                            # Run prediction on the batch
                            batch_outputs = model.predict(batch_windows, verbose=0)

                            # Process each output in the batch
                            for i, (d, h, w) in enumerate(positions):
                                output = batch_outputs[i]

                                # Apply window weights
                                weighted_output = output * window_weights

                                # Accumulate weighted output
                                output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += weighted_output
                                weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                                :] += window_weights

                            # Reset for next batch
                            window_list = []
                            positions = []

                    # Normalize by weight sum - use where to avoid division by zero
                    mask = weight_sum > 0
                    output_data[mask] /= weight_sum[mask]

                    # Remove padding if we added it
                    if need_padding:
                        # Crop back to original size
                        output_data = output_data[
                                      pad_depth_before:depth_size - pad_depth_after,
                                      pad_height_before:height_size - pad_height_after,
                                      pad_width_before:width_size - pad_width_after,
                                      :
                                      ]

                    # Step 1: Find the index of the maximum value across the channels for each pixel
                    output = np.argmax(output_data, axis=-1)
                    return output.astype(np.int16)

                elif len(input_shape) == 4:  # For 2D CNNs
                    _, target_height, target_width, _ = input_shape

                    # Ensure target_height and target_width are initialized properly
                    if target_height is None or target_width is None:
                        raise ValueError(
                            "Model input shape dimensions not properly set for target height and width.")

                    # Transpose the data to feed the model from the side
                    X_train_transposed = np.transpose(data, (2, 0, 1, 3))  # Inline (1, 0, 2, 3)

                    # Z-score normalize the data
                    mean_X = np.mean(X_train_transposed)
                    std_X = np.std(X_train_transposed)
                    X_train_normalized = (X_train_transposed - mean_X) / std_X

                    # Predict using the trained model in batches
                    predicted_output_batches = []
                    num_samples = X_train_normalized.shape[0]
                    for i in range(0, num_samples, batch_size):
                        batch_data = X_train_normalized[i:i + batch_size]

                        # Resize each image in the batch to the model's input size
                        resized_batch_data = np.zeros(
                            (batch_data.shape[0], target_height, target_width, batch_data.shape[3]),
                            dtype=np.float32)
                        for j in range(batch_data.shape[0]):
                            resized_batch_data[j] = resize(batch_data[j], (target_height, target_width))

                        # Predict on resized batch
                        batch_output = model.predict(resized_batch_data, verbose=0)

                        # Resize the predicted output back to the original image size
                        resized_batch_output = np.zeros((batch_output.shape[0], X_train_transposed.shape[1],
                                                         X_train_transposed.shape[2], batch_output.shape[3]),
                                                        dtype=np.float32)
                        for j in range(batch_output.shape[0]):
                            resized_batch_output[j] = resize(batch_output[j],
                                                             (X_train_transposed.shape[1],
                                                              X_train_transposed.shape[2]))

                        predicted_output_batches.append(resized_batch_output)

                    # Concatenate predicted output batches
                    predicted_output = np.concatenate(predicted_output_batches, axis=0)

                    output_data = np.transpose(predicted_output, (1, 2, 0, 3))

                    # Step 1: Find the index of the maximum value across the channels for each pixel
                    output = np.argmax(output_data, axis=-1)
                    return output.astype(np.int16)
                else:
                    raise ValueError("Unsupported input shape dimensions. Expected 4D or 5D input shape.")

            elif model_path.endswith('.pth'):

                target_depth, target_height, target_width = depth, height, width

                # Step 3: Load the model
                model = torch.load(model_path)
                model.eval()  # Set the model to evaluation mode

                # Move the model to the appropriate device (GPU if available, otherwise CPU)
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                model = model.to(device)

                # Normalize data once
                X_train = data
                mean_X = np.mean(X_train)
                std_X = np.std(X_train)
                X_train_normalized = (X_train - mean_X) / std_X

                depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Determine number of output channels
                dummy_input = torch.zeros((1, channels, depth, height, width)).to(device)  # Create a dummy input
                with torch.no_grad():
                    dummy_output = model(dummy_input)
                output_channels = dummy_output.size(1)  # Get the number of output channels

                # Handle case where input dimensions are smaller than target dimensions
                need_padding = False
                pad_depth, pad_height, pad_width = 0, 0, 0

                if depth_size < target_depth:
                    pad_depth = target_depth - depth_size
                    need_padding = True

                if height_size < target_height:
                    pad_height = target_height - height_size
                    need_padding = True

                if width_size < target_width:
                    pad_width = target_width - width_size
                    need_padding = True

                if need_padding:
                    # Calculate padding for each dimension (distribute evenly on both sides)
                    pad_depth_before, pad_depth_after = pad_depth // 2, pad_depth - (pad_depth // 2)
                    pad_height_before, pad_height_after = pad_height // 2, pad_height - (pad_height // 2)
                    pad_width_before, pad_width_after = pad_width // 2, pad_width - (pad_width // 2)

                    # Apply padding with reflection to avoid edge artifacts
                    X_train_normalized = np.pad(
                        X_train_normalized,
                        ((pad_depth_before, pad_depth_after),
                         (pad_height_before, pad_height_after),
                         (pad_width_before, pad_width_after),
                         (0, 0)),
                        mode='reflect'
                    )

                    # Update dimensions after padding
                    depth_size, height_size, width_size, channels = X_train_normalized.shape

                # Initialize arrays to accumulate output and weights
                output_data = np.zeros((depth_size, height_size, width_size, output_channels))
                weight_sum = np.zeros((depth_size, height_size, width_size, output_channels))

                # Create gaussian window only once - more efficient implementation
                def create_gaussian_window(depth, height, width, sigma=0.3):
                    # Compute the 3D Gaussian window
                    d_window = gaussian(depth, std=(depth - 1) * sigma / 2)[:, np.newaxis, np.newaxis]
                    h_window = gaussian(height, std=(height - 1) * sigma / 2)[np.newaxis, :, np.newaxis]
                    w_window = gaussian(width, std=(width - 1) * sigma / 2)[np.newaxis, np.newaxis, :]

                    window = d_window * h_window * w_window

                    return window[:, :, :, np.newaxis]

                # Generate window weights once
                window_weights = create_gaussian_window(target_depth, target_height, target_width)

                # Use 50% overlap - works well with the cosine window
                stride = (target_depth // 2, target_height // 2, target_width // 2)

                # More efficient window generation
                def generate_windows_optimized(data, window_size, stride):
                    depth, height, width, _ = data.shape

                    # Calculate end points to avoid repeated calculations
                    max_d = depth - window_size[0]
                    max_h = height - window_size[1]
                    max_w = width - window_size[2]

                    # Regular grid points - main loop
                    d_positions = list(range(0, max_d + 1, stride[0]))
                    h_positions = list(range(0, max_h + 1, stride[1]))
                    w_positions = list(range(0, max_w + 1, stride[2]))

                    # Make sure we include the right edge if it's not in the regular grid
                    if max_d % stride[0] != 0 and max_d not in d_positions:
                        d_positions.append(max_d)
                    if max_h % stride[1] != 0 and max_h not in h_positions:
                        h_positions.append(max_h)
                    if max_w % stride[2] != 0 and max_w not in w_positions:
                        w_positions.append(max_w)

                    # Generate all windows in a single loop - more efficient for branch prediction
                    for d in d_positions:
                        for h in h_positions:
                            for w in w_positions:
                                yield d, h, w, data[d:d + window_size[0],
                                               h:h + window_size[1],
                                               w:w + window_size[2], :]

                # Process windows with batching if possible
                batch_size = 1  # Adjust based on your model and memory constraints
                window_list = []
                positions = []

                for d, h, w, window in generate_windows_optimized(X_train_normalized,
                                                                  (target_depth, target_height, target_width),
                                                                  stride):
                    # Add window to batch
                    window_list.append(window)
                    positions.append((d, h, w))

                    # Process batch when it reaches batch_size or at the end
                    if len(window_list) == batch_size or (
                            d == positions[-1][0] and h == positions[-1][1] and w == positions[-1][2]):

                        # Convert windows to batch tensor
                        batch_windows = np.stack(window_list, axis=0)
                        batch_tensor = torch.from_numpy(batch_windows).permute(0, 4, 1, 2, 3).float().to(device)

                        # Run prediction on the batch
                        with torch.no_grad():
                            batch_outputs = model(batch_tensor)

                        # Process each output in the batch
                        for i, (d, h, w) in enumerate(positions):
                            # Get individual output and convert back to numpy
                            output = batch_outputs[i].permute(1, 2, 3, 0).cpu().numpy()

                            # Apply window weights
                            weighted_output = output * window_weights

                            # Accumulate weighted output
                            output_data[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += weighted_output
                            weight_sum[d:d + target_depth, h:h + target_height, w:w + target_width,
                            :] += window_weights

                        # Reset for next batch
                        window_list = []
                        positions = []

                # Normalize by weight sum - use where to avoid division by zero
                mask = weight_sum > 0
                output_data[mask] /= weight_sum[mask]

                # Remove padding if we added it
                if need_padding:
                    # Crop back to original size
                    output_data = output_data[
                                  pad_depth_before:depth_size - pad_depth_after,
                                  pad_height_before:height_size - pad_height_after,
                                  pad_width_before:width_size - pad_width_after,
                                  :
                                  ]

                # Step 1: Find the index of the maximum value across the channels for each pixel
                output = np.argmax(output_data, axis=-1)
                return output.astype(np.int16)

    @staticmethod
    def thresholded_amplitude_task(data):
        # Compute mean and std over the spatial dimensions
        mean_amplitude = np.mean(data)
        std_amplitude = np.std(data)

        # Z-score normalization
        normalized_data = (data - mean_amplitude) / (std_amplitude + 1e-8)  # 1e-8 added for numerical stability

        return normalized_data, np.max(normalized_data), np.min(normalized_data)

    def thresholded_amplitude(self):
        try:
            QApplication.setOverrideCursor(self.custom_cursor)
            if not isinstance(self.tensor_data, np.ndarray):
                QApplication.restoreOverrideCursor()
                return
            # Step 1: Flatten the 'channel' dimension to compute the mean and standard deviation
            data = self.tensor_data.astype(np.float32)

            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold * 10:
                result = TensorVisualizer.thresholded_amplitude_task(data)
            else:
                QApplication.restoreOverrideCursor()
                result = self.task_runner.run_task(TensorVisualizer.thresholded_amplitude_task,
                                                   data)

            if result is not None:
                normalized_data, max, min = result

                QApplication.restoreOverrideCursor()

                # Create the dialog and set its title
                dialog = QDialog(self)
                dialog.setWindowTitle('Input Threshold')

                # Create a form layout for the dialog
                layout = QFormLayout(dialog)

                # Create the QDoubleSpinBox for the minimum threshold
                min_spinbox = QDoubleSpinBox(dialog)
                min_spinbox.setDecimals(2)
                min_spinbox.setMinimum(min)
                min_spinbox.setMaximum(max)
                min_spinbox.setValue(-2.00)
                layout.addRow(f"Minimum threshold: (Min: {min:.2f})", min_spinbox)

                # Create the QDoubleSpinBox for the maximum threshold
                max_spinbox = QDoubleSpinBox(dialog)
                max_spinbox.setDecimals(2)
                max_spinbox.setMinimum(min)
                max_spinbox.setMaximum(max)
                max_spinbox.setValue(2.00)
                layout.addRow(f"Maximum threshold: (Max: {max:.2f})", max_spinbox)

                # Create an OK button and make it as wide as the dialog
                button = QPushButton("OK")
                button.clicked.connect(dialog.accept)
                button.setSizePolicy(QSizePolicy.Policy.Expanding, QSizePolicy.Policy.Fixed)
                layout.addRow(button)  # Ensures full width in the layout

                # Execute the dialog and retrieve the threshold values if accepted
                if dialog.exec() == QDialog.DialogCode.Accepted:
                    threshold_min = min_spinbox.value()
                    threshold_max = max_spinbox.value()
                    # Use threshold_min and threshold_max as needed

                    QApplication.setOverrideCursor(self.custom_cursor)

                    normalized_data[(normalized_data >= threshold_min) & (normalized_data <= threshold_max)] = 0

                    self.tensor_data = normalized_data
                    # Store the modified tensor with an informative key
                    self.metadata[f"{self.file_name}_Anomalies"] = self.metadata[
                        self.file_name].copy()
                    self.metadata[f"{self.file_name}_Anomalies"][
                        'template'] = "Anomalies"
                    self.add_tensor(f"{self.file_name}_Anomalies", self.tensor_data)
                    QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def kmeans_amplitude_task(data, n_clusters):
        amplitude_values = data[:, :, :, -1].flatten()

        # Reshape normalized data for K-Means
        reshaped_data = amplitude_values.reshape(-1, 1)

        # Apply K-Means clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        kmeans.fit(reshaped_data)

        # Get the cluster labels for each data point
        cluster_labels = kmeans.labels_

        # Replace the original amplitude values with the cluster labels
        data[:, :, :, -1] = cluster_labels.reshape(data.shape[:-1])

        # Ensure class 0 is not used by incrementing all cluster labels by 1
        data[:, :, :, -1] += 1

        return data.astype(np.int16)

    def kmeans_amplitude(self):

        if not isinstance(self.tensor_data, np.ndarray):
            return
        self.num_clusters[self.file_name], okPressed = QInputDialog.getInt(
            self, "Input Number of Clusters", "Number of Clusters:", 10, 1, 100, 1)
        if okPressed:
            if self.kmeans_dict.get(f"{self.file_name}_{self.num_clusters[self.file_name]}") is None:

                data = self.tensor_data.astype(np.float32)
                n_clusters = self.num_clusters[self.file_name]

                data = self.task_runner.run_task(TensorVisualizer.kmeans_amplitude_task, data, n_clusters)

                if data is not None:

                    copied_original_data = np.copy(data)
                    self.kmeans_dict[f"{self.file_name}_{self.num_clusters[self.file_name]}"] = copied_original_data

                    # After clustering, prompt the user to select which clusters to keep
                    clusters_to_keep = self.select_clusters_to_keep(self.num_clusters[self.file_name])

                    if clusters_to_keep is not None:
                        # Create a mask of clusters to keep
                        keep_mask = np.isin(data[:, :, :, -1], clusters_to_keep)
                        # Invert the mask to get clusters to zero out
                        zero_mask = ~keep_mask
                        data[zero_mask] = 0

                    # Store the modified tensor with an informative key
                    self.tensor_data = data
                    self.metadata[f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}"] = \
                        self.metadata[
                            self.file_name].copy()
                    self.metadata[f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}"][
                        'template'] = f"Clusters {self.num_clusters[self.file_name]}"
                    self.add_tensor(f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}",
                                    self.tensor_data)
            else:
                data = self.kmeans_dict[f"{self.file_name}_{self.num_clusters[self.file_name]}"]

                data = np.copy(data)

                # After clustering, prompt the user to select which clusters to keep
                clusters_to_keep = self.select_clusters_to_keep(self.num_clusters[self.file_name])

                if clusters_to_keep is not None:
                    # Create a mask of clusters to keep
                    keep_mask = np.isin(data[:, :, :, -1], clusters_to_keep)
                    # Invert the mask to get clusters to zero out
                    zero_mask = ~keep_mask
                    data[zero_mask] = 0

                # Store the modified tensor with an informative key
                self.tensor_data = data
                self.metadata[f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}"] = \
                    self.metadata[
                        self.file_name].copy()
                self.metadata[f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}"][
                    'template'] = f"Clusters {self.num_clusters[self.file_name]}"
                self.add_tensor(f"{self.file_name}_Clusters {self.num_clusters[self.file_name]}",
                                self.tensor_data)

    def select_clusters_to_keep(self, num_clusters):
        try:
            dialog = QDialog(self)
            dialog.setWindowTitle('Select Clusters to Keep')
            layout = QVBoxLayout(dialog)

            list_widget = QListWidget(dialog)
            list_widget.setSelectionMode(QAbstractItemView.SelectionMode.MultiSelection)
            for i in range(1, num_clusters + 1):
                list_widget.addItem(f"Cluster {i}")
            layout.addWidget(list_widget)

            button_box = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel
                                          , parent=dialog)
            layout.addWidget(button_box)
            button_box.accepted.connect(dialog.accept)
            button_box.rejected.connect(dialog.reject)

            result = dialog.exec()
            if result == QDialog.DialogCode.Accepted:
                selected_items = [int(item.text().split()[-1]) for item in list_widget.selectedItems()]
                return selected_items
            else:
                return None
        except Exception as e:
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    # Function to update the menu item text based on the current view type
    def update_menu_action_text(self, current_index):
        if self.view_type.get(current_index) == '2D':
            self.toggle_canvas_action.setText("3D View")
        else:
            self.toggle_canvas_action.setText("2D View")

    def toggle_canvas_from_menu(self):
        current_index = self.tab_widget.currentIndex()
        if self.view_type[current_index] == '2D':
            self.toggle_canvas('3D')
        else:
            self.toggle_canvas('2D')

    def apply_grid_color(self):
        color = QColorDialog.getColor(options=QColorDialog.ColorDialogOption.ShowAlphaChannel, parent=self)
        # Get the index of the currently active tab
        active_tab_index = self.tab_widget.currentIndex()

        # Retrieve the widget of the currently active tab
        active_tab_widget = self.tab_widget.widget(active_tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()
        if color.isValid():
            for active_tab_index in self.grid_color.keys():
                self.grid_color[active_tab_index] = color
                # Iterate over all keys in self.grids and set the color
                for grid_key in self.grids.keys():
                    cube_axes = self.grids[grid_key]
                    if isinstance(color, str):  # If it's a hex color
                        qcolor = QColor(color)
                        color = (
                            qcolor.red() / 255.0,
                            qcolor.green() / 255.0,
                            qcolor.blue() / 255.0,
                        )
                    elif isinstance(color, QColor):  # If it's a QColor object
                        color = (
                            color.red() / 255.0,
                            color.green() / 255.0,
                            color.blue() / 255.0,
                        )
                    # Set the color for all cube axes properties
                    cube_axes.GetXAxesLinesProperty().SetColor(color)
                    cube_axes.GetYAxesLinesProperty().SetColor(color)
                    cube_axes.GetZAxesLinesProperty().SetColor(color)
                    cube_axes.GetXAxesGridlinesProperty().SetColor(color)
                    cube_axes.GetYAxesGridlinesProperty().SetColor(color)
                    cube_axes.GetZAxesGridlinesProperty().SetColor(color)

                    # Title text colors
                    cube_axes.GetTitleTextProperty(0).SetColor(color)  # X axis title
                    cube_axes.GetTitleTextProperty(1).SetColor(color)  # Y axis title
                    cube_axes.GetTitleTextProperty(2).SetColor(color)  # Z axis title

                    # Tick mark labels (numbers)
                    cube_axes.GetLabelTextProperty(0).SetColor(color)  # X axis numbers
                    cube_axes.GetLabelTextProperty(1).SetColor(color)  # Y axis numbers
                    cube_axes.GetLabelTextProperty(2).SetColor(color)  # Z axis numbers

            # Redraw the render window
            canvas.GetRenderWindow().Render()
            self.last_grid_color = color

    def set_foreground_color(self, color, tab_index):

        # Retrieve the widget of the currently active tab
        tab_widget = self.tab_widget.widget(tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        # Check if the active tab index exists in the dictionary
        if tab_index in self.grids:
            cube_axes = self.grids[tab_index]  # Get the cube axes actor

            if isinstance(color, str):  # If it's a hex color
                qcolor = QColor(color)
                color = (
                    qcolor.red() / 255.0,
                    qcolor.green() / 255.0,
                    qcolor.blue() / 255.0,
                )
            elif isinstance(color, QColor):  # If it's a QColor object
                color = (
                    color.red() / 255.0,
                    color.green() / 255.0,
                    color.blue() / 255.0,
                )
            # Set the color for all cube axes properties
            cube_axes.GetXAxesLinesProperty().SetColor(color)
            cube_axes.GetYAxesLinesProperty().SetColor(color)
            cube_axes.GetZAxesLinesProperty().SetColor(color)
            cube_axes.GetXAxesGridlinesProperty().SetColor(color)
            cube_axes.GetYAxesGridlinesProperty().SetColor(color)
            cube_axes.GetZAxesGridlinesProperty().SetColor(color)

            # Title text colors
            cube_axes.GetTitleTextProperty(0).SetColor(color)  # X axis title
            cube_axes.GetTitleTextProperty(1).SetColor(color)  # Y axis title
            cube_axes.GetTitleTextProperty(2).SetColor(color)  # Z axis title

            # Tick mark labels (numbers)
            cube_axes.GetLabelTextProperty(0).SetColor(color)  # X axis numbers
            cube_axes.GetLabelTextProperty(1).SetColor(color)  # Y axis numbers
            cube_axes.GetLabelTextProperty(2).SetColor(color)  # Z axis numbers

        # Retrieve and update the color bar, if it exists
        renderer = canvas.renderer
        for i in range(renderer.GetActors2D().GetNumberOfItems()):
            actor = renderer.GetActors2D().GetItemAsObject(i)
            if isinstance(actor, vtk.vtkScalarBarActor):
                actor.GetTitleTextProperty().SetColor(color)
                actor.GetLabelTextProperty().SetColor(color)

        if hasattr(canvas, 'arrow_assembly') and canvas.arrow_assembly is not None:
            arrow_assembly = canvas.arrow_assembly
            parts = arrow_assembly.GetParts()
            if parts.GetNumberOfItems() == 2:
                actor1 = parts.GetItemAsObject(0)
                actor2 = parts.GetItemAsObject(1)

                # Correctly retrieve the background color from the renderer
                bg_color = list(renderer.GetBackground())

                # Convert background to HSV
                h_bg, s_bg, v_bg = colorsys.rgb_to_hsv(*bg_color[:3])

                # Determine if the background is near-white or near-gray
                is_light_bg = v_bg > 0.5
                is_gray_bg = s_bg < 0.2

                # Generate two highly contrasting colors
                if is_gray_bg:
                    # For gray/white backgrounds, use green and red
                    # These colors are 180° apart on the color wheel (complementary)
                    color1_hue = 0.33  # Green
                    color2_hue = 0.0  # Red
                else:
                    # For colored backgrounds, use colors 120° apart from background
                    # and from each other (triadic color scheme)
                    color1_hue = (h_bg + 0.33) % 1.0  # 120° from background
                    color2_hue = (h_bg + 0.66) % 1.0  # 240° from background

                # Set high saturation for vibrant colors
                saturation = 0.9

                # Adjust brightness based on background
                if is_light_bg:
                    # For light backgrounds, use darker colors
                    value1 = 0.4
                    value2 = 0.6
                else:
                    # For dark backgrounds, use brighter colors
                    value1 = 0.7
                    value2 = 0.9

                # Convert to RGB
                color1 = colorsys.hsv_to_rgb(color1_hue, saturation, value1)
                color2 = colorsys.hsv_to_rgb(color2_hue, saturation, value2)

                # Check if color1 is redder than green and color2 is greener than red
                is_color1_reddish = color1[0] > color1[1]  # Redder than green
                is_color2_greenish = color2[1] > color2[0]  # Greener than red

                if is_color1_reddish and is_color2_greenish:
                    color1, color2 = color2, color1

                # Apply colors to actors
                actor1.GetProperty().SetColor(color2)
                actor2.GetProperty().SetColor(color1)

        # Redraw the render window
        canvas.GetRenderWindow().Render()

    def apply_custom_stylesheet(self, widget, percentage):
        # Define color transitions based on usage
        if percentage < 50:
            color = 'green'
        elif 50 <= percentage < 80:
            color = 'orange'
        else:
            color = 'red'

        # Apply styles using object name for specificity
        widget.setStyleSheet(f"""
        QProgressBar#memoryProgressBar {{
            border: 1px solid #000;  /* Ensure border is visible */
            border-radius: 2px;  /* Round the corners */
            background: #ccc;  /* Base color for the bar */
        }}
        QProgressBar#memoryProgressBar::chunk {{
            background-color: {color};
            width: 2px;  /* Width of each bar */
            margin: 1px;  /* Space between bars */
        }}
        """)

    def update_memory_usage(self):
        memory_info = psutil.virtual_memory()
        used_memory = memory_info.used / (1024 ** 3)
        total_memory = memory_info.total / (1024 ** 3)
        percentage_used = int((used_memory / total_memory) * 100)

        # Update progress bar
        self.memory_progress_bar.setValue(percentage_used)
        self.apply_custom_stylesheet(self.memory_progress_bar, percentage_used)

        # Update tooltip
        tooltip_text = f"Used: {used_memory:.2f} GB\nTotal: {total_memory:.2f} GB\nPercentage: {percentage_used}%"

        self.status_bar_widget.setToolTip(tooltip_text)

        QTimer.singleShot(1000, self.update_memory_usage)

    def update_status_bar_color(self):
        if self.isDarkTheme:
            self.statusBar().setStyleSheet(
                "background-color: {darkcolor1}; color: {darkcolor6};".format(
                    darkcolor1=self.darkColor1,
                    darkcolor6=self.darkColor6
                )
            )

        else:
            self.statusBar().setStyleSheet(
                "background-color: {lightcolor1}; color: {lightcolor3};".format(
                    lightcolor1=self.light_color_1,
                    lightcolor3=self.light_color_3
                )
            )

    def canvas_color(self):
        color = QColorDialog.getColor(parent=self)
        if color.isValid():

            self.last_grid_color = self.adjust_grid_color(color)
            self.last_selected_color = color

            for i in range(self.tab_widget.count()):
                # Retrieve the widget of the current tab
                tab_widget = self.tab_widget.widget(i)
                # Assuming the canvas is the first widget in the layout of the tab
                canvas_layout = tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()
                if isinstance(canvas, CustomVTKWidget):  # Check if it's your custom class
                    canvas.setBackgroundColor(color)

            for active_tab_index in self.grid_color.keys():
                self.grid_color[active_tab_index] = self.last_grid_color
                # Iterate over all keys in self.grids and set the color
                for grid_key in self.grids.keys():
                    self.set_foreground_color(self.last_grid_color, grid_key)

            for i in range(self.tab_widget.count()):
                # Retrieve the widget of the current tab
                tab_widget = self.tab_widget.widget(i)

                # Assuming the canvas is the first widget in the layout of the tab
                canvas_layout = tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()
                if isinstance(canvas, RoundedCanvas):
                    # Set the background color for the canvas
                    canvas.setBackground(color.name())

    def toggle_grid(self):
        self.grid_active = not self.grid_active
        # Set the text for the grid action based on its active state
        self.grid_action.setText("Deactivate Axis" if self.grid_active else "Activate Axis")

        for key, value in self.grids.items():
            if value is not None:
                value.SetVisibility(self.grid_active)

        current_canvas = self.tab_widget.currentWidget().layout().itemAt(0).widget()

        if hasattr(current_canvas, 'GetRenderWindow'):
            current_canvas.GetRenderWindow().Render()

    def toggle_compass(self):
        self.compass_active = not self.compass_active
        self.compass_action.setText("Deactivate Compass" if self.compass_active else "Activate Compass")

        # Iterate through all tabs
        for index in range(self.tab_widget.count()):
            tab_widget = self.tab_widget.widget(index)
            if not tab_widget:
                continue

            # Get the canvas from the tab's layout
            canvas_layout = tab_widget.layout()
            if canvas_layout and canvas_layout.count() > 0:
                canvas = canvas_layout.itemAt(0).widget()

                # Update compass visibility for each canvas
                if hasattr(canvas, 'arrow_assembly') and hasattr(canvas, 'orientation_marker'):
                    canvas.arrow_assembly.SetVisibility(self.compass_active)
                    canvas.orientation_marker.SetEnabled(self.compass_active)

        current_canvas = self.tab_widget.currentWidget().layout().itemAt(0).widget()

        if hasattr(current_canvas, 'GetRenderWindow'):
            current_canvas.GetRenderWindow().Render()

    def set_shade_threshold(self):
        """
            Display an input dialog for percentage (0-100) and assign the
            decimal value (0-1.0) to self.shade_threshold
            """
        # Open input dialog with range from 0 to 100
        percentage, ok_pressed = QInputDialog.getInt(
            self,  # parent
            "Set Shade Threshold",  # window title
            "Enter threshold percentage (0-100):",  # label
            int(self.shade_threshold * 100),  # default value
            0,  # minimum value
            100,  # maximum value
            1  # step
        )

        # Check if user pressed OK
        if ok_pressed:
            # Convert percentage (0-100) to decimal (0.0-1.0)
            self.shade_threshold = percentage / 100.0

    def set_cell_mesh(self):
        new_state = not self.cell_mesh
        self.cell_mesh = new_state
        if new_state:  # Only deactivate point mesh when activating cell mesh
            self.point_mesh = False
        # Update both UI elements to reflect current states
        self.cell_mesh_action.setText("Deactivate Cell Mesh" if self.cell_mesh else "Activate Cell Mesh")
        self.point_mesh_action.setText("Deactivate Point Mesh" if self.point_mesh else "Activate Point Mesh")

    def set_point_mesh(self):
        new_state = not self.point_mesh
        self.point_mesh = new_state
        if new_state:  # Only deactivate cell mesh when activating point mesh
            self.cell_mesh = False
        # Update both UI elements to reflect current states
        self.point_mesh_action.setText("Deactivate Point Mesh" if self.point_mesh else "Activate Point Mesh")
        self.cell_mesh_action.setText("Deactivate Cell Mesh" if self.cell_mesh else "Activate Cell Mesh")

    def toggle_three_d_color_bar(self):
        # Toggle the state
        self.three_d_color_bar_active = not self.three_d_color_bar_active

        # Update the action text
        self.toggle_three_d_color_bar_action.setText(
            "Deactivate Color Bar" if self.three_d_color_bar_active else "Activate Color Bar"
        )

        # Iterate through all tabs
        for i in range(self.tab_widget.count()):
            tab_widget = self.tab_widget.widget(i)  # Get the widget in the tab

            # Check if the tab has a layout
            layout = tab_widget.layout()
            if layout and layout.count() > 0:
                canvas = layout.itemAt(0).widget()  # Get the first widget in the layout

                # Ensure the widget has the expected attribute
                if isinstance(canvas, CustomVTKWidget):

                    current_items = self.tab_volume_items.get(i, [])

                    # Step 2: Hide all color bars
                    for item in current_items:
                        if hasattr(item, 'color_bar'):
                            if item.color_bar.GetVisibility():
                                item.color_bar.SetVisibility(0)

                    # Step 3: If no color bar was initially visible, show the color bar of the last visible item
                    if self.three_d_color_bar_active:
                        for item in reversed(current_items):  # Start from the last item
                            if (
                                    (hasattr(item, 'GetEnabled') and item.GetEnabled()) or
                                    (hasattr(item, 'GetVisibility') and item.GetVisibility())
                            ):
                                if hasattr(item, 'color_bar'):
                                    item.color_bar.SetVisibility(1)
                                    break

                    canvas.GetRenderWindow().Render()

    def choose_dark_mode_times(self):
        dialog = QDialog(self)
        layout = QVBoxLayout(dialog)

        # Create time edit widgets with 24-hour format
        start_time_edit = QTimeEdit(dialog)
        start_time_edit.setDisplayFormat("HH:mm")  # 24-hour format
        end_time_edit = QTimeEdit(dialog)
        end_time_edit.setDisplayFormat("HH:mm")  # 24-hour format

        layout.addWidget(start_time_edit)
        layout.addWidget(end_time_edit)

        # Create QDialogButtonBox for OK and Cancel
        button_box = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel
                                      , parent=dialog)
        button_box.accepted.connect(dialog.accept)
        button_box.rejected.connect(dialog.reject)
        layout.addWidget(button_box)

        dialog.setWindowTitle("Choose Dark Mode Schedule")

        # Execute the dialog and check the result
        if dialog.exec() == QDialog.DialogCode.Accepted:
            # Save the chosen times
            self.dark_mode_start_time = start_time_edit.time()
            self.dark_mode_end_time = end_time_edit.time()
            # Update the theme based on the chosen times
            self.update_theme_based_on_time()
            self.update_status_bar_color()

    def update_theme_based_on_time(self):

        current_time = QTime.currentTime()
        # Handle dark mode times that span midnight
        if ((self.dark_mode_start_time < self.dark_mode_end_time and
             current_time >= self.dark_mode_start_time and
             current_time < self.dark_mode_end_time) or
                (self.dark_mode_start_time > self.dark_mode_end_time and
                 (current_time >= self.dark_mode_start_time or
                  current_time < self.dark_mode_end_time))):
            self.isDarkTheme = True
            self.apply_dark_theme()
            self.update_status_bar_color()
        else:
            self.isDarkTheme = False
            self.apply_light_theme()
            self.update_status_bar_color()

    def set_default_shades(self):

        self.light_color_1 = "#B2B2B2"
        self.light_color_2 = "#CACACA"
        self.light_color_3 = "#333333"
        self.light_color_4 = "#EDEDED"
        self.light_color_5 = "#F0F0F0"
        self.light_color_6 = "#D9D9D9"
        self.light_color_7 = "#E8E8E8"
        self.light_color_8 = "#5C5C5C"
        self.light_color_9 = "#999999"
        self.light_color_10 = "#353535"
        # Define the unique colors used in the stylesheet
        self.darkColor1 = "#353535"
        self.darkColor2 = "#404040"
        self.darkColor3 = "#5C5C5C"
        self.darkColor4 = "#FFFFFF"
        self.darkColor5 = "white"  # alias for #FFFFFF
        self.darkColor6 = "Gainsboro"
        self.darkColor7 = "#555555"  # alias for #555
        self.darkColor8 = "#B2B2B2"
        self.darkColor9 = "#4C4C4C"
        self.darkColor10 = "#999999"
        self.darkColor11 = "#E8E8E8"
        self.darkColor12 = "#333333"  # alias for #333
        self.darkColor13 = "#DDDDDD"  # alias for #ddd

        self.update_theme_based_on_time()

    # 1. Calculate luminance of a color
    def luminance(self, hex_color):
        """Calculate the luminance of a color."""
        color = QColor(hex_color)
        r, g, b = color.redF(), color.greenF(), color.blueF()
        return 0.2126 * r + 0.7152 * g + 0.0722 * b

    # 2. Adjust the lightness of a base color to match the target luminance
    def adjust_lightness(self, base_color, target_luminance):
        """Adjust the lightness of base_color to match the target_luminance."""
        base_qcolor = QColor(base_color)
        h, s, l, a = base_qcolor.getHslF()  # Unpack all four values: hue, saturation, lightness, and alpha

        low, high = 0, 1
        for _ in range(100):  # Binary search for the correct luminance match
            l = (low + high) / 2
            candidate_color = QColor.fromHslF(h, s, l, a)  # Include the alpha channel in the color creation
            candidate_luminance = self.luminance(candidate_color.name())
            if candidate_luminance < target_luminance:
                low = l
            else:
                high = l

        return candidate_color.name()

    # 3. Generate a new palette based on the selected base color and reference luminances
    def generate_palette(self, selected_color, reference_luminance):
        """Generate a new palette based on the selected base color."""
        new_palette = {}
        for name, target_luminance in reference_luminance.items():
            new_palette[name] = self.adjust_lightness(selected_color, target_luminance)
        return new_palette

    # 4. Apply the new palette to the existing color references
    def apply_new_palette(self, new_palette):
        """Assign new colors to the references in the object."""
        for name, color in new_palette.items():
            setattr(self, name, color)

        self.update_theme_based_on_time()

    # 5. Open the QColorDialog to let the user pick a color
    def pick_color(self):
        """Open a QColorDialog to let the user select a color."""
        color = QColorDialog.getColor(initial=QColor('#0000FF'), parent=self,
                                      options=QColorDialog.ColorDialogOption.ShowAlphaChannel)

        if color.isValid():
            return color.name()
        return None

    # 6. Main function to be triggered
    def update_colors_based_on_selection(self):
        """Main function to update the color references based on user selection."""
        # Get the reference luminance values
        reference_luminance = {name: self.luminance(getattr(self, name)) for name in dir(self) if
                               name.startswith(('light_color', 'darkColor'))}

        # Let the user pick a color
        selected_color = self.pick_color()

        if selected_color:
            # Generate a new palette based on the selected color
            new_palette = self.generate_palette(selected_color, reference_luminance)

            # Apply the new palette to the class attributes
            self.apply_new_palette(new_palette)

    def apply_light_theme(self):
        if self.gpt_button is not None:
            self.gpt_button.setIcon(QIcon("blackgpt.svg"))
            self.gpt_button.setIconSize(QSize(14, 14))
        if self.terminal_button is not None:
            self.terminal_button.setIcon(QIcon("black_terminal.png"))
            self.terminal_button.setIconSize(QSize(14, 14))
        # Set the light theme styles using the provided colors
        style_sheet = """
        QMainWindow {{
            background-color: {light_color_1}; /* Background color */
        }}
        QMenuBar {{
            background-color: {light_color_2}; /* Light grey */
            color: {light_color_3}; /* Dark grey text */
        }}
        QMenuBar::item:selected {{
            background-color: {light_color_1}; /* Slightly darker grey */
            color: {light_color_3}; /* Dark grey text */
        }}
        QDockWidget {{
            background-color: {light_color_1};  /* Change to your desired hover background color */
        }}
        QGroupBox {{
            color: {light_color_3};  /* Change to your desired text color */
        }}
        QDockWidget::close-button {{
            background-color: transparent;  /* Change to your desired hover background color */
            border: none;  /* No border */
            color: {light_color_3};             /* Change to your desired hover text/icon color */
        }}
        QDockWidget::float-button {{
            background-color: transparent;  /* Change to your desired hover background color */
            border: none;  /* No border */
            color: {light_color_3};             /* Change to your desired hover text/icon color */
        }}
        QToolButton {{
            background-color: transparent;  /* No background */
            border: none;  /* No border */
            color: {light_color_3};  /* Color of the '+' symbol */
            font-size: 20px;  /* Optional: Adjust the size of the '+' symbol */
        }}
        QToolButton:hover {{
            background-color: transparent;  /* No background on hover */
            border: none;  /* No border on hover */
        }}
        QPushButton {{
            background-color: {light_color_4}; /* Button background */
            color: {light_color_3}; /* Button text */
        }}
        QPushButton:hover {{
            background-color: {light_color_1}; /* Hover effect */
        }}
        QLabel {{
            color: {light_color_3}; /* Label text color */
        }}
        QMenu {{
            background-color: {light_color_5}; /* Light grey */
            color: {light_color_3}; /* Menu text */
        }}
        QMenu::item {{
            background-color: {light_color_5}; /* Light grey */
            color: {light_color_3}; /* Menu item text */
        }}
        QMenu::item:selected {{
            background-color: {light_color_1}; /* Slightly darker grey */
            color: {light_color_3}; /* Consistent text color */
        }}
        QTabWidget::pane {{
            background-color: {light_color_4}; /* Pane background */
            border-style: solid;
            border-color: {light_color_4}; /* Pane border */
            border-width: 0 1px 1px 1px; /* Border width */
        }}
        QTabBar::tab:selected {{
            background: {light_color_4}; /* Selected tab */
            border-style: solid;
            border-color: {light_color_4}; /* Selected tab border */
            border-width: 1px 1px 0 1px; /* Border width */
            border-top-left-radius: 3px; /* Corner rounding */
            border-top-right-radius: 3px; /* Corner rounding */
            padding: 4px; /* Padding */
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3); /* Shadow */
        }}
        QTabBar::tab {{
            background: {light_color_6}; /* Tab background */
            color: {light_color_3}; /* Tab text */
            border-style: solid;
            border-color: {light_color_6}; /* Tab border */
            border-width: 1px 1px 0 1px; /* Border width */
            padding: 4px; /* Padding */
            border-top-left-radius: 3px; /* Corner rounding */
            border-top-right-radius: 3px; /* Corner rounding */
            box-shadow: none; /* No shadow */
        }}
        QTabBar::scroller {{
            background: {light_color_7}; /* Scroller background */
        }}
        QTabBar QToolButton {{
            background: {light_color_7}; /* Tool button background */
            border: none; /* No border */
            padding: 3px; /* Padding */
        }}
        QTabBar QToolButton:hover {{
            background-color: {light_color_1}; /* Hover effect */
            color: {light_color_3}; /* Hover text color */
        }}
        QTabBar::tear {{
            background: {light_color_5}; /* Tear background */
        }}
        QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QLineEdit {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
            border-radius: 2px; /* Corner rounding */
            height: 20px; /* Height */
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QLineEdit:focus {{
            border: 1px solid {light_color_8}; /* Focus border */
            border-radius: 2px; /* Corner rounding */
        }}
        QComboBox {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
            border-radius: 2px; /* Corner rounding */
            height: 20px; /* Height */
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QComboBox:focus {{
            border: 1px solid {light_color_8}; /* Focus border */
            border-radius: 2px; /* Corner rounding */
        }}
        QComboBox QAbstractItemView {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QTreeWidget {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
        }}
        QTreeWidget::item {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
        }}
        QTreeWidget::item:selected {{
            background-color: {light_color_2}; /* Selection background */
            color: {light_color_3}; /* Selection text color */
        }}
        QTableView {{
            background-color: {light_color_5}; /* Background color */
            color: {light_color_3}; /* Text color */
            border: 1px solid {light_color_8}; /* Border color */
            gridline-color: {light_color_8}; /* Gridline color */
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QTableView QTableCornerButton::section {{
            background-color: {light_color_5}; /* Corner button background */
            border: 1px solid {light_color_8}; /* Corner button border */
        }}
        QHeaderView::section {{
            background-color: {light_color_7}; /* Header section background */
            color: {light_color_3}; /* Header section text */
        }}
        QTreeWidget QMenu {{
            background-color: {light_color_5}; /* Menu background */
            color: {light_color_3}; /* Menu text */
        }}
        QTreeWidget QMenu::item:selected {{
            background-color: {light_color_2}; /* Selection background */
            color: {light_color_3}; /* Selection text color */
        }}
        QDialog {{
            background-color: {light_color_1}; /* Background color */
        }}
        QAbstractItemView {{
            selection-background-color: {light_color_2}; /* Selection background */
            selection-color: {light_color_3}; /* Selection text color */
        }}
        QSlider::groove:horizontal {{
            border: 0px solid {light_color_9}; /* Groove border */
            height: 1px; /* Groove height */
            background: {light_color_7}; /* Groove background */
            margin: 1px 0; /* Groove margin */
            border-radius: 5px; /* Groove rounding */
        }}
        QSlider::handle:horizontal {{
            background: {light_color_10}; /* Handle background */
            border: 0px solid {light_color_7}; /* Handle border */
            height: 18px; /* Handle height */
            width: 10px; /* Handle width */
            margin: -5px 0; /* Handle margin */
            border-radius: 3px; /* Handle rounding */
        }}
        QSlider::add-page:horizontal {{
            background: {light_color_2}; /* Filled groove */
        }}
        QSlider::sub-page:horizontal {{
            background: {light_color_10}; /* Unfilled groove */
        }}
        QProgressBar {{
            border: 2px solid {light_color_8}; /* Progress bar border */
            border-radius: 5px; /* Progress bar rounding */
            background-color: {light_color_8}; /* Progress bar background */
            text-align: center; /* Centered text */
        }}
        QProgressBar::chunk {{
            background-color: {light_color_8}; /* Progress bar chunk */
        }}
        QScrollArea {{
            background-color: {light_color_5}; /* Scroll area background */
            color: {light_color_3}; /* Scroll area text */
        }}
        QScrollArea QWidget {{
            background-color: {light_color_5}; /* Content background */
            color: {light_color_3}; /* Content text */
        }}
        QToolTip {{
            background-color: {light_color_5}; /* Tooltip background */
            color: {light_color_3}; /* Tooltip text */
            border: 0px solid {light_color_5}; /* Tooltip border */
            border-radius: 0px; /* Tooltip rounding */
            padding: 0px; /* Tooltip padding */
            font-weight: bold; /* Tooltip font weight */
        }}
        QScrollBar:vertical {{
            background-color: {light_color_5}; /* Scrollbar background */
        }}
        QScrollBar:horizontal {{
            background-color: {light_color_5}; /* Scrollbar background */
        }}
        QListWidget {{
            background-color: {light_color_5}; /* Light grey background */
            color: {light_color_3}; /* Dark grey text */
        }}
        QListWidget::item:selected {{
            background-color: {light_color_2}; /* Slightly darker grey for selected item */
            color: {light_color_3}; /* Dark grey text for consistency */
        }}
        QTextEdit, QPlainTextEdit {{
            background-color: {light_color_5};
            color: {light_color_3};
            selection-background-color: {light_color_2};
            selection-color: {light_color_3};
        }}
        """.format(
            light_color_1=self.light_color_1,
            light_color_2=self.light_color_2,
            light_color_3=self.light_color_3,
            light_color_4=self.light_color_4,
            light_color_5=self.light_color_5,
            light_color_6=self.light_color_6,
            light_color_7=self.light_color_7,
            light_color_8=self.light_color_8,
            light_color_9=self.light_color_9,
            light_color_10=self.light_color_10
        )
        self.setStyleSheet(style_sheet)
        stylesheet2 = """
                    QWidget {{
                        background-color: {light_color_2};
                        color: {light_color_3};
                    }}
                    QListWidget {{
                        background-color: {light_color_5}; /* Light grey background */
                        color: {light_color_3}; /* Dark grey text */
                    }}
                    QListWidget::item:selected {{
                        background-color: {light_color_2}; /* Slightly darker grey for selected item */
                        color: {light_color_3}; /* Dark grey text for consistency */
                    }}
                    QTreeWidget {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                    }}
                    QTreeWidget::item {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                    }}
                    QTreeWidget::item:selected {{
                        background-color: {light_color_2}; /* Selection background */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QTreeWidget QMenu {{
                        background-color: {light_color_5}; /* Menu background */
                        color: {light_color_3}; /* Menu text */
                    }}
                    QTreeWidget QMenu::item:selected {{
                        background-color: {light_color_2}; /* Selection background */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QLineEdit {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        border-radius: 2px; /* Corner rounding */
                        height: 20px; /* Height */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QLineEdit:focus {{
                        border: 1px solid {light_color_8}; /* Focus border */
                        border-radius: 2px; /* Corner rounding */
                    }}
                    QComboBox {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        border-radius: 2px; /* Corner rounding */
                        height: 20px; /* Height */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QComboBox:focus {{
                        border: 1px solid {light_color_8}; /* Focus border */
                        border-radius: 2px; /* Corner rounding */
                    }}
                    QComboBox QAbstractItemView {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QDialog {{
                        background-color: {light_color_1}; /* Background color */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QAbstractItemView {{
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QScrollArea {{
                        background-color: {light_color_5}; /* Scroll area background */
                        color: {light_color_3}; /* Scroll area text */
                    }}
                    QScrollArea QWidget {{
                        background-color: {light_color_5}; /* Content background */
                        color: {light_color_3}; /* Content text */
                    }}
                    QScrollBar:vertical {{
                        background-color: {light_color_5}; /* Scrollbar background */
                    }}
                    QScrollBar:horizontal {{
                        background-color: {light_color_5}; /* Scrollbar background */
                    }}
                    """.format(
            light_color_1=self.light_color_1,
            light_color_2=self.light_color_2,
            light_color_3=self.light_color_3,
            light_color_4=self.light_color_4,
            light_color_5=self.light_color_5,
            light_color_6=self.light_color_6,
            light_color_7=self.light_color_7,
            light_color_8=self.light_color_8,
            light_color_9=self.light_color_9,
            light_color_10=self.light_color_10,
        )

        # List of dialog variables
        dialogs = [
            self.export_plot_spec_dialog,
            self.export_petro_dialog,
            self.export_well_dialog,
            self.export_dialog
        ]

        # Apply condition to each dialog
        for dialog in dialogs:
            if dialog:
                dialog.setStyleSheet(stylesheet2)

    def apply_dark_theme(self):
        if self.gpt_button is not None:
            self.gpt_button.setIcon(QIcon("whitegpt.png"))
            self.gpt_button.setIconSize(QSize(14, 14))
        if self.terminal_button is not None:
            self.terminal_button.setIcon(QIcon("white_terminal.png"))
            self.terminal_button.setIconSize(QSize(14, 14))

        # Set the dark theme styles
        style_sheet = """
        QTextEdit, QPlainTextEdit {{
            background-color: {darkcolor1};
            color: {darkcolor5};
            selection-background-color: {darkcolor2};
            selection-color: {darkcolor4};
        }}
        QMainWindow {{
            background-color: {darkcolor1};
        }}
        QMenuBar {{
            background-color: {darkcolor2};
            color: {darkcolor4};
        }}
        QMenuBar::item:selected {{
            background-color: {darkcolor3};
            color: {darkcolor5};
        }}
        QDockWidget {{
            background-color: {darkcolor1};  /* Change to your desired hover background color */
        }}
        QGroupBox {{
            color: {darkcolor5};  /* Change to your desired text color */
        }}
        QDockWidget::close-button {{
            background-color: transparent;  /* Change to your desired hover background color */
            border: none;  /* No border */
            color: {darkcolor5};             /* Change to your desired hover text/icon color */
        }}
        QDockWidget::float-button {{
            background-color: transparent;  /* Change to your desired hover background color */
            border: none;  /* No border */
            color: {darkcolor5};             /* Change to your desired hover text/icon color */
        }}
        QToolButton {{
            background-color: transparent;  /* No background */
            border: none;  /* No border */
            color: {darkcolor5};  /* Color of the '+' symbol */
            font-size: 20px;  /* Optional: Adjust the size of the '+' symbol */
        }}
        QToolButton:hover {{
            background-color: transparent;  /* No background on hover */
            border: none;  /* No border on hover */
        }}
        QPushButton {{
            background-color: {darkcolor2};
            color: {darkcolor5};
        }}
        QPushButton:hover {{
            background-color: {darkcolor3};
        }}
        QLabel {{
            color: {darkcolor6};
        }}
        QMenu {{
            background-color: {darkcolor2};
            color: {darkcolor4};
        }}
        QMenu::item {{
            background-color: {darkcolor2};
            color: {darkcolor4};
        }}
        QMenu::item:selected {{
            background-color: {darkcolor3};
        }}
        QTabWidget::pane {{
            background-color: {darkcolor2}; /* Choose from the pane background color options */
            border-style: solid;
            border-color: {darkcolor2};
            border-width: 0 1px 1px 1px; /* Top, Right, Bottom, Left */
        }}
        QTabBar::tab:selected {{
            background: {darkcolor2};
            border-style: solid;
            border-color: {darkcolor2};
            border-width: 1px 1px 0 1px; /* Top, Right, Bottom, Left */
            border-top-left-radius: 3px; /* Rounds the top-left corner */
            border-top-right-radius: 3px; /* Rounds the top-right corner */
            padding: 4px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3);
        }}
        QTabBar::tab {{
            background: {darkcolor3};
            color: {darkcolor5};
            border-style: solid;
            border-color: {darkcolor3};
            border-width: 1px 1px 0 1px; /* Top, Right, Bottom, Left */
            padding: 4px;
            border-top-left-radius: 3px; /* Rounds the top-left corner */
            border-top-right-radius: 3px; /* Rounds the top-right corner */
            box-shadow: none;
        }}
        QTabBar::scroller {{ /* The scroller sub-control */
            background: {darkcolor2}; /* Change the color as desired */
        }}
        QTabBar QToolButton {{ /* Targeting the scroll buttons specifically */
            background: {darkcolor2}; /* Change the color as desired */
            border: solid; /* Optional: removes the border */
            padding: 3px;
        }}
        QTabBar QToolButton:hover {{
            background-color: {darkcolor7}; /* Change to your desired hover background color */
            color: {darkcolor13}; /* Change to your desired hover text color */
            /* Additional styling properties as needed */
        }}
        QTabBar::tear {{
            background: {darkcolor2}; /* Set your desired color */
        }}
        QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
            background-color: {darkcolor1};
            color: {darkcolor5};
            selection-background-color: {darkcolor2};
            selection-color: {darkcolor4};
        }}
        QLineEdit {{
            background-color: {darkcolor3};
            color: {darkcolor5};
            border-radius: 2px;
            height: 20px;
            selection-background-color: {darkcolor2};
            selection-color: {darkcolor4};
        }}
        QLineEdit:focus {{ 
            border: 1px solid {darkcolor8}; 
            border-radius: 2px; /* Rounded corners for the progress bar */
        }}
        QComboBox {{
            background-color: {darkcolor3};
            color: {darkcolor5};
            border-radius: 2px;
            height: 20px;
            selection-background-color: {darkcolor2};
            selection-color: {darkcolor4};
        }}
        QComboBox:focus {{ 
            border: 1px solid {darkcolor8}; 
            border-radius: 2px; /* Rounded corners for the progress bar */
        }}
        QComboBox QAbstractItemView {{
            background-color: {darkcolor3}; /* Same as the combo box background */
            color: {darkcolor5}; /* Text color */
            selection-background-color: {darkcolor9}; /* Background color when an item is selected */
            selection-color: {darkcolor5}; /* Text color when an item is selected */
        }}
        QTreeWidget {{
            background-color: {darkcolor2};
            color: {darkcolor5};
        }}
        QTreeWidget::item {{
            background-color: {darkcolor2};
            color: {darkcolor5};
        }}
        QTreeWidget::item:selected {{
            background-color: {darkcolor3};
            color: {darkcolor5};
        }}
        QTableView {{
            background-color: {darkcolor2};
            color: {darkcolor5};
            border: 1px solid {darkcolor8};
            gridline-color: {darkcolor8};
            selection-background-color: {darkcolor9}; /* Background color when a row is selected */
            selection-color: {darkcolor5}; /* Text color when a row is selected */
        }}
        QTableView QTableCornerButton::section {{
            background-color: {darkcolor2};
            border: 1px solid {darkcolor8};
        }}
        QHeaderView::section {{
            background-color: {darkcolor2};
            color: {darkcolor5};
        }}
        QTreeWidget QMenu {{
            background-color: {darkcolor2};
            color: {darkcolor5};
        }}
        QTreeWidget QMenu::item:selected {{
            background-color: {darkcolor3};
            color: {darkcolor5};
        }}
        QDialog {{
            background-color: {darkcolor1};
        }}
        QAbstractItemView {{
            selection-background-color: {darkcolor3};
            selection-color: {darkcolor5};
        }}
        /* Style for QSlider groove */
        QSlider::groove:horizontal {{
            border: 0px solid {darkcolor10};
            height: 1px; /* Set the height of the groove */
            background: {darkcolor11}; /* Light grey background for the unfilled part */
            margin: 1px 0;
            border-radius: 5px; /* Rounded corners for the handle */
        }}

        /* Style for QSlider handle */
        QSlider::handle:horizontal {{
            background: {darkcolor8}; /* Slightly darker grey for the handle */
            border: 0px solid {darkcolor3}; /* Border color for the handle */
            height: 18px; /* Taller handle */
            width: 10px; /* Slimmer handle */
            margin: -5px 0; /* Expand outside the groove */
            border-radius: 3px; /* Rounded corners for the handle */
        }}

        /* Style for QSlider filled part */
        QSlider::add-page:horizontal {{
            background: {darkcolor3}; /* Slightly darker grey for the filled part */
        }}

        /* Style for QSlider unfilled part */
        QSlider::sub-page:horizontal {{
            background: {darkcolor8}; /* Light grey background for the unfilled part */
        }}

        QProgressBar {{
            border: 2px solid {darkcolor3}; /* Light grey border */
            border-radius: 5px; /* Rounded corners for the progress bar */
            background-color: {darkcolor3}; /* Light grey background */
            text-align: center; /* Center the text */
        }}

        QProgressBar::chunk {{
            background-color: {darkcolor3}; /* Slightly darker grey for the progress */
        }}

        QScrollArea {{
            background-color: {darkcolor2}; /* Dark grey background for the scroll area */
            color: {darkcolor5}; /* White text for better readability */
        }}

        QScrollArea QWidget {{
            background-color: {darkcolor9}; /* Dark grey background for the content inside the scroll area */
            color: {darkcolor5}; /* White text for better readability */
        }}

        QToolTip {{
            background-color: {darkcolor12};
            color: {darkcolor13};
            border: 0px solid {darkcolor12};
            border-radius: 0px;
            padding: 0px;
            font-weight: bold;
        }}

        QScrollBar:vertical {{
            background-color: {darkcolor2};
        }}

        QScrollBar:horizontal {{
            background-color: {darkcolor2};
        }}
        
        QListWidget {{
            background-color: {darkcolor2}; /* Light grey background */
            color: {darkcolor5}; /* Dark grey text */
        }}
        
        QListWidget::item:selected {{
            background-color: {darkcolor3}; /* Slightly darker grey for selected item */
            color: {darkcolor5}; /* Dark grey text for consistency */
        }}
        
        """.format(
            darkcolor1=self.darkColor1,
            darkcolor2=self.darkColor2,
            darkcolor3=self.darkColor3,
            darkcolor4=self.darkColor4,
            darkcolor5=self.darkColor5,
            darkcolor6=self.darkColor6,
            darkcolor7=self.darkColor7,
            darkcolor8=self.darkColor8,
            darkcolor9=self.darkColor9,
            darkcolor10=self.darkColor10,
            darkcolor11=self.darkColor11,
            darkcolor12=self.darkColor12,
            darkcolor13=self.darkColor13,
        )

        stylesheet2 = """
                    QWidget {{
                        background-color: {darkColor1};
                        color: {darkColor8};
                    }}
                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                        background-color: {darkColor1};
                        color: {darkColor5};
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QLineEdit {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                        border-radius: 2px;
                        height: 20px;
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QLineEdit:focus {{ 
                        border: 1px solid {darkColor8}; 
                        border-radius: 2px; /* Rounded corners for the progress bar */
                    }}
                    QComboBox {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                        border-radius: 2px;
                        height: 20px;
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QComboBox:focus {{ 
                        border: 1px solid {darkColor8}; 
                        border-radius: 2px; /* Rounded corners for the progress bar */
                    }}
                    QComboBox QAbstractItemView {{
                        background-color: {darkColor3}; /* Same as the combo box background */
                        color: {darkColor5}; /* Text color */
                        selection-background-color: {darkColor9}; /* Background color when an item is selected */
                        selection-color: {darkColor5}; /* Text color when an item is selected */
                    }}
                    QTreeWidget {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget::item {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget::item:selected {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                    }}
                    QTreeWidget QMenu {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget QMenu::item:selected {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                    }}
                    QDialog {{
                        background-color: {darkColor1};
                    }}
                    QAbstractItemView {{
                        selection-background-color: {darkColor3};
                        selection-color: {darkColor5};
                    }}
                    QScrollArea {{
                        background-color: {darkColor2}; /* Dark grey background for the scroll area */
                        color: {darkColor5}; /* White text for better readability */
                    }}
                    QScrollArea QWidget {{
                        background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                        color: {darkColor5}; /* White text for better readability */
                    }}
                    QScrollBar:vertical {{
                        background-color: {darkColor2};
                    }}
                    QScrollBar:horizontal {{
                        background-color: {darkColor2};
                    }}
                    """.format(
            darkColor1=self.darkColor1,
            darkColor2=self.darkColor2,
            darkColor3=self.darkColor3,
            darkColor4=self.darkColor4,
            darkColor5=self.darkColor5,
            darkColor6=self.darkColor6,
            darkColor7=self.darkColor7,
            darkColor8=self.darkColor8,
            darkColor9=self.darkColor9,
            darkColor10=self.darkColor10,
            darkColor11=self.darkColor11,
            darkColor12=self.darkColor12,
            darkColor13=self.darkColor13,
        )

        self.setStyleSheet(style_sheet)

        # List of dialog variables
        dialogs = [
            self.export_plot_spec_dialog,
            self.export_petro_dialog,
            self.export_well_dialog,
            self.export_dialog
        ]

        # Apply condition to each dialog
        for dialog in dialogs:
            if dialog:
                dialog.setStyleSheet(stylesheet2)

    def update_recent_files_submenu(self):
        # Clear existing actions
        self.recent_files_menu.clear()

        # First, remove the Recent Files menu if it exists
        self.file_menu.removeAction(self.recent_files_menu.menuAction())

        if self.loaded_file_paths:
            # Insert the "Recent Files" menu as the first item in the File menu
            first_action = self.file_menu.actions()[0] if self.file_menu.actions() else None
            self.file_menu.insertMenu(first_action, self.recent_files_menu)
            # Add actions for each loaded file
            for file_name in self.loaded_file_paths:
                recent_file_action = QAction(file_name, self)
                recent_file_action.triggered.connect(lambda _, name=file_name: self.load_recent_file(name))
                self.recent_files_menu.addAction(recent_file_action)
        else:
            # Remove the "Recent Files" menu from the File menu if it exists
            self.file_menu.removeAction(self.recent_files_menu.menuAction())
            # Create a submenu for "Recent Files"
            self.recent_files_menu = QMenu("Recent Files", self)

    def keyPressEvent(self, event):
        if event.key() == Qt.Key.Key_Delete:
            selected_action = self.recent_files_menu.activeAction()
            if selected_action:
                file_name = selected_action.text()
                # Remove the file from your data structure (e.g., loaded_file_paths)
                if file_name in self.loaded_file_paths:
                    del self.loaded_file_paths[file_name]  # Remove the file
                    self.update_recent_files_submenu()  # Re-populate the submenu after deletion
            else:
                selected_items = self.treeWidget.selectedItems()
                if selected_items:
                    selected_item = selected_items[0]
                    parent_item = selected_item.parent()
                    is_parent = parent_item is None

                    # Perform deletion from custom data structures
                    if is_parent:
                        if self.num_clusters.get(self.file_name) is not None:
                            if self.kmeans_dict.get(
                                    f"{self.file_name}_{self.num_clusters[self.file_name]}") is not None:

                                # Create a list to store keys to delete
                                keys_to_delete = []

                                # Iterate over the keys in the dictionary
                                for key in self.kmeans_dict.keys():
                                    # Split the key into parts using "_" as the delimiter
                                    key_parts = key.split("_")
                                    # Check if the first part of the key matches the reference key
                                    if key_parts[0] == self.file_name:
                                        # If it does, add the key to the list of keys to delete
                                        keys_to_delete.append(key)

                                # Delete the items associated with the keys found
                                for key in keys_to_delete:
                                    self.kmeans_dict.pop(key, None)

                        # Remove all child tensors associated with this parent
                        child_keys_to_delete = []
                        data = self.tensor_dict[self.file_name]
                        if isinstance(data, np.ndarray):
                            parent_name_or_source = self.metadata[self.file_name].get('name') or self.metadata[
                                self.file_name].get('source')

                            for key, value in self.metadata.items():
                                name_or_source = value.get('name') or value.get('source')
                                if name_or_source == parent_name_or_source:
                                    child_keys_to_delete.append(key)

                            for child_key in child_keys_to_delete:
                                self.tensor_dict.pop(child_key, None)
                                self.metadata.pop(child_key, None)

                        elif isinstance(data, pd.DataFrame):
                            parent_name_or_source = self.well_header_info_library[self.file_name].get('Well Name')

                            for key, value in self.well_header_info_library.items():
                                name_or_source = value.get('Well Name')
                                if name_or_source == parent_name_or_source:
                                    child_keys_to_delete.append(key)

                            for child_key in child_keys_to_delete:
                                self.tensor_dict.pop(child_key, None)
                                self.well_header_info_library.pop(child_key, None)
                                self.well_deviation_dict.pop(child_key, None)
                                self.well_head_dict.pop(child_key, None)

                    # Remove the tensor from the dictionary
                    if self.file_name in self.tensor_dict:
                        self.tensor_dict.pop(self.file_name, None)
                        self.metadata.pop(self.file_name, None)
                        self.well_header_info_library.pop(self.file_name, None)
                        self.well_deviation_dict.pop(self.file_name, None)
                        self.well_head_dict.pop(self.file_name, None)

                    # If it's a child item, remove it from the parent
                    if not is_parent:
                        parent_item.removeChild(selected_item)

                    # Remove the selected item from the tree
                    index_of_deleted = self.treeWidget.indexOfTopLevelItem(
                        selected_item) if is_parent else parent_item.indexOfChild(selected_item)
                    if is_parent:
                        self.treeWidget.takeTopLevelItem(index_of_deleted)
                    else:
                        parent_item.takeChild(index_of_deleted)

                    # Handle the selection and setting of the new item
                    new_selected_item = None
                    if self.treeWidget.topLevelItemCount() > 0:
                        if index_of_deleted == 0:
                            new_selected_item = self.treeWidget.topLevelItem(
                                0)  # Select the first item if the deleted was the first
                        else:
                            new_selected_item = self.treeWidget.topLevelItem(
                                index_of_deleted - 1)  # Select the previous item if the deleted was not the first
                    else:
                        # If no items are left, clear the selection and data
                        self.tensor_data = None
                        self.file_name = None
                        self.three_d()
                        self.Three_D_button.hide()
                        self.time_slice_button.hide()
                        self.cross_line_button.hide()
                        self.inline_button.hide()
                        self.plot_button.hide()
                        self.channel_index_label.hide()
                        self.channel_index_entry.hide()
                        self.Channel_slider.hide()
                        # Clear the tensor dictionary
                        for key in list(self.tensor_dict.keys()):
                            del self.tensor_dict[key]

                    if self.seismic_calc_dialogue:
                        self.update_array_selectors(self.seismic_calc_dialogue.array_selector)

                    if hasattr(self, 'well_calc_dialogue') and hasattr(self.well_calc_dialogue,
                                                                       'df_selector'):

                        calc_keys = [key for key in self.tensor_dict.keys() if
                                     isinstance(self.tensor_dict[key], pd.DataFrame)]

                        existing_items = [self.well_calc_dialogue.df_selector.itemText(i) for i in
                                          range(self.well_calc_dialogue.df_selector.count())]

                        # Remove items not in calc_keys
                        for item in existing_items:
                            if item not in calc_keys:
                                index = existing_items.index(item)
                                self.well_calc_dialogue.df_selector.removeItem(index)
                                existing_items.pop(index)  # Update the local list to keep in sync

                        # Add missing items from calc_keys
                        for key in calc_keys:
                            if key not in existing_items:
                                self.well_calc_dialogue.df_selector.addItem(key)

                        # Check if df_selector is empty
                        if self.well_calc_dialogue.df_selector.count() == 0 and hasattr(self.well_calc_dialogue,
                                                                                        'column_selector'):
                            # Clear the column_selector if df_selector is empty
                            self.well_calc_dialogue.column_selector.clear()

                    if new_selected_item:
                        self.treeWidget.setCurrentItem(new_selected_item)
                        new_key = new_selected_item.text(0).strip()
                        self.selectTensor(new_key)  # Call the selectTensor function with the new key

    def load_recent_file(self, file_name):
        if file_name in self.loaded_file_paths:
            file_path = self.loaded_file_paths[file_name]
            # Check if the file exists
            if not file_path:

                return
            else:
                self.three_d()
                try:
                    if file_path.endswith('.npy') or file_path.endswith('.npz'):
                        QApplication.setOverrideCursor(self.custom_cursor)

                        if file_path.endswith('.npy'):
                            self.tensor_data = np.load(file_path)
                            # Check if the tensor is 3D
                            if self.tensor_data.ndim == 3:
                                # Add a test double channel dimension as the last dimension
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                            elif self.tensor_data.ndim == 2:
                                # Add two test double dimensions at the end for 2D tensor
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                            # Create a new dialog to select Template Type
                            template_dialog = QDialog(self)
                            template_dialog.setWindowTitle("Select Template Type")
                            layout = QVBoxLayout(template_dialog)
                            template_type_combo = QComboBox()
                            template_type_combo.addItems([
                                'Seismic',
                                'App. Polarity',
                                'Inst. Frequency',
                                'Cos Phase',
                                'Inst. Phase',
                                'Envelope',
                                'Inst. Bandwidth',
                                'Dom. Frequency',
                                'Sweetness',
                                'RMS Amplitude',
                                'Geobodies',
                                'Anomalies',
                                'Probe',
                                'Upscaled',
                                'Clusters',
                                'Zones',
                                'Annotations',
                                'Other'
                            ])

                            layout.addWidget(template_type_combo)

                            select_button = QPushButton("OK")
                            select_button.clicked.connect(template_dialog.accept)  # Close the current dialog
                            layout.addWidget(select_button)

                            template_dialog.setLayout(layout)
                            template_dialog.adjustSize()
                            template_dialog.setMinimumWidth(225)
                            # Disable the '?' help button on the dialog
                            template_dialog.setWindowFlags(
                                template_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                            QApplication.restoreOverrideCursor()
                            result = template_dialog.exec()  # Use exec() to block until the dialog is closed
                            if result == QDialog.DialogCode.Accepted:

                                if template_type_combo.currentText() == 'Other':
                                    template_name, ok = QInputDialog.getText(self, "Input Dialog",
                                                                             "Enter Template name:")
                                    if ok:
                                        template_type = template_name
                                    else:
                                        template_type = 'Seismic'
                                else:
                                    template_type = template_type_combo.currentText()
                            else:

                                template_type = 'Seismic'

                            number, ok = QInputDialog.getInt(self, "Input Sampling Interval",
                                                             "Enter Sampling Interval (ms):",
                                                             1, 0, 100)

                            source_name, ok = QInputDialog.getText(self, "Input Dialog", "Enter Dataset name:",
                                                                   text=file_name)

                            QApplication.setOverrideCursor(self.custom_cursor)

                            self.metadata[file_name] = {}
                            self.metadata[file_name]['name'] = source_name
                            self.metadata[file_name]['sampling_interval_ms'] = number
                            self.metadata[file_name]['template'] = template_type
                        else:
                            # Load the NPZ file
                            data = np.load(file_path, allow_pickle=True)

                            # Access the tensor and metadata
                            self.tensor_data = data['tensor']
                            # Check if the tensor is 3D
                            if self.tensor_data.ndim == 3:
                                # Add a test double channel dimension as the last dimension
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                            elif self.tensor_data.ndim == 2:
                                # Add two test double dimensions at the end for 2D tensor
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                                self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                            self.metadata[file_name] = data[
                                'metadata'].item()  # .item() converts the metadata back to a dictionary

                        if self.metadata[file_name].get('inline_range') is None:
                            self.metadata[file_name]['inline_range'] = (0, self.tensor_data.shape[1] - 1)

                        if self.metadata[file_name].get('xline_range') is None:
                            self.metadata[file_name]['xline_range'] = (0, self.tensor_data.shape[2] - 1)

                        if self.metadata[file_name].get('x_range') is None:
                            self.metadata[file_name]['x_range'] = (
                            0, self.tensor_data.shape[2] - 1, self.tensor_data.shape[2] - 1)

                        if self.metadata[file_name].get('y_range') is None:
                            self.metadata[file_name]['y_range'] = (
                            0, self.tensor_data.shape[1] - 1, self.tensor_data.shape[1] - 1)

                        if self.metadata[file_name].get('time_range') is None:
                            self.metadata[file_name]['time_range'] = (
                                (-(self.tensor_data.shape[0] - 1) * self.metadata[file_name][
                                    'sampling_interval_ms']), 0,
                                (self.tensor_data.shape[0] - 1) * self.metadata[file_name]['sampling_interval_ms'])

                        if self.metadata[file_name].get('origin') is None:
                            self.metadata[file_name]['origin'] = (0, 0)

                        if self.metadata[file_name].get('xline_end') is None:
                            self.metadata[file_name]['xline_end'] = (self.tensor_data.shape[2] - 1, 0)

                        if self.metadata[file_name].get('inline_end') is None:
                            self.metadata[file_name]['inline_end'] = (0, self.tensor_data.shape[1] - 1)

                        # Add the loaded tensor to the dictionary
                        self.add_tensor(file_name, self.tensor_data)
                        if self.tensor_data.shape[-1] > 1:
                            self.set_channel_slider_range(self.tensor_data.shape[-1])
                            self.channel_index_label.show()
                            self.channel_index_entry.show()
                            self.Channel_slider.show()
                        else:
                            self.channel_index_label.hide()
                            self.channel_index_entry.hide()
                            self.Channel_slider.hide()

                        if self.seismic_calc_dialogue:
                            self.update_array_selectors(self.seismic_calc_dialogue.array_selector)

                        QApplication.restoreOverrideCursor()

                    elif file_path.endswith(".csv") or file_path.endswith((".xls", ".xlsx")):
                        QApplication.setOverrideCursor(self.custom_cursor)

                        # Determine the file format and load the data
                        if file_path.endswith(".csv"):
                            well_log_df = pd.read_csv(file_path)
                            # Replace missing values with -999.25
                            well_log_df.fillna(-999.25, inplace=True)
                            self.units_dict[file_name] = {'DEPT': '.m', 'DEPTH': '.m', 'CALI': '.in',
                                                          'DRHO': '.g/cm3', 'DT': '.us/ft',
                                                          'GR': '.gAPI', 'RD': '.ohm.m',
                                                          'RHOB': '.g/cm3', 'RS': '.sm3/sm3',
                                                          'One-waytime1': '.ms', 'Rhom': '.g/cm3',
                                                          'Vp': '.m/s', 'Rho': '.g/cm3',
                                                          'Vpm': '.m/s', 'PHIE': '.m3/m3',
                                                          'PHIT': '.m3/m3', 'SWT': '._', 'SWE': '._',
                                                          'VOL_ANHYDR': '._', 'VOL_CALCITE': '._',
                                                          'VOL_DOLOM': '._', 'VOL_KAOLIN': '._',
                                                          'VOL_QUARTZ': '._', 'VOL_UGAS': '._',
                                                          'VOL_UOIL': '._', 'VOL_UWAT': '._',
                                                          'VOL_WCS': '._', 'VOL_SIDER': '._'}
                            if file_name not in self.well_header_info_library:
                                self.well_header_info_library[file_name] = {}
                            self.well_header_info_library[file_name]['Well Name'] = file_name
                            self.well_header_info_library[file_name]['Template'] = "Original"
                        elif file_path.endswith((".xls", ".xlsx")):
                            # Read all sheet names
                            all_sheets = pd.ExcelFile(file_path).sheet_names  # Get a list of sheet names
                            QApplication.restoreOverrideCursor()
                            # Ask the user to select a sheet
                            selected_sheet, ok = QInputDialog.getItem(
                                self,
                                "Select Sheet",
                                "Choose a sheet to load:",
                                all_sheets,
                                editable=False  # Prevent user from typing custom sheet names
                            )
                            QApplication.setOverrideCursor(self.custom_cursor)
                            if not ok:  # If the user clicked OK and selected a sheet
                                QApplication.restoreOverrideCursor()
                                return

                            well_log_df = pd.read_excel(file_path, sheet_name=selected_sheet)
                            # Replace missing values with -999.25
                            well_log_df.fillna(-999.25, inplace=True)
                            self.units_dict[file_name] = {'DEPT': '.m', 'DEPTH': '.m', 'CALI': '.in',
                                                          'DRHO': '.g/cm3', 'DT': '.us/ft',
                                                          'GR': '.gAPI', 'RD': '.ohm.m',
                                                          'RHOB': '.g/cm3', 'RS': '.sm3/sm3',
                                                          'One-waytime1': '.ms', 'Rhom': '.g/cm3',
                                                          'Vp': '.m/s', 'Rho': '.g/cm3',
                                                          'Vpm': '.m/s', 'PHIE': '.m3/m3',
                                                          'PHIT': '.m3/m3', 'SWT': '._', 'SWE': '._',
                                                          'VOL_ANHYDR': '._', 'VOL_CALCITE': '._',
                                                          'VOL_DOLOM': '._', 'VOL_KAOLIN': '._',
                                                          'VOL_QUARTZ': '._', 'VOL_UGAS': '._',
                                                          'VOL_UOIL': '._', 'VOL_UWAT': '._',
                                                          'VOL_WCS': '._', 'VOL_SIDER': '._'}
                            if file_name not in self.well_header_info_library:
                                self.well_header_info_library[file_name] = {}
                            self.well_header_info_library[file_name]['Well Name'] = file_name
                            self.well_header_info_library[file_name]['Template'] = "Original"

                        # Store the data in the tensor dictionary with the display name as the key
                        self.tensor_dict[file_name] = well_log_df

                        # Check if there is any checkshot data that matches this well log
                        for checkshot_name, checkshot_data in self.checkshot_dict.items():
                            # Transform checkshot data: make all values positive and replace -999 with -999.25
                            checkshot_data = checkshot_data.applymap(
                                lambda x: -999.25 if x == -999 else (
                                    abs(x) if isinstance(x, (int, float)) else x)
                            )

                            checkshot_well_names = checkshot_data['Well'].str.strip(
                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                         regex=True)  # Remove all spaces around the hyphen
                            file_name_clean = self.well_header_info_library[file_name].get(
                                'Well Name').strip().lower()

                            if file_name_clean in checkshot_well_names.values:
                                matched_rows = checkshot_data[
                                    checkshot_well_names == file_name_clean]

                                def identify_depth_column():
                                    """Identify the depth column, typically the first column, and return its name."""
                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                    for name in possible_names:
                                        if name in well_log_df.columns:
                                            return name
                                    return well_log_df.columns[
                                        0]  # Assume the first column is depth if no common name matches

                                depth = identify_depth_column()

                                well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                                # Create a rounded copy of the depth column
                                rounded_depth = well_log_df[depth].round(2)

                                # Set the index of the well log DataFrame to the rounded depth for faster lookup
                                well_log_df.set_index(rounded_depth, inplace=True)

                                # Only add new columns to the well log DataFrame if they don't already exist
                                new_cols = [col for col in matched_rows.columns if
                                            col not in well_log_df.columns and col != 'Well']
                                for col in new_cols:
                                    well_log_df[col] = -999.25

                                # Set the index of the matched rows DataFrame to 'MD' for fast lookup
                                matched_rows.set_index(matched_rows['MD'].round(2), inplace=True)

                                # Update the well log DataFrame using vectorized operations
                                well_log_df.update(matched_rows[new_cols])

                                # Reset index after operations
                                well_log_df.reset_index(drop=True, inplace=True)

                                # Update the dictionary with the modified DataFrame
                                self.tensor_dict[file_name] = well_log_df

                        # Check if there is any well top data that matches this well log
                        for welltop_name, welltop_data in self.welltop_dict.items():
                            welltop_well_names = welltop_data['Well'].str.strip(
                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                         regex=True)  # Remove all spaces around the hyphen
                            file_name_clean = self.well_header_info_library[file_name].get(
                                'Well Name').strip().lower()

                            if file_name_clean in welltop_well_names.values:
                                matched_rows = welltop_data[
                                    welltop_well_names == file_name_clean].copy()

                                def identify_depth_column():
                                    """Identify the depth column, typically the first column, and return its name."""
                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                    for name in possible_names:
                                        if name in well_log_df.columns:
                                            return name
                                    return well_log_df.columns[
                                        0]  # Assume the first column is depth if no common name matches

                                depth = identify_depth_column()

                                well_log_df[depth] = pd.to_numeric(well_log_df[depth],
                                                                   errors='coerce')

                                # Ensure the index is monotonic increasing
                                well_log_df.sort_index(inplace=True)

                                # Interpolate to find the closest matching depth
                                matched_rows.loc[:, 'MD'] = pd.to_numeric(matched_rows['MD'],
                                                                          errors='coerce')
                                matched_rows.set_index(matched_rows['MD'], inplace=True)

                                # Create new columns if they don't exist
                                new_cols = [col for col in matched_rows.columns if
                                            col not in well_log_df.columns and col != 'Well']
                                for col in new_cols:
                                    well_log_df[col] = -999.25

                                # Use KDTree for nearest neighbor search
                                well_log_depths = well_log_df[depth].values.reshape(-1, 1)
                                kdtree = cKDTree(well_log_depths)

                                # Define the maximum allowed distance (threshold)
                                threshold = 2.0

                                for md_value, row_data in matched_rows.iterrows():
                                    dist, idx = kdtree.query([[md_value]])
                                    nearest_depth = well_log_df.iloc[idx[0]][depth]

                                    # Check if the distance is within the threshold
                                    if dist[0] <= threshold:
                                        # Update well_log_df at the nearest depth with matched row data
                                        well_log_df.loc[well_log_df[depth] == nearest_depth, new_cols] = \
                                            row_data[new_cols].values

                                well_log_df.reset_index(drop=True, inplace=True)
                                self.tensor_dict[file_name] = well_log_df

                        # Use existing function to add the entry to the tree widget
                        self.add_tensor(file_name, well_log_df)

                        if hasattr(self, 'well_calc_dialogue') and hasattr(self.well_calc_dialogue,
                                                                           'df_selector'):

                            calc_keys = [key for key in self.tensor_dict.keys() if
                                         isinstance(self.tensor_dict[key], pd.DataFrame)]

                            existing_items = [self.well_calc_dialogue.df_selector.itemText(i) for i in
                                              range(self.well_calc_dialogue.df_selector.count())]

                            # Remove items not in calc_keys
                            for item in existing_items:
                                if item not in calc_keys:
                                    index = existing_items.index(item)
                                    self.well_calc_dialogue.df_selector.removeItem(index)
                                    existing_items.pop(index)  # Update the local list to keep in sync

                            # Add missing items from calc_keys
                            for key in calc_keys:
                                if key not in existing_items:
                                    self.well_calc_dialogue.df_selector.addItem(key)

                            # Check if df_selector is empty
                            if self.well_calc_dialogue.df_selector.count() == 0 and hasattr(
                                    self.well_calc_dialogue,
                                    'column_selector'):
                                # Clear the column_selector if df_selector is empty
                                self.well_calc_dialogue.column_selector.clear()

                        QApplication.restoreOverrideCursor()

                    else:

                        with open(file_path, 'r') as file:
                            # Read the first few lines to determine the format
                            lines = file.readlines()

                            # Find the first non-empty line
                            line = next((line for line in lines if line.strip()), None)

                            if line:
                                if "checkshots" in line:
                                    if file_path:
                                        QApplication.setOverrideCursor(self.custom_cursor)
                                        # Read and parse the file to extract data into a DataFrame
                                        checkshot_data = self.parse_text_file(file_path)

                                        # Ensure all checkshot data columns are positive and replace -999 with -999.25
                                        checkshot_data = checkshot_data.applymap(
                                            lambda x: -999.25 if x == -999 else (
                                                abs(x) if isinstance(x, (int, float)) else x)
                                        )

                                        # Store the checkshot data in a separate dictionary
                                        self.checkshot_dict[file_name] = checkshot_data

                                        for well_name, well_log_df in self.tensor_dict.items():
                                            well_info = self.well_header_info_library.get(well_name)

                                            # Skip the loop iteration if well_info is None (i.e., well_name is missing)
                                            if well_info is None:
                                                continue

                                            # Process the well name if found, stripping and converting to lowercase
                                            well_name_clean = well_info.get('Well Name').strip().lower()

                                            # Remove quotes and strip whitespace from checkshot well names
                                            check_well_names = checkshot_data['Well'].str.strip(
                                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                                         regex=True)  # Remove all spaces around the hyphen

                                            if well_name_clean in check_well_names.values:
                                                matched_rows = checkshot_data[check_well_names == well_name_clean]

                                                def identify_depth_column():
                                                    """Identify the depth column, typically the first column, and return its name."""
                                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                                    for name in possible_names:
                                                        if name in well_log_df.columns:
                                                            return name
                                                    return well_log_df.columns[
                                                        0]  # If no common name matches, assume the first column is depth

                                                depth = identify_depth_column()

                                                well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                                                # Create a rounded copy of the depth column
                                                rounded_depth = well_log_df[depth].round(2)

                                                # Set the index of the well log DataFrame to the rounded depth for faster lookup
                                                well_log_df.set_index(rounded_depth, inplace=True)

                                                # Only add new columns to the well log DataFrame if they don't already exist
                                                new_cols = [col for col in matched_rows.columns if
                                                            col not in well_log_df.columns and col != 'Well']
                                                for col in new_cols:
                                                    well_log_df[col] = -999.25

                                                # Set the index of the matched rows DataFrame to 'MD' for fast lookup
                                                matched_rows.set_index(matched_rows['MD'].round(2), inplace=True)

                                                # Update the well log DataFrame using vectorized operations
                                                well_log_df.update(matched_rows[new_cols])

                                                # Reset index after operations
                                                well_log_df.reset_index(drop=True, inplace=True)

                                                # Update the dictionary with the modified DataFrame
                                                self.tensor_dict[well_name] = well_log_df

                                        QApplication.restoreOverrideCursor()
                                        QMessageBox.information(self, "Loading Completed",
                                                                "The Loading process has finished successfully.")
                                if "log file" in line:
                                    if file_path:
                                        QApplication.setOverrideCursor(self.custom_cursor)

                                        # Parse the LAS file
                                        well_log_df = self.parse_las_file(file_path)

                                        # Store the data in the tensor dictionary with the display name as the key
                                        self.tensor_dict[file_name] = well_log_df

                                        # Check if there is any checkshot data that matches this well log
                                        for checkshot_name, checkshot_data in self.checkshot_dict.items():
                                            # Transform checkshot data: make all values positive and replace -999 with -999.25
                                            checkshot_data = checkshot_data.applymap(
                                                lambda x: -999.25 if x == -999 else (
                                                    abs(x) if isinstance(x, (int, float)) else x)
                                            )

                                            checkshot_well_names = checkshot_data['Well'].str.strip(
                                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                                         regex=True)  # Remove all spaces around the hyphen
                                            file_name_clean = self.well_header_info_library[file_name].get \
                                                ('Well Name').strip().lower()

                                            if file_name_clean in checkshot_well_names.values:
                                                matched_rows = checkshot_data[
                                                    checkshot_well_names == file_name_clean]

                                                def identify_depth_column():
                                                    """Identify the depth column, typically the first column, and return its name."""
                                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                                    for name in possible_names:
                                                        if name in well_log_df.columns:
                                                            return name
                                                    return well_log_df.columns[
                                                        0]  # Assume the first column is depth if no common name matches

                                                depth = identify_depth_column()

                                                well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                                                # Create a rounded copy of the depth column
                                                rounded_depth = well_log_df[depth].round(2)

                                                # Set the index of the well log DataFrame to the rounded depth for faster lookup
                                                well_log_df.set_index(rounded_depth, inplace=True)

                                                # Only add new columns to the well log DataFrame if they don't already exist
                                                new_cols = [col for col in matched_rows.columns if
                                                            col not in well_log_df.columns and col != 'Well']
                                                for col in new_cols:
                                                    well_log_df[col] = -999.25

                                                # Set the index of the matched rows DataFrame to 'MD' for fast lookup
                                                matched_rows.set_index(matched_rows['MD'].round(2), inplace=True)

                                                # Update the well log DataFrame using vectorized operations
                                                well_log_df.update(matched_rows[new_cols])

                                                # Reset index after operations
                                                well_log_df.reset_index(drop=True, inplace=True)

                                                # Update the dictionary with the modified DataFrame
                                                self.tensor_dict[file_name] = well_log_df

                                        # Check if there is any well top data that matches this well log
                                        for welltop_name, welltop_data in self.welltop_dict.items():
                                            welltop_well_names = welltop_data['Well'].str.strip(
                                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                                         regex=True)  # Remove all spaces around the hyphen
                                            file_name_clean = self.well_header_info_library[file_name].get \
                                                ('Well Name').strip().lower()

                                            if file_name_clean in welltop_well_names.values:
                                                matched_rows = welltop_data[
                                                    welltop_well_names == file_name_clean].copy()

                                                def identify_depth_column():
                                                    """Identify the depth column, typically the first column, and return its name."""
                                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                                    for name in possible_names:
                                                        if name in well_log_df.columns:
                                                            return name
                                                    return well_log_df.columns[
                                                        0]  # Assume the first column is depth if no common name matches

                                                depth = identify_depth_column()

                                                well_log_df[depth] = pd.to_numeric(well_log_df[depth],
                                                                                   errors='coerce')

                                                # Ensure the index is monotonic increasing
                                                well_log_df.sort_index(inplace=True)

                                                # Interpolate to find the closest matching depth
                                                matched_rows.loc[:, 'MD'] = pd.to_numeric(matched_rows['MD'],
                                                                                          errors='coerce')
                                                matched_rows.set_index(matched_rows['MD'], inplace=True)

                                                # Create new columns if they don't exist
                                                new_cols = [col for col in matched_rows.columns if
                                                            col not in well_log_df.columns and col != 'Well']
                                                for col in new_cols:
                                                    well_log_df[col] = -999.25

                                                # Use KDTree for nearest neighbor search
                                                well_log_depths = well_log_df[depth].values.reshape(-1, 1)
                                                kdtree = cKDTree(well_log_depths)

                                                # Define the maximum allowed distance (threshold)
                                                threshold = 2.0

                                                for md_value, row_data in matched_rows.iterrows():
                                                    dist, idx = kdtree.query([[md_value]])
                                                    nearest_depth = well_log_df.iloc[idx[0]][depth]

                                                    # Check if the distance is within the threshold
                                                    if dist[0] <= threshold:
                                                        # Update well_log_df at the nearest depth with matched row data
                                                        well_log_df.loc[well_log_df[depth] == nearest_depth, new_cols] = \
                                                            row_data[new_cols].values

                                                well_log_df.reset_index(drop=True, inplace=True)
                                                self.tensor_dict[file_name] = well_log_df

                                        # Use existing function to add the entry to the tree widget
                                        self.add_tensor(file_name, well_log_df)

                                        if hasattr(self, 'well_calc_dialogue') and hasattr(self.well_calc_dialogue,
                                                                                           'df_selector'):

                                            calc_keys = [key for key in self.tensor_dict.keys() if
                                                         isinstance(self.tensor_dict[key], pd.DataFrame)]

                                            existing_items = [self.well_calc_dialogue.df_selector.itemText(i) for i in
                                                              range(self.well_calc_dialogue.df_selector.count())]

                                            # Remove items not in calc_keys
                                            for item in existing_items:
                                                if item not in calc_keys:
                                                    index = existing_items.index(item)
                                                    self.well_calc_dialogue.df_selector.removeItem(index)
                                                    existing_items.pop(index)  # Update the local list to keep in sync

                                            # Add missing items from calc_keys
                                            for key in calc_keys:
                                                if key not in existing_items:
                                                    self.well_calc_dialogue.df_selector.addItem(key)

                                            # Check if df_selector is empty
                                            if self.well_calc_dialogue.df_selector.count() == 0 and hasattr(
                                                    self.well_calc_dialogue,
                                                    'column_selector'):
                                                # Clear the column_selector if df_selector is empty
                                                self.well_calc_dialogue.column_selector.clear()

                                    QApplication.restoreOverrideCursor()

                                if "well tops" in line:
                                    if file_path:
                                        QApplication.setOverrideCursor(self.custom_cursor)
                                        # Read and parse the file to extract data into a DataFrame
                                        well_top_data = self.parse_top_file(file_path)

                                        # Ensure all WellTop data columns are positive and replace -999 with -999.25
                                        well_top_data = well_top_data.applymap(
                                            lambda x: -999.25 if x == -999 else (
                                                abs(x) if isinstance(x, (int, float)) else x)
                                        )

                                        # Drop columns that are entirely -999.25
                                        well_top_data = well_top_data.loc[:, (well_top_data != -999.25).any(axis=0)]

                                        # Store the WellTop data in a separate dictionary
                                        self.welltop_dict[file_name] = well_top_data

                                        for well_name, well_log_df in self.tensor_dict.items():
                                            well_info = self.well_header_info_library.get(well_name)

                                            # Skip the loop iteration if well_info is None (i.e., well_name is missing)
                                            if well_info is None:
                                                continue

                                            # Process the well name if found, stripping and converting to lowercase
                                            well_name_clean = well_info.get('Well Name').strip().lower()

                                            # Remove quotes and strip whitespace from WellTop well names
                                            top_well_names = well_top_data['Well'].str.strip(
                                                '"').str.strip().str.lower().str.replace(r'\s*-\s*', '-',
                                                                                         regex=True)  # Remove all spaces around the hyphen

                                            if well_name_clean in top_well_names.values:
                                                matched_rows = well_top_data[top_well_names == well_name_clean].copy()

                                                def identify_depth_column():
                                                    """Identify the depth column, typically the first column, and return its name."""
                                                    possible_names = ['DEPT', 'DEPTH', 'Depth', 'depth']
                                                    for name in possible_names:
                                                        if name in well_log_df.columns:
                                                            return name
                                                    return well_log_df.columns[
                                                        0]  # If no common name matches, assume the first column is depth

                                                depth = identify_depth_column()

                                                well_log_df[depth] = pd.to_numeric(well_log_df[depth], errors='coerce')

                                                # Ensure the index is monotonic increasing
                                                well_log_df.sort_index(inplace=True)

                                                # Interpolate to find the closest matching depth
                                                matched_rows.loc[:, 'MD'] = pd.to_numeric(matched_rows['MD'],
                                                                                          errors='coerce')
                                                matched_rows.set_index(matched_rows['MD'], inplace=True)

                                                # Create new columns if they don't exist
                                                new_cols = [col for col in matched_rows.columns if
                                                            col not in well_log_df.columns and col != 'Well']
                                                for col in new_cols:
                                                    well_log_df[col] = -999.25

                                                # Use KDTree for nearest neighbor search
                                                well_log_depths = well_log_df[depth].values.reshape(-1, 1)
                                                kdtree = cKDTree(well_log_depths)

                                                # Define the maximum allowed distance (threshold)
                                                threshold = 2.0

                                                for md_value, row_data in matched_rows.iterrows():
                                                    dist, idx = kdtree.query([[md_value]])
                                                    nearest_depth = well_log_df.iloc[idx[0]][depth]

                                                    # Check if the distance is within the threshold
                                                    if dist[0] <= threshold:
                                                        # Update well_log_df at the nearest depth with matched row data
                                                        well_log_df.loc[well_log_df[depth] == nearest_depth, new_cols] = \
                                                            row_data[new_cols].values

                                                well_log_df.reset_index(drop=True, inplace=True)
                                                self.tensor_dict[well_name] = well_log_df

                                        QApplication.restoreOverrideCursor()
                                        QMessageBox.information(self, "Loading Completed",
                                                                "The Loading process has finished successfully.")
                                if "well head" in line or not line.startswith("#"):
                                    if file_path:
                                        QApplication.setOverrideCursor(self.custom_cursor)
                                        try:
                                            # Parse the file and update the well_head_dict
                                            self.parse_well_head(file_path)
                                            QApplication.restoreOverrideCursor()
                                            QMessageBox.information(
                                                self, "Loading Completed", "Well Head data loaded successfully."
                                            )
                                        except Exception as e:
                                            QApplication.restoreOverrideCursor()
                                            # Handle exceptions and display error messages
                                            QMessageBox.critical(
                                                self, "Error", f"An error occurred while loading the file: {e}"
                                            )

                except Exception as e:
                    QApplication.restoreOverrideCursor()
                    traceback.print_exc()
                    QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def save_background_color(self, color):

        # Convert color to QColor if it's a string
        if isinstance(color, str):
            color = QColor(color)

        # Save the background color to a JSON file
        settings = {'background_color': color.name()}

        color_bar = {'three_d_color_bar_active': self.three_d_color_bar_active}

        settings.update(color_bar)

        axis = {'grid_active': self.grid_active}

        settings.update(axis)

        compass = {'compass_active': self.compass_active}

        settings.update(compass)

        multiprocessing_threshold = {'multiprocessing_threshold': str(self.multiprocessing_threshold)}

        settings.update(multiprocessing_threshold)

        window_size = {'window_size': str(self.window_size)}

        settings.update(window_size)

        shade_threshold = {'shade_threshold': str(self.shade_threshold)}

        settings.update(shade_threshold)

        three_d_interpolation = {'three_d_interpolation': str(self.three_d_interpolation)}

        settings.update(three_d_interpolation)

        two_d_interpolation = {'two_d_interpolation': str(self.two_d_interpolation)}

        settings.update(two_d_interpolation)

        # Add the new color references to the settings
        color_references = {
            'light_color_1': self.light_color_1,
            'light_color_2': self.light_color_2,
            'light_color_3': self.light_color_3,
            'light_color_4': self.light_color_4,
            'light_color_5': self.light_color_5,
            'light_color_6': self.light_color_6,
            'light_color_7': self.light_color_7,
            'light_color_8': self.light_color_8,
            'light_color_9': self.light_color_9,
            'light_color_10': self.light_color_10,
            'darkColor1': self.darkColor1,
            'darkColor2': self.darkColor2,
            'darkColor3': self.darkColor3,
            'darkColor4': self.darkColor4,
            'darkColor5': self.darkColor5,
            'darkColor6': self.darkColor6,
            'darkColor7': self.darkColor7,
            'darkColor8': self.darkColor8,
            'darkColor9': self.darkColor9,
            'darkColor10': self.darkColor10,
            'darkColor11': self.darkColor11,
            'darkColor12': self.darkColor12,
            'darkColor13': self.darkColor13
        }

        settings.update(color_references)

        with open('settings.json', 'w') as file:
            json.dump(settings, file)

    def load_background_color(self):

        # Try to load the background color from a JSON file
        try:
            with open('settings.json', 'r') as file:
                settings = json.load(file)
                color = QColor(settings['background_color'])
                self.last_selected_color = color

                self.three_d_color_bar_active = settings.get('three_d_color_bar_active', self.three_d_color_bar_active)

                self.grid_active = settings.get('grid_active', self.grid_active)

                self.compass_active = settings.get('compass_active', self.compass_active)

                self.multiprocessing_threshold = np.int64(
                    settings.get('multiprocessing_threshold', self.multiprocessing_threshold))

                self.window_size = np.int64(settings.get('window_size', self.window_size))

                self.shade_threshold = float(
                    settings.get('shade_threshold', self.shade_threshold))

                self.three_d_interpolation = np.int64(settings.get('three_d_interpolation', self.three_d_interpolation))

                self.two_d_interpolation = np.int64(settings.get('two_d_interpolation', self.two_d_interpolation))

                # Load the new color references
                self.light_color_1 = settings.get('light_color_1', self.light_color_1)
                self.light_color_2 = settings.get('light_color_2', self.light_color_2)
                self.light_color_3 = settings.get('light_color_3', self.light_color_3)
                self.light_color_4 = settings.get('light_color_4', self.light_color_4)
                self.light_color_5 = settings.get('light_color_5', self.light_color_5)
                self.light_color_6 = settings.get('light_color_6', self.light_color_6)
                self.light_color_7 = settings.get('light_color_7', self.light_color_7)
                self.light_color_8 = settings.get('light_color_8', self.light_color_8)
                self.light_color_9 = settings.get('light_color_9', self.light_color_9)
                self.light_color_10 = settings.get('light_color_10', self.light_color_10)
                self.darkColor1 = settings.get('darkColor1', self.darkColor1)
                self.darkColor2 = settings.get('darkColor2', self.darkColor2)
                self.darkColor3 = settings.get('darkColor3', self.darkColor3)
                self.darkColor4 = settings.get('darkColor4', self.darkColor4)
                self.darkColor5 = settings.get('darkColor5', self.darkColor5)
                self.darkColor6 = settings.get('darkColor6', self.darkColor6)
                self.darkColor7 = settings.get('darkColor7', self.darkColor7)
                self.darkColor8 = settings.get('darkColor8', self.darkColor8)
                self.darkColor9 = settings.get('darkColor9', self.darkColor9)
                self.darkColor10 = settings.get('darkColor10', self.darkColor10)
                self.darkColor11 = settings.get('darkColor11', self.darkColor11)
                self.darkColor12 = settings.get('darkColor12', self.darkColor12)
                self.darkColor13 = settings.get('darkColor13', self.darkColor13)

                return color

        except (FileNotFoundError, KeyError):
            self.last_selected_color = QColor(193, 193, 193)
            # If the file doesn't exist or the key is not found, use the default color
            return QColor(193, 193, 193)

    def closeEvent(self, event):

        # Generate a standard message box when clicking the 'X' button in the title bar
        reply = QMessageBox(
            QMessageBox.Icon.Question,
            "Exit Confirmation",
            "Are you sure you want to exit?",
            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
            self
        )

        if reply.exec() == QMessageBox.StandardButton.Yes:

            QApplication.setOverrideCursor(self.custom_cursor)
            self.save_recent_files()  # Save the recent files before closing
            self.save_background_color(self.last_selected_color)

            # Ensure the log files are closed when the application exits
            if sys.stdout:
                sys.stdout.close()
            if sys.stderr:
                sys.stderr.close()

            # Clean exit
            QApplication.quit()
        else:
            # Cancel the exit
            event.ignore()

    def save_recent_files(self):
        # Save the loaded_file_paths dictionary to a JSON file
        with open(self.recent_files_path, 'w') as file:
            json.dump(self.loaded_file_paths, file, indent=4)

    def load_recent_files(self):
        # Load the recent files from a JSON file into the loaded_file_paths dictionary
        if os.path.exists(self.recent_files_path):
            with open(self.recent_files_path, 'r') as file:
                file_paths = json.load(file)
            # Clear the current dictionary to avoid duplicates
            self.loaded_file_paths.clear()
            # Update the dictionary with file names as keys and full paths as values
            for file_name, file_path in file_paths.items():
                if os.path.exists(file_path):
                    self.loaded_file_paths[file_name] = file_path

    def show_context_menu2(self, position):
        sender = self.sender()
        context_menu = QMenu(self)

        if isinstance(sender, RoundedCanvas):
            # Get the index of the currently active tab
            active_tab_index = self.tab_widget.currentIndex()

            add_tab_action = context_menu.addAction("Add Tab")
            add_tab_action.triggered.connect(self.add_tab)

            switch_to_3d_action = context_menu.addAction("Switch to 3D")
            # New action to choose a color
            save_action = context_menu.addAction("Save")

            choose_color_action = context_menu.addAction("Choose Background Color")
            choose_color_action.triggered.connect(self.choose_color)

            # Add toggle editing mode action
            toggle_edit_action = context_menu.addAction(
                "Deactivate Editing" if sender.edit_mode else "Activate Editing"
            )
            toggle_edit_action.triggered.connect(
                lambda: self.toggle_edit_mode(sender, toggle_edit_action)
            )

            custom_color_mappings = {
                'Seismic',
                'App. Polarity',
                'Polarity',
                'Inst. Frequency',
                'Frequency',
                'Cos Phase',
                'Inst. Phase',
                'Phase',
                'Envelope',
                'Inst. Bandwidth',
                'Dom. Frequency',
                'Sweetness',
                'RMS Amplitude',
                'Coherence',
                'GST'
            }

            # Ensure this function works with the nested dictionary structure
            if active_tab_index in self.cbar:
                # Check if self.cbar is not None and has the active_tab_index key
                if self.cbar is not None and self.cbar.get(active_tab_index) is not None:
                    # Loop through all filenames in the active tab index
                    for file_name in self.tensor_image_items[active_tab_index]:
                        # Check if file_name exists in self.metadata and has the 'template' entry
                        if self.metadata.get(file_name, {}).get('template') in custom_color_mappings:
                            # Call the function with the filename
                            self.addCustomColorMappingSubMenu(context_menu, active_tab_index, file_name)
                            break

            if active_tab_index in self.tensor_image_items and self.tensor_image_items[active_tab_index]:
                # Create the Hide submenu
                hide_menu = QMenu("Hide", self)
                for tensor_key, img_item in self.tensor_image_items[active_tab_index].items():
                    if img_item:  # Check if img_item still exists
                        # Extract the display name from the tensor key
                        display_name = tensor_key.split('_', 1)[-1] if '_' in tensor_key else tensor_key
                        visibility_status = "Shown" if img_item.isVisible() else "Hidden"
                        hide_action = QAction(f"Image {display_name} ({visibility_status})", self)
                        hide_action.triggered.connect(
                            lambda _, key=tensor_key: self.toggle_image_visibility(active_tab_index, key))
                        hide_menu.addAction(hide_action)
                context_menu.addMenu(hide_menu)

                # Create the Remove submenu
                remove_menu = QMenu("Remove", self)
                for tensor_key, img_item in self.tensor_image_items[active_tab_index].items():
                    if img_item:  # Check if img_item still exists
                        # Extract the display name from the tensor key
                        display_name = tensor_key.split('_', 1)[-1] if '_' in tensor_key else tensor_key
                        remove_action = QAction(f"Image {display_name}", self)
                        remove_action.triggered.connect(
                            lambda _, key=tensor_key: self.remove_image(active_tab_index, key))
                        remove_menu.addAction(remove_action)
                context_menu.addMenu(remove_menu)

            action = context_menu.exec(sender.mapToGlobal(position))

            if action == switch_to_3d_action:
                self.toggle_canvas('3D')
            elif action == save_action:
                # Save the plot using PyQtGraph's exporter
                # Get the index of the currently active tab
                active_tab_index = self.tab_widget.currentIndex()
                # Retrieve the widget of the currently active tab
                active_tab_widget = self.tab_widget.widget(active_tab_index)
                # Assuming the canvas is the first widget in the layout of the active tab
                canvas_layout = active_tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()
                self.export_dialog = exportDialog.ExportDialog(canvas.scene())
                if not self.isDarkTheme:
                    stylesheet = """
                    QWidget {{
                        background-color: {light_color_2};
                        color: {light_color_3};
                    }}
                    QListWidget {{
                        background-color: {light_color_5}; /* Light grey background */
                        color: {light_color_3}; /* Dark grey text */
                    }}
                    QListWidget::item:selected {{
                        background-color: {light_color_2}; /* Slightly darker grey for selected item */
                        color: {light_color_3}; /* Dark grey text for consistency */
                    }}
                    QTreeWidget {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                    }}
                    QTreeWidget::item {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                    }}
                    QTreeWidget::item:selected {{
                        background-color: {light_color_2}; /* Selection background */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QTreeWidget QMenu {{
                        background-color: {light_color_5}; /* Menu background */
                        color: {light_color_3}; /* Menu text */
                    }}
                    QTreeWidget QMenu::item:selected {{
                        background-color: {light_color_2}; /* Selection background */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QLineEdit {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        border-radius: 2px; /* Corner rounding */
                        height: 20px; /* Height */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QLineEdit:focus {{
                        border: 1px solid {light_color_8}; /* Focus border */
                        border-radius: 2px; /* Corner rounding */
                    }}
                    QComboBox {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        border-radius: 2px; /* Corner rounding */
                        height: 20px; /* Height */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QComboBox:focus {{
                        border: 1px solid {light_color_8}; /* Focus border */
                        border-radius: 2px; /* Corner rounding */
                    }}
                    QComboBox QAbstractItemView {{
                        background-color: {light_color_5}; /* Background color */
                        color: {light_color_3}; /* Text color */
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QDialog {{
                        background-color: {light_color_1}; /* Background color */
                        color: {light_color_3}; /* Selection text color */
                    }}
                    QAbstractItemView {{
                        selection-background-color: {light_color_2}; /* Selection background */
                        selection-color: {light_color_3}; /* Selection text color */
                    }}
                    QScrollArea {{
                        background-color: {light_color_5}; /* Scroll area background */
                        color: {light_color_3}; /* Scroll area text */
                    }}
                    QScrollArea QWidget {{
                        background-color: {light_color_5}; /* Content background */
                        color: {light_color_3}; /* Content text */
                    }}
                    QScrollBar:vertical {{
                        background-color: {light_color_5}; /* Scrollbar background */
                    }}
                    QScrollBar:horizontal {{
                        background-color: {light_color_5}; /* Scrollbar background */
                    }}
                    """.format(
                        light_color_1=self.light_color_1,
                        light_color_2=self.light_color_2,
                        light_color_3=self.light_color_3,
                        light_color_4=self.light_color_4,
                        light_color_5=self.light_color_5,
                        light_color_6=self.light_color_6,
                        light_color_7=self.light_color_7,
                        light_color_8=self.light_color_8,
                        light_color_9=self.light_color_9,
                        light_color_10=self.light_color_10,
                    )
                else:
                    stylesheet = """
                    QWidget {{
                        background-color: {darkColor1};
                        color: {darkColor8};
                    }}
                    QSpinBox, QDateTimeEdit, QDoubleSpinBox {{
                        background-color: {darkColor1};
                        color: {darkColor5};
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QLineEdit {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                        border-radius: 2px;
                        height: 20px;
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QLineEdit:focus {{ 
                        border: 1px solid {darkColor8}; 
                        border-radius: 2px; /* Rounded corners for the progress bar */
                    }}
                    QComboBox {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                        border-radius: 2px;
                        height: 20px;
                        selection-background-color: {darkColor2};
                        selection-color: {darkColor4};
                    }}
                    QComboBox:focus {{ 
                        border: 1px solid {darkColor8}; 
                        border-radius: 2px; /* Rounded corners for the progress bar */
                    }}
                    QComboBox QAbstractItemView {{
                        background-color: {darkColor3}; /* Same as the combo box background */
                        color: {darkColor5}; /* Text color */
                        selection-background-color: {darkColor9}; /* Background color when an item is selected */
                        selection-color: {darkColor5}; /* Text color when an item is selected */
                    }}
                    QTreeWidget {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget::item {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget::item:selected {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                    }}
                    QTreeWidget QMenu {{
                        background-color: {darkColor2};
                        color: {darkColor5};
                    }}
                    QTreeWidget QMenu::item:selected {{
                        background-color: {darkColor3};
                        color: {darkColor5};
                    }}
                    QDialog {{
                        background-color: {darkColor1};
                    }}
                    QAbstractItemView {{
                        selection-background-color: {darkColor3};
                        selection-color: {darkColor5};
                    }}
                    QScrollArea {{
                        background-color: {darkColor2}; /* Dark grey background for the scroll area */
                        color: {darkColor5}; /* White text for better readability */
                    }}
                    QScrollArea QWidget {{
                        background-color: {darkColor2}; /* Dark grey background for the content inside the scroll area */
                        color: {darkColor5}; /* White text for better readability */
                    }}
                    QScrollBar:vertical {{
                        background-color: {darkColor2};
                    }}
                    QScrollBar:horizontal {{
                        background-color: {darkColor2};
                    }}
                    """.format(
                        darkColor1=self.darkColor1,
                        darkColor2=self.darkColor2,
                        darkColor3=self.darkColor3,
                        darkColor4=self.darkColor4,
                        darkColor5=self.darkColor5,
                        darkColor6=self.darkColor6,
                        darkColor7=self.darkColor7,
                        darkColor8=self.darkColor8,
                        darkColor9=self.darkColor9,
                        darkColor10=self.darkColor10,
                        darkColor11=self.darkColor11,
                        darkColor12=self.darkColor12,
                        darkColor13=self.darkColor13,
                    )
                self.export_dialog.setStyleSheet(stylesheet)
                self.export_dialog.show(canvas.plotItem)

    def toggle_edit_mode(self, canvas, toggle_edit_action):

        if not canvas.edit_mode:

            def create_styled_double_input_dialog(title, label1, label2, initial_value1, initial_value2):
                dialog = QDialog(self)
                dialog.setFixedWidth(150)  # Set the fixed width
                dialog.setWindowTitle(title)
                # Remove the question mark from the dialog
                dialog.setWindowFlags(dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                layout = QVBoxLayout()

                label1_widget = QLabel(label1)
                spin_box1 = QSpinBox(self)
                spin_box1.setRange(0, 10000000)
                spin_box1.setValue(initial_value1)

                label2_widget = QLabel(label2)
                spin_box2 = QSpinBox(self)
                spin_box2.setRange(0, 1000)
                spin_box2.setValue(initial_value2)

                ok_button = QPushButton("OK")
                ok_button.clicked.connect(dialog.accept)

                layout.addWidget(label1_widget)
                layout.addWidget(spin_box1)
                layout.addWidget(label2_widget)
                layout.addWidget(spin_box2)
                layout.addWidget(ok_button)

                dialog.setLayout(layout)

                if dialog.exec() == QDialog.DialogCode.Accepted:
                    return spin_box1.value(), spin_box2.value()
                else:
                    return

            # Usage example
            canvas.annotation_value, canvas.annotation_radius = create_styled_double_input_dialog(
                "Input", "Annotation value", "Annotation Radius", 1, 0
            )

        # Toggle the editing mode flag
        canvas.edit_mode = not canvas.edit_mode
        # Update the action text based on the new mode
        toggle_edit_action.setText(
            "Deactivate Editing" if canvas.edit_mode else "Activate Editing"
        )
        message = "Editing mode is now " + ("active" if canvas.edit_mode else "inactive")
        QMessageBox.information(self, "Editing Mode Status", message)

    def toggle_image_visibility(self, tab_index, tensor_key):
        # Check if the image is visible
        if tab_index in self.tensor_image_items and tensor_key in self.tensor_image_items[tab_index]:
            img_item = self.tensor_image_items[tab_index][tensor_key]

            # Assuming there's a method `isVisible()` to check visibility
            if img_item.isVisible():
                # If visible, hide the image and its color bar
                self.hide_image(tab_index, tensor_key)
            else:
                # If hidden, show the image and its color bar
                self.show_image(tab_index, tensor_key)

    def hide_image(self, tab_index, tensor_key):
        active_tab_index = self.tab_widget.currentIndex()

        # Retrieve the widget of the currently active tab
        active_tab_widget = self.tab_widget.widget(active_tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        plot_item = canvas.getPlotItem()

        # Hide the specified image
        if tab_index in self.tensor_image_items and tensor_key in self.tensor_image_items[tab_index]:
            img_item = self.tensor_image_items[tab_index][tensor_key]
            img_item.setVisible(False)  # Assuming there is a method to hide the image item

        # Hide the corresponding color bar
        if tab_index in self.cbar and tensor_key in self.cbar[tab_index]:
            color_bar = self.cbar[tab_index][tensor_key]
            color_bar.setVisible(False)

        # Find the last visible image's tensor key
        last_visible_tensor_key = None
        for key, item in reversed(self.tensor_image_items[tab_index].items()):
            if item.isVisible():
                last_visible_tensor_key = key
                break

        # Show the color bar of the last visible image
        if last_visible_tensor_key:
            last_color_bar = self.cbar[tab_index][last_visible_tensor_key]
            last_color_bar.setVisible(True)
            # Add the color bar to the layout to the right of the image
            plot_item.layout.addItem(last_color_bar, 2, 4)
        else:
            # If no images are visible, no color bar is shown
            pass

    def show_image(self, tab_index, tensor_key):
        active_tab_index = self.tab_widget.currentIndex()

        # Retrieve the widget of the currently active tab
        active_tab_widget = self.tab_widget.widget(active_tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        plot_item = canvas.getPlotItem()

        # Show the specified image
        if tab_index in self.tensor_image_items and tensor_key in self.tensor_image_items[tab_index]:
            img_item = self.tensor_image_items[tab_index][tensor_key]
            img_item.setVisible(True)  # Assuming there is a method to show the image item

        # Hide all other color bars first
        for key in self.cbar.get(tab_index, {}):
            other_color_bar = self.cbar[tab_index][key]
            if other_color_bar is not None:
                other_color_bar.setVisible(False)

        # Find the last visible image's tensor key, including the one being shown
        last_visible_tensor_key = None
        for key, item in reversed(self.tensor_image_items[tab_index].items()):
            if item.isVisible():
                last_visible_tensor_key = key
                break

        # Show the color bar of the last visible image
        if last_visible_tensor_key:
            last_color_bar = self.cbar[tab_index][last_visible_tensor_key]
            last_color_bar.setVisible(True)
            # Add the color bar to the layout to the right of the image
            plot_item.layout.addItem(last_color_bar, 2, 4)

    def remove_image(self, tab_index, tensor_key):
        img_item = self.tensor_image_items[tab_index].pop(tensor_key, None)
        active_tab_index = self.tab_widget.currentIndex()

        # Retrieve the widget of the currently active tab
        active_tab_widget = self.tab_widget.widget(active_tab_index)

        # Assuming the canvas is the first widget in the layout of the active tab
        canvas_layout = active_tab_widget.layout()
        canvas = canvas_layout.itemAt(0).widget()

        if img_item is not None:
            canvas.plotItem.removeItem(img_item)

        # Remove the corresponding color bar
        if tab_index in self.cbar and tensor_key in self.cbar[tab_index]:
            color_bar = self.cbar[tab_index].pop(tensor_key, None)
            if color_bar is not None:
                canvas.plotItem.layout.removeItem(color_bar)

        # Check for the last visible image
        last_visible_tensor_key = None
        for key, item in self.tensor_image_items.get(tab_index, {}).items():
            if item.isVisible():
                last_visible_tensor_key = key

        # Determine which color bar to show
        if last_visible_tensor_key:
            # Show the color bar of the last visible image
            if last_visible_tensor_key in self.cbar[tab_index]:
                last_color_bar = self.cbar[tab_index][last_visible_tensor_key]
                last_color_bar.setVisible(True)
                # Add the color bar to the layout to the right of the image
                canvas.plotItem.layout.addItem(last_color_bar, 2, 4)

        # If there are no more images left on the canvas, clear the entire canvas
        if not self.tensor_image_items[tab_index]:
            canvas.edit_mode = False
            # Hide all color bars for the current active tab index
            if active_tab_index in self.cbar:
                for tensor_key in self.cbar[active_tab_index]:
                    color_bar = self.cbar[active_tab_index][tensor_key]
                    if color_bar is not None:
                        color_bar.hide()

            # Hide all axes and clear the title on the canvas
            canvas.plotItem.hideAxis('top')
            canvas.plotItem.hideAxis('bottom')
            canvas.plotItem.hideAxis('left')
            canvas.plotItem.setTitle('')

    # Function to apply the LUT to the colorBarItem and update the dictionary
    def applyLUT(self, colorBarItem, colorMap, active_tab_index, tensor_key):
        colorBarItem.setColorMap(colorMap)
        # Ensure that changes are reflected in the dictionary
        if active_tab_index in self.cbar:
            self.cbar[active_tab_index][tensor_key] = colorBarItem
        else:
            self.cbar[active_tab_index] = {tensor_key: colorBarItem}

    # Function to add custom color mapping options to the context menu
    def addCustomColorMappingSubMenu(self, context_menu, active_tab_index, tensor_key):
        # Define your custom color mappings here
        customColorMappings = {
            'Seismic Default': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
            'Polarity': [(0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 1.0, 1.0, 1.0),
                         (0.7490196078431373, 0.0, 0.0, 1.0)],
            'Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0),
                          (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                          (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)],
            'Cosine of Phase': [(0.0, 0.0, 0.0, 1.0), (1.0, 1.0, 1.0, 1.0)],
            'Ins. Phase': [(1.0, 0.4117647058823529, 0.7058823529411765, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                           (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                           (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)]
        }

        # Create the custom color mapping submenu
        customColorMapMenu = context_menu.addMenu('Custom Color Mapping')

        # Populate the submenu with custom color map options
        for cmapName, colors in customColorMappings.items():
            action = customColorMapMenu.addAction(cmapName)
            action.triggered.connect(
                lambda checked, colors=colors: self.createAndApplyLUT(active_tab_index, tensor_key, colors))

    # Function to create a LUT from a list of colors and apply it to the colorBarItem
    def createAndApplyLUT(self, active_tab_index, tensor_key, colors):
        # Create a custom colormap for the color bar visualization
        custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', colors, N=256)

        # Extract the color values including alpha
        color_values = custom_cmap(np.linspace(0, 1, 256))

        # Convert colormap values to PyQtGraph's ColorMap format
        pg_colormap = pg.ColorMap(np.linspace(0, 1, 256),
                                  [pg.mkColor((c[0] * 255, c[1] * 255, c[2] * 255, c[3] * 255)) for
                                   c in color_values])

        # Apply the LUT to the correct colorBarItem
        if active_tab_index in self.cbar and tensor_key in self.cbar[active_tab_index]:
            colorBarItem = self.cbar[active_tab_index][tensor_key]
            self.applyLUT(colorBarItem, pg_colormap, active_tab_index, tensor_key)

    def choose_color(self):
        color = QColorDialog.getColor(parent=self)
        if color.isValid():
            # Get the index of the currently active tab
            active_tab_index = self.tab_widget.currentIndex()
            # Retrieve the widget of the currently active tab
            active_tab_widget = self.tab_widget.widget(active_tab_index)
            # Assuming the canvas is the first widget in the layout of the active tab
            canvas_layout = active_tab_widget.layout()
            canvas = canvas_layout.itemAt(0).widget()
            # Assuming you want to set the color for the RoundedCanvas background
            canvas.setBackground(color.name())

    def three_d(self):
        self.dim1_entry.clear()
        self.dim2_entry.clear()
        self.index_dim_entry.clear()
        self.Three_D_button.setChecked(True)
        self.index_label.hide()
        self.index_entry.hide()
        self.index_slider.hide()

    def toggle_canvas(self, view_type):
        current_index = self.tab_widget.currentIndex()
        self.tab_widget.currentChanged.connect(self.on_tab_change)
        current_widget = self.tab_widget.widget(current_index)
        if isinstance(current_widget, QWidget):
            current_layout = current_widget.layout()
            if current_layout:
                canvas = current_layout.itemAt(0).widget()
                if view_type == '3D' and isinstance(canvas, RoundedCanvas):
                    # Switch to OpenGL canvas
                    opengl_canvas = CustomVTKWidget(parent=self)
                    current_layout.addWidget(opengl_canvas)
                    current_layout.removeWidget(canvas)
                    canvas.deleteLater()  # Remove the previous canvas from memory
                    # Update the dictionaries with references to the new canvas items
                    self.grid_color[current_index] = self.last_grid_color
                    self.view_type[current_index] = '3D'
                    self.plot_button.clicked.disconnect()  # Disconnect previous signal
                    self.plot_button.clicked.connect(self.plot_data3d)  # Connect to 3D plot function
                    self.tab_3D_state[current_index] = False  # Initialize the state as no 3D plot displayed
                    if isinstance(self.tensor_data, np.ndarray):
                        self.Three_D_button.show()
                    self.three_d()
                    self.tensor_image_items.pop(current_index, None)
                    self.update_active_tab_name()
                    self.update_menu_action_text(current_index)
                    if current_index in self.grids:
                        del self.grids[current_index]
                    if not isinstance(self.tensor_data, np.ndarray):
                        self.time_slice_button.hide()
                        self.cross_line_button.hide()
                        self.inline_button.hide()
                        self.channel_index_entry.hide()
                        self.channel_index_label.hide()
                        self.Channel_slider.hide()
                        self.Three_D_button.hide()
                        self.plot_button.clicked.disconnect()  # Disconnect previous signal
                        self.plot_button.clicked.connect(self.well_log_viewer)
                elif view_type == '2D' and isinstance(canvas, CustomVTKWidget):
                    # Switch to rounded canvas
                    rounded_canvas = RoundedCanvas(self)
                    current_layout.addWidget(rounded_canvas)
                    current_layout.removeWidget(canvas)
                    canvas.deleteLater()  # Remove the previous canvas from memory
                    self.plot_button.clicked.disconnect()  # Disconnect previous signal
                    self.plot_button.clicked.connect(self.plot_tensor)
                    self.Three_D_button.hide()
                    self.three_d()
                    self.tab_volume_items.pop(current_index, None)
                    self.cbar[current_index] = None
                    self.view_type[current_index] = '2D'
                    self.grid_color[current_index] = self.last_grid_color
                    if current_index in self.grids:
                        del self.grids[current_index]
                    rounded_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                    rounded_canvas.customContextMenuRequested.connect(self.show_context_menu2)
                    self.update_active_tab_name()
                    self.update_menu_action_text(current_index)
                    # Check if the loaded tensor has sampling_interval
                    if not isinstance(self.tensor_data, np.ndarray):
                        self.time_slice_button.hide()
                        self.cross_line_button.hide()
                        self.inline_button.hide()
                        self.channel_index_entry.hide()
                        self.channel_index_label.hide()
                        self.Channel_slider.hide()
                        self.Three_D_button.hide()
                        self.plot_button.clicked.disconnect()  # Disconnect previous signal
                        self.plot_button.clicked.connect(self.well_log_viewer)

    def add_tab(self):
        new_tab = QWidget()  # Create a new tab widget
        new_tab_layout = QVBoxLayout(new_tab)  # Create a layout for the new tab
        current_index = self.tab_widget.currentIndex()
        current_widget = self.tab_widget.widget(current_index)
        new_tab_index = self.tab_widget.count()  # Index for the new tab

        if isinstance(current_widget, QWidget):
            current_layout = current_widget.layout()
            if current_layout:
                canvas = current_layout.itemAt(0).widget()
                if isinstance(canvas, RoundedCanvas):
                    new_canvas = RoundedCanvas(self)
                    tab_name = "2D Plot"  # Name for tabs with RoundedCanvas
                    new_canvas.setContextMenuPolicy(Qt.ContextMenuPolicy.CustomContextMenu)
                    new_canvas.customContextMenuRequested.connect(self.show_context_menu2)
                    self.view_type[new_tab_index] = '2D'
                    self.cbar[new_tab_index] = None
                elif isinstance(canvas, CustomVTKWidget):
                    new_canvas = CustomVTKWidget(parent=self)
                    tab_name = "3D Plot"  # Name for tabs with CustomVTKWidget
                    self.view_type[new_tab_index] = '3D'
                    self.tab_3D_state[new_tab_index] = False  # Initialize the state as no 3D plot displayed
                new_tab_layout.addWidget(new_canvas)
                # Add the new tab to the tab widget with the next sequential number
                self.tab_widget.addTab(new_tab, f"{tab_name} {self.tab_widget.count() + 1} ")

                if isinstance(new_canvas, CustomVTKWidget):
                    self.grid_color[new_tab_index] = self.last_grid_color

                self.update_tab_name()  # Update all tab names to be sequential
                self.update_active_tab_name()
                self.tab_widget.setCurrentIndex(new_tab_index)

    def update_tab_name(self):
        # Update the names of all tabs to be sequential
        for index in range(self.tab_widget.count()):
            current_tab_name = self.tab_widget.tabText(index)
            tab_type = current_tab_name.split()[0]  # Extract the type (2D or 3D) from the current tab name
            # Update the tab name with the new sequential number
            self.tab_widget.setTabText(index, f"{tab_type} Plot {index + 1} ")

    def update_active_tab_name(self):
        current_index = self.tab_widget.currentIndex()
        current_tab_name = self.tab_widget.tabText(current_index)
        tab_number = current_tab_name.split()[-1]  # Extract the number from the current tab name

        # Get the current widget in the tab
        current_widget = self.tab_widget.widget(current_index)
        if current_widget.layout():
            # Get the first widget in the layout, which should be the canvas
            canvas = current_widget.layout().itemAt(0).widget()
            if isinstance(canvas, CustomVTKWidget):
                # If the current canvas is CustomVTKWidget, it's 3D
                self.tab_widget.setTabText(current_index, f"3D Plot {tab_number} ")
            elif isinstance(canvas, RoundedCanvas):
                # If the current canvas is RoundedCanvas, it's 2D
                self.tab_widget.setTabText(current_index, f"2D Plot {tab_number} ")

    def on_tab_change(self, current_index):
        current_widget = self.tab_widget.widget(current_index)
        if isinstance(current_widget, QWidget):
            current_layout = current_widget.layout()
            if current_layout:
                canvas = current_layout.itemAt(0).widget()
                # Update connections for the plot button
                self.plot_button.clicked.disconnect()
                # Connect to the appropriate plotting function based on the type of canvas
                if isinstance(canvas, CustomVTKWidget):
                    self.plot_button.clicked.connect(self.plot_data3d)
                    if isinstance(self.tensor_data, np.ndarray):
                        self.Three_D_button.show()
                    self.three_d()
                    self.update_menu_action_text(current_index)
                elif isinstance(canvas, RoundedCanvas):
                    self.plot_button.clicked.connect(self.plot_tensor)
                    self.Three_D_button.hide()
                    self.three_d()
                    self.update_menu_action_text(current_index)
                if not isinstance(self.tensor_data, np.ndarray):
                    self.time_slice_button.hide()
                    self.cross_line_button.hide()
                    self.inline_button.hide()
                    self.channel_index_entry.hide()
                    self.channel_index_label.hide()
                    self.Channel_slider.hide()
                    self.Three_D_button.hide()
                    self.plot_button.clicked.disconnect()  # Disconnect previous signal
                    self.plot_button.clicked.connect(self.well_log_viewer)

    def close_tab(self, closed_tab_index):
        # Remove the tab at closed_tab_index
        self.tab_widget.removeTab(closed_tab_index)

        # Update dictionaries for remaining tabs
        for dictionary in [self.grids,
                           self.tab_volume_items,
                           self.grid_color, self.view_type, self.tab_3D_state,
                           self.cbar, self.tensor_image_items, self.tensor_metadata]:
            # Create a new dictionary to store updated keys and values
            updated_dict = {}
            for key, value in dictionary.items():
                # Decrement the keys by 1 for tabs after the closed tab
                if key > closed_tab_index:
                    updated_dict[key - 1] = dictionary[key]
                elif key < closed_tab_index:
                    updated_dict[key] = dictionary[key]
            # Replace the old dictionary with the updated dictionary
            dictionary.clear()
            dictionary.update(updated_dict)

        # Update tab names and any other necessary UI elements
        self.update_tab_name()
        self.update_active_tab_name()
        self.update_menu_action_text(closed_tab_index)

    def connect_slider_to_index_dimension(self):
        # Get the value of the index dimension from the index_dim_entry
        index_dim_value = int(self.index_dim_entry.text())

        # Get the shape of the tensor data
        tensor_shape = self.tensor_data.shape
        metadata = self.metadata[self.file_name]

        # Get the maximum value for the slider from the shape of the tensor data
        max_value = tensor_shape[index_dim_value] - 1

        if index_dim_value == 0:
            sampling_interval = metadata.get('sampling_interval_ms')
            max_slider_value = int(max_value * sampling_interval) + max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
            min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
            step = sampling_interval
        else:
            if index_dim_value == 1:
                axis_range = metadata.get('inline_range', (0, max_value))
            else:
                axis_range = metadata.get('xline_range', (0, max_value))

            max_slider_value = axis_range[1]
            min_slider_value = axis_range[0]
            step = (axis_range[1] - axis_range[0]) / (max_value) if \
                tensor_shape[index_dim_value] > 1 else 1

        # Set the maximum value of the slider
        self.index_slider.setMaximum(int(max_slider_value))
        self.index_slider.setMinimum(int(min_slider_value))
        self.index_slider.setSingleStep(int(step))

        # Set the maximum value of the spinbox
        self.index_entry.setMaximum(int(max_slider_value))
        self.index_entry.setMinimum(int(min_slider_value))
        self.index_entry.setSingleStep(int(step))
        self.index_entry.setValue(int(min_slider_value))

    def set_time_slice(self):
        if self.time_slice_button.isChecked():
            self.dim1_entry.setText("1")
            self.dim2_entry.setText("2")
            self.index_dim_entry.setText("0")
            self.cross_line_button.setChecked(False)
            self.inline_button.setChecked(False)
            self.connect_slider_to_index_dimension()
            if self.index_slider.maximum() > 0:
                self.index_label.show()
                self.index_entry.show()
                self.index_slider.show()
        else:
            self.dim1_entry.clear()
            self.dim2_entry.clear()
            self.index_dim_entry.clear()

    def set_cross_line(self):
        if self.cross_line_button.isChecked():
            self.dim1_entry.setText("1")
            self.dim2_entry.setText("0")
            self.index_dim_entry.setText("2")
            self.time_slice_button.setChecked(False)
            self.inline_button.setChecked(False)
            self.connect_slider_to_index_dimension()
            if self.index_slider.maximum() > 0:
                self.index_label.show()
                self.index_entry.show()
                self.index_slider.show()
        else:
            self.dim1_entry.clear()
            self.dim2_entry.clear()
            self.index_dim_entry.clear()

    def set_inline(self):
        if self.inline_button.isChecked():
            self.dim1_entry.setText("2")
            self.dim2_entry.setText("0")
            self.index_dim_entry.setText("1")
            self.time_slice_button.setChecked(False)
            self.cross_line_button.setChecked(False)
            self.connect_slider_to_index_dimension()
            if self.index_slider.maximum() > 0:
                self.index_label.show()
                self.index_entry.show()
                self.index_slider.show()
        else:
            self.dim1_entry.clear()
            self.dim2_entry.clear()
            self.index_dim_entry.clear()

    def load_tensor(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Open NumPy Tensor File", "", "NumPy Files (*.npy *.npz)")
        # Check if the file dialog was canceled and no file was selected
        if not file_path:
            return  # Exit the method without changing anything
        else:
            self.three_d()
            # Add the loaded file path to the dictionary
            self.file_name = os.path.splitext(os.path.basename(file_path))[0].strip()
            self.loaded_file_paths[self.file_name] = file_path
            self.update_recent_files_submenu()  # Update the recent files submenu
            try:
                QApplication.setOverrideCursor(self.custom_cursor)

                if file_path.endswith('.npy'):
                    self.tensor_data = np.load(file_path)
                    # Check if the tensor is 3D
                    if self.tensor_data.ndim == 3:
                        # Add a test double channel dimension as the last dimension
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                    elif self.tensor_data.ndim == 2:
                        # Add two test double dimensions at the end for 2D tensor
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                    # Create a new dialog to select Template Type
                    template_dialog = QDialog(self)
                    template_dialog.setWindowTitle("Select Template Type")
                    layout = QVBoxLayout(template_dialog)
                    template_type_combo = QComboBox()
                    template_type_combo.addItems([
                        'Seismic',
                        'App. Polarity',
                        'Inst. Frequency',
                        'Cos Phase',
                        'Inst. Phase',
                        'Envelope',
                        'Inst. Bandwidth',
                        'Dom. Frequency',
                        'Sweetness',
                        'RMS Amplitude',
                        'Geobodies',
                        'Anomalies',
                        'Probe',
                        'Upscaled',
                        'Clusters',
                        'Zones',
                        'Annotations',
                        'Other'
                    ])

                    layout.addWidget(template_type_combo)

                    select_button = QPushButton("OK")
                    select_button.clicked.connect(template_dialog.accept)  # Close the current dialog
                    layout.addWidget(select_button)

                    template_dialog.setLayout(layout)
                    template_dialog.adjustSize()
                    template_dialog.setMinimumWidth(225)
                    # Disable the '?' help button on the dialog
                    template_dialog.setWindowFlags(
                        template_dialog.windowFlags() & ~Qt.WindowType.WindowContextHelpButtonHint)

                    QApplication.restoreOverrideCursor()
                    result = template_dialog.exec()  # Use exec() to block until the dialog is closed
                    if result == QDialog.DialogCode.Accepted:

                        if template_type_combo.currentText() == 'Other':
                            template_name, ok = QInputDialog.getText(self, "Input Dialog", "Enter Template name:")
                            if ok:
                                template_type = template_name
                            else:
                                template_type = 'Seismic'
                        else:
                            template_type = template_type_combo.currentText()
                    else:

                        template_type = 'Seismic'

                    number, ok = QInputDialog.getInt(self, "Input Sampling Interval",
                                                     "Enter Sampling Interval (ms):",
                                                     1, 0, 100)

                    source_name, ok = QInputDialog.getText(self, "Input Dialog", "Enter Dataset name:",
                                                           text=self.file_name)

                    QApplication.setOverrideCursor(self.custom_cursor)

                    self.metadata[self.file_name] = {}
                    self.metadata[self.file_name]['name'] = source_name
                    self.metadata[self.file_name]['sampling_interval_ms'] = number
                    self.metadata[self.file_name]['template'] = template_type
                else:
                    # Load the NPZ file
                    data = np.load(file_path, allow_pickle=True)

                    # Access the tensor and metadata
                    self.tensor_data = data['tensor']
                    # Check if the tensor is 3D
                    if self.tensor_data.ndim == 3:
                        # Add a test double channel dimension as the last dimension
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                    elif self.tensor_data.ndim == 2:
                        # Add two test double dimensions at the end for 2D tensor
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                        self.tensor_data = np.expand_dims(self.tensor_data, axis=-1)
                    self.metadata[self.file_name] = data[
                        'metadata'].item()  # .item() converts the metadata back to a dictionary

                if self.metadata[self.file_name].get('inline_range') is None:
                    self.metadata[self.file_name]['inline_range'] = (0, self.tensor_data.shape[1]-1)

                if self.metadata[self.file_name].get('xline_range') is None:
                    self.metadata[self.file_name]['xline_range'] = (0, self.tensor_data.shape[2]-1)

                if self.metadata[self.file_name].get('x_range') is None:
                    self.metadata[self.file_name]['x_range'] = (0, self.tensor_data.shape[2]-1, self.tensor_data.shape[2]-1)

                if self.metadata[self.file_name].get('y_range') is None:
                    self.metadata[self.file_name]['y_range'] = (0, self.tensor_data.shape[1]-1, self.tensor_data.shape[1]-1)

                if self.metadata[self.file_name].get('time_range') is None:
                    self.metadata[self.file_name]['time_range'] = (
                        (-(self.tensor_data.shape[0] - 1) * self.metadata[self.file_name]['sampling_interval_ms']), 0,
                        (self.tensor_data.shape[0] - 1) * self.metadata[self.file_name]['sampling_interval_ms'])

                if self.metadata[self.file_name].get('origin') is None:
                    self.metadata[self.file_name]['origin'] = (0, 0)

                if self.metadata[self.file_name].get('xline_end') is None:
                    self.metadata[self.file_name]['xline_end'] = (self.tensor_data.shape[2] - 1, 0)

                if self.metadata[self.file_name].get('inline_end') is None:
                    self.metadata[self.file_name]['inline_end'] = (0, self.tensor_data.shape[1] - 1)

                self.add_tensor(self.file_name, self.tensor_data)

                if self.tensor_data.shape[-1] > 1:
                    self.set_channel_slider_range(self.tensor_data.shape[-1])
                    self.channel_index_label.show()
                    self.channel_index_entry.show()
                    self.Channel_slider.show()
                else:
                    self.channel_index_label.hide()
                    self.channel_index_entry.hide()
                    self.Channel_slider.hide()

                if self.seismic_calc_dialogue:
                    self.update_array_selectors(self.seismic_calc_dialogue.array_selector)

                QApplication.restoreOverrideCursor()

            except Exception as e:
                QApplication.restoreOverrideCursor()
                traceback.print_exc()
                QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def gst_gpu_task(data, sigma):
        def gradients_gpu(seismic_gpu, sigma):
            """Builds a 4-d array of the gaussian gradient of *seismic* on GPU."""
            grads = []
            for axis in range(3):
                grad = gpu_ndimage.gaussian_filter1d(seismic_gpu, sigma, axis=axis, order=1)
                grads.append(grad[..., cp.newaxis])
            return cp.concatenate(grads, axis=3)

        def moving_window4d_gpu(grad, window, func):
            """GPU version of moving window operation matching CPU implementation exactly."""
            half_window = [(x // 2, x // 2) for x in window] + [(0, 0)]
            padded = cp.pad(grad, half_window, mode='reflect')

            out = cp.empty(grad.shape[:3], dtype=cp.float32)

            kernel_code = r'''
                    #define M_PI 3.14159265358979323846f
                extern "C" __global__
                void process_window(const float* padded, float* out,
                                   int ni, int nj, int nk, int window_i, int window_j, int window_k,
                                   int pad_nj, int pad_nk) {
                    int i = blockIdx.x * blockDim.x + threadIdx.x;
                    int j = blockIdx.y * blockDim.y + threadIdx.y;
                    int k = blockIdx.z * blockDim.z + threadIdx.z;

                    if (i < ni && j < nj && k < nk) {
                        // First reshape the region into a matrix (n_points x 3)
                        float region[27][3];  // Max window size of 3x3x3
                        int n_points = 0;

                        for (int wi = 0; wi < window_i; wi++) {
                            for (int wj = 0; wj < window_j; wj++) {
                                for (int wk = 0; wk < window_k; wk++) {
                                    int pi = i + wi;
                                    int pj = j + wj;
                                    int pk = k + wk;

                                    // Copy the gradient vector
                                    for (int g = 0; g < 3; g++) {
                                        region[n_points][g] = padded[(pi * pad_nj * pad_nk + pj * pad_nk + pk) * 3 + g];
                                    }
                                    n_points++;
                                }
                            }
                        }

                        // Compute GST = region.T @ region (3x3 matrix)
                        float gst[9] = {0};  // Row-major order
                        for (int r = 0; r < 3; r++) {
                            for (int c = 0; c < 3; c++) {
                                float sum = 0.0f;
                                for (int p = 0; p < n_points; p++) {
                                    sum += region[p][r] * region[p][c];
                                }
                                gst[r * 3 + c] = sum;
                            }
                        }

                        // Compute eigenvalues using characteristic equation for 3x3 matrix
                        float p1 = gst[1] * gst[1] + gst[2] * gst[2] + gst[5] * gst[5];
                        float q = (gst[0] + gst[4] + gst[8]) / 3.0f;
                        float p2 = (gst[0] - q) * (gst[0] - q) + (gst[4] - q) * (gst[4] - q) + 
                                  (gst[8] - q) * (gst[8] - q) + 2.0f * p1;
                        float p = sqrt(p2 / 6.0f);

                        float det = gst[0] * (gst[4] * gst[8] - gst[5] * gst[7]) -
                                   gst[1] * (gst[3] * gst[8] - gst[5] * gst[6]) +
                                   gst[2] * (gst[3] * gst[7] - gst[4] * gst[6]);

                        float r = det / (2.0f * p * p * p);
                        r = fmax(fmin(r, 1.0f), -1.0f);

                        float phi = acos(r) / 3.0f;

                        // The three eigenvalues
                        float eig1 = q + 2.0f * p * cos(phi);
                        float eig2 = q + 2.0f * p * cos(phi + 2.0f * M_PI / 3.0f);
                        float eig3 = q + 2.0f * p * cos(phi + 4.0f * M_PI / 3.0f);

                        // Sort eigenvalues in descending order
                        if (eig1 < eig2) { float tmp = eig1; eig1 = eig2; eig2 = tmp; }
                        if (eig1 < eig3) { float tmp = eig1; eig1 = eig3; eig3 = tmp; }
                        if (eig2 < eig3) { float tmp = eig2; eig2 = eig3; eig3 = tmp; }

                        // Compute coherence exactly as in CPU version
                        float denom = eig1 + eig2;
                        if (denom > 1e-10f) {
                            out[i * nj * nk + j * nk + k] = (eig1 - eig2) / denom;
                        } else {
                            out[i * nj * nk + j * nk + k] = 0.0f;
                        }
                    }
                }
                '''

            module = cp.RawModule(code=kernel_code)
            kernel = module.get_function('process_window')

            block_dim = (8, 8, 8)
            grid_dim = (
                (grad.shape[0] + block_dim[0] - 1) // block_dim[0],
                (grad.shape[1] + block_dim[1] - 1) // block_dim[1],
                (grad.shape[2] + block_dim[2] - 1) // block_dim[2]
            )

            kernel(grid_dim, block_dim, (
                padded, out,
                grad.shape[0], grad.shape[1], grad.shape[2],
                window[0], window[1], window[2],
                padded.shape[1], padded.shape[2]
            ))

            return out

        def gst_coherence_gpu(data, window, sigma=1):
            """GPU-optimized version of GST coherence calculation."""
            seismic_gpu = cp.asarray(np.transpose((np.squeeze(data, axis=-1)), (1, 2, 0)))
            grad = gradients_gpu(seismic_gpu, sigma)
            coherence_gpu = moving_window4d_gpu(grad, window, None)
            return cp.asnumpy(coherence_gpu)

        return gst_coherence_gpu(data, (3, 3, 3), sigma)

    def gst_gpu(self):
        try:
            if not isinstance(self.tensor_data, np.ndarray):
                return

            # Create an integer input dialog with a default value
            sigma, ok = QInputDialog.getInt(
                self,
                "Sigma Size",
                f"Enter the Sigma size:",
                value=1,
                min=1,
                max=100  # Ensure ZWIN is less than the number of depth samples
            )
            if not ok:
                return

            QApplication.setOverrideCursor(self.custom_cursor)

            data = self.tensor_data.astype(np.float32)

            if gpu_available:
                # Check available GPU memory before allocating
                free_memory, total_memory = cp.cuda.Device().mem_info
                required_memory = data.nbytes

                # Safety margin (e.g., 80% of free memory)
                safety_margin = 0.8
                if required_memory > free_memory * safety_margin / 2:
                    cp.get_default_memory_pool().free_all_blocks()

                    QApplication.restoreOverrideCursor()
                    QMessageBox.critical(self, "Critical Error",
                                         f"Error: Data failed to fit in the GPU. The required memory is "
                                         f"{required_memory * 2 / (1024 ** 2):.2f} MB, but only"
                                         f" {free_memory * safety_margin / (1024 ** 2):.2f} MB is available.")
                    return
            else:
                QApplication.restoreOverrideCursor()
                QMessageBox.critical(self, "Critical Error",
                                     "Error: No GPU detected."
                                     " Please ensure a compatible GPU is installed and accessible.")
                return

            # Compute total sample count
            num_samples = np.prod(data.shape, dtype=np.int64)

            if num_samples <= self.multiprocessing_threshold:
                result_gpu = TensorVisualizer.gst_gpu_task(data, sigma)
                QApplication.restoreOverrideCursor()
            else:
                QApplication.restoreOverrideCursor()
                result_gpu = self.task_runner.run_task(TensorVisualizer.gst_gpu_task, data, sigma)

            if result_gpu is not None:
                self.tensor_data = np.expand_dims(np.transpose(result_gpu, (2, 0, 1)), axis=-1)
                # Store the modified tensor with an informative key
                self.metadata[f"{self.file_name}_GST"] = self.metadata[
                    self.file_name].copy()
                self.metadata[f"{self.file_name}_GST"][
                    'template'] = "GST"
                self.add_tensor(f"{self.file_name}_GST", self.tensor_data)

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def threshold_4d_data(self):
        """
        Apply thresholding to a 4D NumPy tensor with dimensions (time, height, width, channel).

        Args:
            data (np.ndarray): 4D input data array of shape (time, height, width, channel).
                               Data should be continuous.
            app (QApplication, optional): An existing PyQt6 application, if running in a GUI environment.

        Returns:
            np.ndarray: A 4D binary array of the same shape as the input, with values 0 and 1.
        """
        try:
            QApplication.setOverrideCursor(self.custom_cursor)

            data = np.copy(self.tensor_data)

            assert data.ndim == 4, "Input data must be 4D (time, height, width, channel)."

            # Normalize the data between 0 and 1
            data_min, data_max = data.min(), data.max()
            scaled_data = (data - data_min) / (data_max - data_min)

            # Get the number of channels
            time, height, width, channels = data.shape
            binary_data = np.zeros_like(scaled_data, dtype=np.int16)

            for c in range(channels):
                channel_data = scaled_data[:, :, :, c]

                # Compute Otsu's threshold for the current channel
                otsu_threshold = threshold_otsu(channel_data)

                # Show input dialog to adjust the threshold
                QApplication.restoreOverrideCursor()
                dialog_text = (
                    f"Channel {c + 1}/{channels}\n"
                    f"Otsu suggested threshold: {otsu_threshold:.4f}\n"
                    f"Adjust threshold (between 0 and 1):"
                )
                input_threshold, ok = QInputDialog.getDouble(
                    self,
                    "Set Threshold",
                    dialog_text,
                    value=otsu_threshold,
                    min=0.0,
                    max=1.0,
                    decimals=4
                )

                QApplication.setOverrideCursor(self.custom_cursor)

                # If user cancels, use the default Otsu threshold
                if not ok:
                    QApplication.restoreOverrideCursor()
                    return

                # Threshold the channel data
                binary_data[:, :, :, c] = (channel_data >= input_threshold)

            self.tensor_data = binary_data
            self.metadata[f"{self.file_name}_Binary Thresholded {input_threshold}"] = self.metadata[
                self.file_name].copy()
            self.metadata[f"{self.file_name}_Binary Thresholded {input_threshold}"][
                'template'] = f"Binary Thresholded {input_threshold} {self.metadata[f'{self.file_name}_Binary Thresholded {input_threshold}'].get('template', '')}"
            self.add_tensor(f"{self.file_name}_Binary Thresholded {input_threshold}", self.tensor_data)
            QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    @staticmethod
    def gpu_3d_interpolation_task(data, three_d_interpolation, order):
        # Use CuPy for GPU-based interpolation
        data_gpu = cp.array(data)  # Move data to GPU
        interpolated_data = zoom(data_gpu, zoom=three_d_interpolation, order=order)
        return cp.asnumpy(interpolated_data)

    @staticmethod
    def cpu_3d_interpolation_task(data, three_d_interpolation, order):
        return ndimage.zoom(data, zoom=three_d_interpolation, order=order)

    def plot_data3d(self):
        try:
            if isinstance(self.tensor_data, np.ndarray):
                QApplication.setOverrideCursor(self.custom_cursor)
                # Get the index of the currently active tab
                active_tab_index = self.tab_widget.currentIndex()

                # Retrieve the widget of the currently active tab
                active_tab_widget = self.tab_widget.widget(active_tab_index)

                # Assuming the canvas is the first widget in the layout of the active tab
                canvas_layout = active_tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()

                if hasattr(self, "tab_volume_items") and active_tab_index in self.tab_volume_items:
                    for item in list(self.tab_volume_items[active_tab_index]):
                        if hasattr(item, 'transparent'):
                            if item.template == self.color_mapp:
                                # Remove the item from the renderer
                                if hasattr(item, "SetVisibility"):  # For volumes
                                    self.tab_volume_items[active_tab_index].remove(item)
                                    canvas.renderer.RemoveActor(item)
                                    if hasattr(item, 'color_bar'):
                                        canvas.renderer.RemoveActor2D(item.color_bar)
                                    del item
                    # Check if this was the last item in the list
                    if not self.tab_volume_items[active_tab_index]:
                        if active_tab_index in self.grids:
                            grid = self.grids.pop(active_tab_index)  # Remove grid from the dictionary
                            canvas.renderer.RemoveActor(grid)  # Remove grid from the renderer

                # Get the state for the current tab
                is_3D_displayed = self.tab_3D_state.get(active_tab_index, False)

                # Retrieve the file name for the current data for the active tab
                current_data_name = self.tab_last_plotted_data_name.get(active_tab_index)

                d_key = self.file_name

                # Compare the file names instead of the full file paths
                if current_data_name != self.metadata[d_key].get('name') if self.metadata[d_key].get('name') else\
                        self.metadata[d_key].get('source'):
                    # Get the renderer of the QVTK canvas
                    # Removes all actors from the renderer
                    if hasattr(self, "tab_volume_items") and active_tab_index in self.tab_volume_items:
                        for item in list(self.tab_volume_items[active_tab_index]):
                            self.tab_volume_items[active_tab_index].remove(item)
                            # Remove the item from the renderer
                            if hasattr(item, "SetVisibility"):  # For volumes
                                canvas.renderer.RemoveActor(item)
                                if hasattr(item, 'color_bar'):
                                    canvas.renderer.RemoveActor2D(item.color_bar)
                            elif hasattr(item, "SetEnabled"):  # For plain widgets (e.g., vtkImagePlaneWidget)
                                item.SetEnabled(0)
                                if hasattr(item, 'color_bar'):
                                    canvas.renderer.RemoveActor2D(item.color_bar)
                            del item
                        # Check if this was the last item in the list
                        if not self.tab_volume_items[active_tab_index]:
                            if active_tab_index in self.grids:
                                grid = self.grids.pop(active_tab_index)  # Remove grid from the dictionary
                                canvas.renderer.RemoveActor(grid)  # Remove grid from the renderer
                    self.tab_3D_state[active_tab_index] = False  # Update the state in the dictionary

                    # Update the dictionary with the new file name
                    self.tab_last_plotted_data_name[active_tab_index] = self.metadata[d_key].get('name') if \
                        self.metadata[d_key].get('name') else\
                        self.metadata[d_key].get('source')

                # Check if a 3D plot is currently displayed and a 2D slice is to be plotted
                if is_3D_displayed and any([self.time_slice_button.isChecked(), self.cross_line_button.isChecked(),
                                            self.inline_button.isChecked()]):
                    # Clear the 3D plot from the canvas
                    if hasattr(self, "tab_volume_items") and active_tab_index in self.tab_volume_items:
                        for item in list(self.tab_volume_items[active_tab_index]):
                            if not hasattr(item, 'transparent'):
                                # Remove the item from the renderer
                                if hasattr(item, "SetVisibility"):  # For volumes
                                    self.tab_volume_items[active_tab_index].remove(item)
                                    canvas.renderer.RemoveActor(item)
                                    if hasattr(item, 'color_bar'):
                                        canvas.renderer.RemoveActor2D(item.color_bar)
                                    del item
                        # Check if this was the last item in the list
                        if not self.tab_volume_items[active_tab_index]:
                            if active_tab_index in self.grids:
                                grid = self.grids.pop(active_tab_index)  # Remove grid from the dictionary
                                canvas.renderer.RemoveActor(grid)  # Remove grid from the renderer

                    self.tab_3D_state[active_tab_index] = False  # Update the state in the dictionary

                data = self.tensor_data
                metadata = self.metadata[self.file_name]

                if np.issubdtype(data.dtype, np.floating):
                    if data.dtype not in (np.float16, np.float32):
                        data = data.astype(np.float32)  # Convert only if not already float16/float32
                    else:
                        data = np.copy(data)  # Copy if dtype remains unchanged
                elif np.issubdtype(data.dtype, np.bool_):
                    data = data.astype(np.int16)  # Convert bool → int32 (no need to check if already int32)
                elif np.issubdtype(data.dtype, np.integer):
                    if data.dtype in (np.int16, np.int32):
                        data = np.copy(data)  # Copy if dtype remains unchanged (int16 or int32)
                    else:
                        data = data.astype(np.int32)  # Convert other integer types to int32

                # Loop through all actors in the renderer
                for actor in canvas.renderer.GetActors2D():
                    # Check if the actor is a vtkScalarBarActor
                    if isinstance(actor, vtk.vtkScalarBarActor) and actor.GetVisibility():
                        actor.SetVisibility(0)

                if ((self.color_mapp.startswith('Predicted') and any(
                            word in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones']))
                        or
                        (self.color_mapp.startswith('Upscaled') and any(
                            word in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones']))
                        or
                        self.color_mapp.startswith('Clusters')):
                    # Get mask of non-zero elements
                    if self.color_mapp.startswith('Clusters'):
                        mask = data != 0
                        # Get unique non-zero values and sort them
                        unique_vals = np.unique(data[mask])

                        mapping = np.zeros(int(unique_vals.max() if not unique_vals.size == 0 else 0) + 1, dtype=data.dtype)
                        mapping[unique_vals.astype(int)] = np.arange(1, len(unique_vals) + 1)

                        # Create output array and apply mapping
                        data = mapping[data.astype(int)]

                    elif self.color_mapp.startswith('Upscaled'):

                        # Create a mask for non-background values
                        mask = (data != -999.25)

                        # Get unique values excluding background
                        unique_vals = np.unique(data[mask])

                        # Create a mapping dictionary
                        val_map = {old_val: new_val + 1 for new_val, old_val in enumerate(unique_vals)}

                        # Apply mapping only to non-background values
                        data[mask] = np.vectorize(val_map.get)(data[mask])

                    else:
                        # Get unique non-zero values and sort them
                        unique_vals = np.unique(data)

                        mapping = np.zeros(int(unique_vals.max() if not unique_vals.size == 0 else 0) + 1, dtype=data.dtype)
                        mapping[unique_vals.astype(int)] = np.arange(1, len(unique_vals) + 1)

                        # Create output array and apply mapping
                        data = mapping[data.astype(int)]

                # Define the custom color schemes
                custom_color_mappings = {
                    'Seismic': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'App. Polarity': [(0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 1.0, 1.0, 1.0),
                                      (0.7490196078431373, 0.0, 0.0, 1.0)],
                    'Polarity': [(0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 1.0, 1.0, 1.0),
                                 (0.7490196078431373, 0.0, 0.0, 1.0)],
                    'Inst. Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                        (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                        (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                        (1.0, 0.0, 1.0, 1.0)],
                    'Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                  (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                  (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                  (1.0, 0.0, 1.0, 1.0)],
                    'Cos Phase': [(0.0, 0.0, 0.0, 1.0), (1.0, 1.0, 1.0, 1.0)],
                    'Inst. Phase': [(1.0, 0.4117647058823529, 0.7058823529411765, 1.0),
                                    (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0),
                                    (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                                    (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)],
                    'Phase': [(1.0, 0.4117647058823529, 0.7058823529411765, 1.0),
                              (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0),
                              (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                              (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)],
                    'Envelope': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                 (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                 (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                 (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'Inst. Bandwidth': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                        (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                        (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                        (1.0, 0.0, 1.0, 1.0)],
                    'Dom. Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                       (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                       (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                       (1.0, 0.0, 1.0, 1.0)],
                    'Sweetness': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                  (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                  (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                  (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'RMS Amplitude': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                      (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                      (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                      (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'Coherence': [
                        (1.0, 1.0, 1.0, 1.0),  # White
                        (0.5, 0.5, 0.5, 1.0),  # Gray
                        (0.0, 0.0, 0.0, 1.0),  # Black (middle)
                        (1.0, 0.0, 0.0, 1.0),  # Red
                        (1.0, 0.647, 0.0, 1.0),  # Orange
                        (1.0, 1.0, 0.0, 1.0)  # Yellow
                    ],
                    'GST': [
                        (1.0, 1.0, 1.0, 1.0),  # White
                        (0.5, 0.5, 0.5, 1.0),  # Gray
                        (0.0, 0.0, 0.0, 1.0),  # Black (middle)
                        (1.0, 0.0, 0.0, 1.0),  # Red
                        (1.0, 0.647, 0.0, 1.0),  # Orange
                        (1.0, 1.0, 0.0, 1.0)  # Yellow
                    ],

                }

                if self.three_d_interpolation is not None and self.three_d_interpolation != 1.0:

                    order = 0

                    if (self.color_mapp in custom_color_mappings.keys()) or (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                        order = 3

                    gpu_interpolation = False

                    if gpu_available:
                        # Check available GPU memory before allocating
                        free_memory, total_memory = cp.cuda.Device().mem_info
                        required_memory = data.nbytes

                        # Safety margin (e.g., 80% of free memory)
                        safety_margin = 0.8
                        if required_memory * self.three_d_interpolation > free_memory * safety_margin:
                            cp.get_default_memory_pool().free_all_blocks()

                            gpu_interpolation = False
                        else:
                            gpu_interpolation = True

                    if gpu_interpolation:
                        # Compute total sample count
                        num_samples = np.prod(data.shape, dtype=np.int64)

                        if num_samples * self.three_d_interpolation <= self.multiprocessing_threshold:
                            result = TensorVisualizer.gpu_3d_interpolation_task(data, self.three_d_interpolation, order)
                        else:
                            QApplication.restoreOverrideCursor()
                            result = self.task_runner.run_task(TensorVisualizer.gpu_3d_interpolation_task,
                                                               data, self.three_d_interpolation, order)
                            QApplication.processEvents()
                            QApplication.setOverrideCursor(self.custom_cursor)

                        if result is not None:
                            data = result
                    else:
                        QApplication.restoreOverrideCursor()
                        result = self.task_runner.run_task(TensorVisualizer.cpu_3d_interpolation_task,
                                                           data, self.three_d_interpolation, order)
                        QApplication.processEvents()
                        QApplication.setOverrideCursor(self.custom_cursor)

                        if result is not None:
                            data = result

                if data.shape[-1] > 1:
                    channel_index = int(self.channel_index_entry.value())
                    data = data[:, :, :, channel_index:channel_index + 1]

                physical_max = np.max(data)
                physical_min = np.min(data)

                # Extract intensity values from the tensor data
                intensity = data[..., 0]

                # Make a copy of the intensity values to keep the original intact
                intensity_copy = np.copy(intensity)

                # If a full 3D plot is created, update the state in the dictionary
                if not any([self.time_slice_button.isChecked(), self.cross_line_button.isChecked(),
                            self.inline_button.isChecked()]):
                    self.tab_3D_state[active_tab_index] = True
                    # Clear all items from the plot widget if 3D button is checked
                    if hasattr(self, "tab_volume_items") and active_tab_index in self.tab_volume_items:
                        for item in list(self.tab_volume_items[active_tab_index]):
                            # Remove the item from the renderer
                            if hasattr(item, "SetVisibility"):  # For volumes
                                if not (self.color_mapp in custom_color_mappings.keys() or self.color_mapp.startswith(
                                        'Predicted')) and hasattr(item, 'transparent'):
                                    pass
                                else:
                                    self.tab_volume_items[active_tab_index].remove(item)
                                    canvas.renderer.RemoveActor(item)
                                    if hasattr(item, 'color_bar'):
                                        canvas.renderer.RemoveActor2D(item.color_bar)
                                    del item
                            elif hasattr(item, "SetEnabled"):  # For plain widgets (e.g., vtkImagePlaneWidget)
                                if self.color_mapp in custom_color_mappings.keys() or self.color_mapp.startswith('Predicted'):
                                    self.tab_volume_items[active_tab_index].remove(item)
                                    item.SetEnabled(0)
                                    if hasattr(item, 'color_bar'):
                                        canvas.renderer.RemoveActor2D(item.color_bar)
                                    del item
                                elif not (self.color_mapp in custom_color_mappings.keys() or self.color_mapp.startswith('Predicted')) and item.template_2d == self.color_mapp:
                                    self.tab_volume_items[active_tab_index].remove(item)
                                    item.SetEnabled(0)
                                    if hasattr(item, 'color_bar'):
                                        canvas.renderer.RemoveActor2D(item.color_bar)
                                    del item

                        # Check if this was the last item in the list
                        if not self.tab_volume_items[active_tab_index]:
                            if active_tab_index in self.grids:
                                grid = self.grids.pop(active_tab_index)  # Remove grid from the dictionary
                                canvas.renderer.RemoveActor(grid)  # Remove grid from the renderer

                # Check if self.color_mapp matches the patterns
                if self.color_mapp in custom_color_mappings.keys():
                    # Normalize the intensity values using z-score normalization
                    mean_intensity = np.mean(intensity_copy)
                    std_intensity = np.std(intensity_copy)

                    if std_intensity == 0 or np.isnan(std_intensity):
                        intensity_z_score_normalized = np.zeros_like(intensity_copy)
                    else:
                        intensity_z_score_normalized = (intensity_copy - mean_intensity) / std_intensity

                else:
                    intensity_z_score_normalized = intensity

                def generate_rgba_color_list(num_colors):
                    # Fixed colors: transparent white for zero, predefined second color
                    colours = [
                        [1.0, 1.0, 1.0, 0.0],  # Transparent white for zero
                        self.geo_objects_color  # Predefined color (fully opaque)
                    ]

                    # If more colors are needed, generate them dynamically
                    if num_colors > 1:
                        additional_colors = num_colors - 1  # Exclude the two fixed colors
                        cmap = plt.get_cmap(self.geophysical_Object_CMap, additional_colors + 1)
                        rgba_array = np.hstack([cmap(np.arange(1, additional_colors + 1))[:, :3],
                                                np.ones((additional_colors, 1))])
                        colours.extend(rgba_array.tolist())

                    # Convert to numpy array with dtype np.float32
                    colors_array = np.array(colours, dtype=np.float32)

                    return colors_array

                def get_colors_from_colormap(colormap_name):
                    """
                    Generate six evenly spaced RGBA colors from the given colormap.

                    Parameters:
                        colormap_name (str): The name of the colormap.

                    Returns:
                        list: A list of six RGBA tuples.
                    """
                    colormap = plt.get_cmap(colormap_name)  # Get the colormap
                    colors = [colormap(i) for i in np.linspace(0, 1, 6)]  # Generate 6 evenly spaced colors
                    return colors

                # Check the name of the reference and choose the color mapping
                matching_key = next((key for key in custom_color_mappings if key in self.color_mapp), None)
                if matching_key:
                    colors = custom_color_mappings[matching_key]
                elif self.color_mapp.startswith('Anomalies') or self.color_mapp.startswith('Probe') or \
                        (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])) or \
                        (self.color_mapp.startswith('Upscaled') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                    colors = get_colors_from_colormap(self.geophysical_Object_CMap)
                else:
                    # Get the unique values in the array, excluding zero
                    unique_values = np.unique(intensity_z_score_normalized)
                    num_clusters = len(unique_values[unique_values != 0])  # Exclude zero explicitly
                    colors = generate_rgba_color_list(num_clusters)

                # Convert numpy array to VTK data (ensure Fortran ordering)
                tensor_3d = np.transpose(intensity_z_score_normalized, (2, 0, 1))
                # Create VTK pipeline
                if self.color_mapp in custom_color_mappings.keys():
                    # For special mapping, use original values without rescaling
                    intensity_z_score_normalized = np.clip(intensity_z_score_normalized, -3, 3)
                    data_max = np.max(abs(intensity_z_score_normalized))

                    if np.any(intensity_z_score_normalized < 0):
                        data_min = - data_max
                    else:
                        data_min = np.min(intensity_z_score_normalized)

                    # Rescale to [0, 255] for visualization
                    denom = 1 if data_max == data_min else data_max - data_min
                    tensor_3d = ((tensor_3d - data_min) / denom) * 255
                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    image_data.GetPointData().SetScalars(vtk_data)

                elif (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                    data_max = np.max(intensity_z_score_normalized)
                    data_min = np.min(intensity_z_score_normalized)
                    # Rescale to [0, 255] for visualization
                    denom = 1 if data_max == data_min else data_max - data_min
                    tensor_3d = ((tensor_3d - data_min) / denom) * 255
                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    image_data.GetPointData().SetScalars(vtk_data)

                elif (self.color_mapp.startswith('Upscaled') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                    data_max = np.max(intensity_z_score_normalized)
                    data_min = np.min(intensity_z_score_normalized)
                    background_value = -999.25  # Your background value
                    background_indices = 10

                    # Create a mask for background values
                    background_mask = (tensor_3d <= background_value)

                    # First, handle the background pixels (always map to 0)
                    tensor_3d = np.where(background_mask, 0, tensor_3d)

                    # Now handle the non-background pixels
                    if np.any(~background_mask):
                        # There are non-background pixels
                        min_value = np.min(tensor_3d[~background_mask])
                        max_value = np.max(tensor_3d[~background_mask])
                        range_value = max_value - min_value

                        # If all non-background values are identical
                        if range_value == 0:
                            # Map all non-background values to background_indices
                            tensor_3d[~background_mask] = background_indices
                        else:
                            # Normal case - scale non-background values to [background_indices, 255]
                            tensor_3d[~background_mask] = ((tensor_3d[~background_mask] - min_value) / range_value) * \
                                                          (255 - background_indices) + background_indices

                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    image_data.GetPointData().SetScalars(vtk_data)

                elif self.color_mapp.startswith('Anomalies'):
                    data_max = np.max(abs(intensity_z_score_normalized))
                    data_min = - data_max

                    # Rescale to [0, 255] for visualization
                    denom = 1 if data_max == data_min else data_max - data_min
                    tensor_3d = ((tensor_3d - data_min) / denom) * 255
                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    image_data.GetPointData().SetScalars(vtk_data)
                elif self.color_mapp.startswith('Probe'):

                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True, array_type=vtk.VTK_FLOAT)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    vtk_data.SetName('scalars')
                    image_data.GetPointData().SetScalars(vtk_data)

                else:
                    tensor_data = np.asfortranarray(tensor_3d)
                    vtk_data = numpy_support.numpy_to_vtk(tensor_data.ravel(), deep=True)

                    # Create image data
                    image_data = vtk.vtkImageData()
                    image_data.SetDimensions(tensor_3d.shape[::-1])
                    image_data.GetPointData().SetScalars(vtk_data)

                center = [dim / 2 for dim in tensor_data.shape[::-1]]
                # Calculate bounds and diagonal size of the volume
                bounds = image_data.GetBounds()  # (xmin, xmax, ymin, ymax, zmin, zmax)
                xmin, xmax, ymin, ymax, zmin, zmax = bounds
                diagonal_size = ((xmax - xmin) ** 2 + (ymax - ymin) ** 2 + (zmax - zmin) ** 2) ** 0.5

                # Set up camera
                camera = canvas.renderer.GetActiveCamera()

                # Set focal point to the center of the volume
                camera.SetFocalPoint(0, 0, 0)

                # Position the camera at a 45-degree angle (viewing top, left, and right)
                camera_distance = diagonal_size * 2.5  # Adjust factor as needed
                offset = camera_distance / (3 ** 0.5)  # Offset for 45-degree angle
                camera.SetPosition(
                    center[0] + offset,  # X offset
                    center[1] - offset,  # Y offset (negative to view top)
                    center[2] + offset  # Z offset
                )

                # Maintain the correct view-up direction
                camera.SetViewUp(0, -1, 0)  # Do not touch this

                # ----------------------------------------
                # Main Renderer Logic with Cross-Section Condition
                # ----------------------------------------

                # Determine if we're creating a volume or a cross-section
                if any([self.time_slice_button.isChecked(), self.cross_line_button.isChecked(),
                        self.inline_button.isChecked()]):

                    # Get the value of the index dimension from the index_dim_entry
                    index_dim_value = int(self.index_dim_entry.text())

                    # Get the shape of the tensor data
                    tensor_shape = self.tensor_data.shape

                    # Get the maximum value for the slider from the shape of the tensor data
                    max_value = tensor_shape[index_dim_value] - 1

                    if index_dim_value == 0:
                        sampling_interval = metadata.get('sampling_interval_ms')
                        min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
                        index = int(((int(self.index_entry.value()) - min_slider_value) / sampling_interval) * self.three_d_interpolation)

                    else:
                        if index_dim_value == 1:
                            axis_range = metadata.get('inline_range', (0, max_value))
                        else:
                            axis_range = metadata.get('xline_range', (0, max_value))

                        min_slider_value = axis_range[0]
                        step = (axis_range[1] - axis_range[0]) / max_value if tensor_shape[index_dim_value] > 1 else 1
                        index = int(
                                (int(self.index_entry.value()) - min_slider_value) / step) * self.three_d_interpolation

                    if self.time_slice_button.isChecked():
                        axis = 1  # Time corresponds to the first axis
                    elif self.cross_line_button.isChecked():
                        axis = 2  # Cross-line corresponds to the third axis
                    elif self.inline_button.isChecked():
                        axis = 0  # Inline corresponds to the second axis

                    """
                    Adds a cross-section at the specified axis and slice index.
                    :param renderer: The VTK renderer.
                    :param interactor: The VTK interactor.
                    :param image_data: The 3D VTK image data.
                    :param axis: Axis along which to create the cross-section (0=X, 1=Y, 2=Z).
                    :param slice_index: Index of the slice along the specified axis.
                    :param colors: List of (r, g, b, a) values for color transfer function.
                    """
                    # Step 1: Create a Lookup Table with proper interpolation
                    color_lut = vtk.vtkLookupTable()

                    # Center the origin
                    spacing = image_data.GetSpacing()
                    dimensions = image_data.GetDimensions()

                    # Compute the center
                    center = [(dim - 1) * space / 2.0 for dim, space in zip(dimensions, spacing)]
                    image_data.SetOrigin(-center[0], -center[1], -center[2])

                    # Check if self.color_mapp matches the patterns
                    if (self.color_mapp in custom_color_mappings.keys()) or (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                        color_lut.SetRange(0, 255)
                        color_lut.SetNumberOfTableValues(256)  # Changed to 256 for smooth interpolation

                        # Calculate indices for the seven colors
                        indices = np.linspace(0, 255, len(colors)).astype(int)

                        # Fill the lookup table with interpolated colors
                        for i in range(256):
                            # Find the appropriate color segment
                            for j in range(len(indices) - 1):
                                if i <= indices[j + 1]:
                                    # Linear interpolation between two adjacent colors
                                    t = (i - indices[j]) / (indices[j + 1] - indices[j])
                                    color1 = colors[j]
                                    color2 = colors[j + 1]
                                    r = color1[0] + t * (color2[0] - color1[0])
                                    g = color1[1] + t * (color2[1] - color1[1])
                                    b = color1[2] + t * (color2[2] - color1[2])
                                    a = color1[3] + t * (color2[3] - color1[3])
                                    color_lut.SetTableValue(i, r, g, b, a)
                                    break
                            if i >= indices[-1]:
                                color_lut.SetTableValue(i, *colors[-1])

                        color_lut.Build()

                        # Add color bar for ImagePlaneWidget
                        color_bar = vtk.vtkScalarBarActor()
                        label_prop = color_bar.GetLabelTextProperty()
                        label_prop.SetBold(0)
                        color_bar.SetVisibility(self.three_d_color_bar_active)
                        # Create a separate lookup table for the color bar display
                        display_lut = vtk.vtkLookupTable()
                        display_lut.DeepCopy(color_lut)  # Copy the color mapping
                        display_lut.SetRange(physical_min, physical_max)  # Only change the display range
                        color_bar.SetLookupTable(display_lut)
                        color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                        color_bar.SetNumberOfLabels(7)  # 10 ticks

                        # Adjust appearance
                        color_bar.SetWidth(0.06)  # Narrower
                        color_bar.SetHeight(0.4)  # Longer
                        color_bar.SetPosition(0.03, 0.55)  # Position in view
                        color_bar.SetLabelFormat("%.1f")
                        color_bar.SetVerticalTitleSeparation(10)

                        # Add the color bar to the renderer
                        canvas.renderer.AddActor2D(color_bar)
                    elif self.color_mapp.startswith('Probe'):

                        if np.min(tensor_3d) < 0:
                            mini = np.min(tensor_3d)
                            maxi = np.max(tensor_3d[tensor_3d != 0])
                            # Calculate the correct number of entries to cover from mini to 0 inclusive
                            num_entries = int(abs(mini)) + 1
                            color_lut.SetRange(mini, 0)
                            color_lut.SetNumberOfTableValues(num_entries)
                            color_lut.Build()

                            # Set background values (from maxi to 0 inclusive) to transparent white
                            for scalar_value in range(int(maxi), 1):  # Range includes 0 when maxi is negative
                                index = scalar_value - int(mini)
                                color_lut.SetTableValue(index, 1.0, 1.0, 1.0, 0.0)
                        else:
                            # Only try to find minimum of non-zero values if they exist
                            non_zero_values = tensor_3d[tensor_3d != 0]
                            if len(non_zero_values) > 0:
                                mini = np.min(non_zero_values)
                            else:
                                # Fallback value when no non-zero elements exist
                                mini = 0  # or another appropriate default value

                            maxi = np.max(tensor_3d)
                            # Calculate the correct number of entries to cover from 0 to maxi inclusive
                            num_entries = int(maxi) + 1
                            color_lut.SetRange(0, maxi)
                            color_lut.SetNumberOfTableValues(num_entries)
                            color_lut.Build()

                            # Set background values (from 0 to mini exclusive) to transparent white
                            for scalar_value in range(0, int(mini)):
                                color_lut.SetTableValue(scalar_value, 1.0, 1.0, 1.0, 0.0)

                        # Get non-zero values for percentile calculation
                        data_values = tensor_3d[tensor_3d != 0]

                        if len(data_values) > 0:
                            # Create intensity points with better distribution
                            # Using percentile-based mapping to spread colors more evenly
                            percentiles = np.linspace(0, 100, len(colors))
                            intensity_points = np.percentile(data_values, percentiles)
                        else:
                            intensity_points = np.linspace(mini, maxi, len(colors))

                        # Map the actual data range to colors
                        for i in range(len(colors) - 1):
                            start_idx = int(np.interp(intensity_points[i], [mini, maxi], [mini, maxi]))
                            end_idx = int(np.interp(intensity_points[i + 1], [mini, maxi], [mini, maxi]))
                            start_color = colors[i]
                            end_color = colors[i + 1]

                            # Fill in all values between the current pair of color points
                            if start_idx < end_idx:
                                for idx in range(start_idx, end_idx + 1):
                                    # Calculate interpolation factor
                                    alpha = (idx - start_idx) / (end_idx - start_idx)

                                    # Interpolate RGB values
                                    r = start_color[0] + alpha * (end_color[0] - start_color[0])
                                    g = start_color[1] + alpha * (end_color[1] - start_color[1])
                                    b = start_color[2] + alpha * (end_color[2] - start_color[2])

                                    # Set opacity to 1.0 for non-background values
                                    if idx <= mini or idx == 0:
                                        a = 0.0  # Background is transparent
                                    else:
                                        a = 1.0  # Non-background is fully opaque

                                    color_lut.SetTableValue(idx, r, g, b, a)

                        if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                            # Add color bar for Volume Rendering
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)
                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(physical_min, physical_max)
                            lut.Build()

                            num_colors = len(colors)
                            # Create an interpolation range
                            indices = np.linspace(0, 255, num_colors)
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Interpolate the color components
                            r_interp = np.interp(range(256), indices, r_vals)
                            g_interp = np.interp(range(256), indices, g_vals)
                            b_interp = np.interp(range(256), indices, b_vals)
                            a_interp = np.interp(range(256), indices, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Configure the color bar
                            color_bar.SetLookupTable(lut)  # Use the new lookup table
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(7)  # 10 ticks

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                        else:
                            color_bar = None
                    elif self.color_mapp.startswith('Anomalies'):
                        # Create a new color lookup table
                        color_lut.SetRange(0, 255)
                        color_lut.SetNumberOfTableValues(256)

                        # Find the smallest non-zero positive and largest negative values
                        positive_values = intensity_z_score_normalized[intensity_z_score_normalized > 0]
                        if positive_values.size == 0:
                            smallest_non_zero_positive = 0
                        else:
                            smallest_non_zero_positive = np.min(positive_values)
                        negative_values = intensity_z_score_normalized[intensity_z_score_normalized < 0]
                        if negative_values.size == 0:
                            largest_negative = 0
                        else:
                            largest_negative = np.max(negative_values)

                        # Calculate normalized values
                        zero_normalized = (-data_min) / (data_max - data_min) * 255 if data_min != data_max else 0
                        one_normalized = (smallest_non_zero_positive - data_min) / (
                                    data_max - data_min) * 255 if data_min != data_max else 0
                        neg_one_normalized = (largest_negative - data_min) / (
                                    data_max - data_min) * 255 if data_min != data_max else 0

                        # Add zero points with transparency
                        color_lut.SetTableValue(int(zero_normalized), 1.0, 1.0, 1.0, 0.0)  # White, transparent
                        color_lut.SetTableValue(int(one_normalized), 1.0, 1.0, 1.0, 0.0)  # White, transparent
                        color_lut.SetTableValue(int(neg_one_normalized), 1.0, 1.0, 1.0, 0.0)  # White, transparent

                        # Calculate intensity points and populate the lookup table
                        num_colors = len(colors)
                        intensity_points = np.linspace(0, 255, num_colors)

                        for i in range(256):
                            if one_normalized < i or i < neg_one_normalized:
                                # Find the appropriate color segment
                                for j in range(len(intensity_points) - 1):
                                    if i <= intensity_points[j + 1]:
                                        # Linear interpolation between two adjacent colors
                                        t = (i - intensity_points[j]) / (
                                                    intensity_points[j + 1] - intensity_points[j])
                                        color1 = colors[j]
                                        color2 = colors[j + 1]
                                        r = color1[0] + t * (color2[0] - color1[0])
                                        g = color1[1] + t * (color2[1] - color1[1])
                                        b = color1[2] + t * (color2[2] - color1[2])
                                        a = color1[3] + t * (color2[3] - color1[3])
                                        color_lut.SetTableValue(i, r, g, b, a)
                                        break
                                if i >= intensity_points[-1]:
                                    color_lut.SetTableValue(i, *colors[-1])

                        color_lut.Build()

                        if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                            # Add color bar for ImagePlaneWidget
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)
                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(physical_min, physical_max)
                            lut.Build()

                            num_colors = len(colors)
                            # Create an interpolation range
                            indices = np.linspace(0, 255, num_colors)
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Interpolate the color components
                            r_interp = np.interp(range(256), indices, r_vals)
                            g_interp = np.interp(range(256), indices, g_vals)
                            b_interp = np.interp(range(256), indices, b_vals)
                            a_interp = np.interp(range(256), indices, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Configure the color bar
                            color_bar.SetLookupTable(lut)  # Use the new lookup table
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(7)  # 10 ticks

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                        else:
                            color_bar = None

                    elif self.color_mapp.startswith('Upscaled') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones']):
                        # Configure the number of table entries
                        color_lut.SetRange(0, 255)
                        color_lut.SetNumberOfTableValues(256)
                        color_lut.Build()

                        background_value = -999.25  # Your background value
                        physical_min = np.min(intensity_z_score_normalized[intensity_z_score_normalized > background_value]) if np.any(intensity_z_score_normalized > background_value) else 0 # Find min of actual data
                        physical_max = np.max(intensity_z_score_normalized)  # Max of actual data

                        # Reserve just a few indices for background (e.g., first 10 indices)
                        background_indices = 10

                        # Set background value to transparent white
                        for i in range(background_indices):
                            color_lut.SetTableValue(i, 1.0, 1.0, 1.0, 0.0)  # White, fully transparent

                        # Now use the remaining range (background_indices to 255) for actual data
                        intensity_points = np.linspace(background_indices, 255, len(colors))

                        # Map the actual data range to colors
                        for i in range(len(colors) - 1):
                            start_idx = int(intensity_points[i])
                            end_idx = int(intensity_points[i + 1])
                            start_color = colors[i]
                            end_color = colors[i + 1]

                            # Fill in all values between the current pair of color points
                            for idx in range(start_idx, end_idx + 1):
                                alpha = (idx - start_idx) / (end_idx - start_idx) if end_idx != start_idx else 0
                                r = start_color[0] + alpha * (end_color[0] - start_color[0])
                                g = start_color[1] + alpha * (end_color[1] - start_color[1])
                                b = start_color[2] + alpha * (end_color[2] - start_color[2])
                                a = start_color[3] + alpha * (end_color[3] - start_color[3])

                                color_lut.SetTableValue(idx, r, g, b, a)

                        if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                            # Add color bar for Volume Rendering
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)
                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(physical_min, physical_max)
                            lut.Build()

                            num_colors = len(colors)
                            # Create an interpolation range
                            indices = np.linspace(0, 255, num_colors)
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Interpolate the color components
                            r_interp = np.interp(range(256), indices, r_vals)
                            g_interp = np.interp(range(256), indices, g_vals)
                            b_interp = np.interp(range(256), indices, b_vals)
                            a_interp = np.interp(range(256), indices, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Configure the color bar
                            color_bar.SetLookupTable(lut)  # Use the new lookup table
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(7)  # 10 ticks

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                        else:
                            color_bar = None

                    else:

                        # Determine special value based on color mapping type
                        special_value = -999.25 if self.color_mapp.startswith('Upscaled') else 0

                        # Get the actual data range
                        data_min = np.min(intensity_z_score_normalized)
                        data_max = np.max(intensity_z_score_normalized)

                        # Set the range to match the actual data range
                        color_lut.SetRange(data_min, data_max)
                        color_lut.SetNumberOfTableValues(int(data_max - data_min + 1))  # One slot per integer

                        # Get unique values and sort them
                        unique_values = np.unique(intensity_z_score_normalized)
                        unique_values = np.sort(unique_values[unique_values != special_value])
                        num_values = len(unique_values)
                        physical_min = np.min(unique_values) if not unique_values.size == 0 else 0
                        physical_max = np.max(unique_values) if not unique_values.size == 0 else 0

                        # Initialize all values with the last color
                        for i in range(int(data_max - data_min + 1)):
                            color_lut.SetTableValue(i, *colors[-1])

                        # Handle special value first
                        if data_min <= special_value <= data_max:
                            special_idx = int(special_value - data_min)
                            color_lut.SetTableValue(special_idx, 1.0, 1.0, 1.0, 0.0)  # White, transparent

                        # Map colors to unique values
                        for idx, value in enumerate(unique_values):
                            if idx + 1 < len(colors):  # Match the volume version's indexing (idx + 1)
                                table_idx = int(value - data_min)  # Convert to lookup table index
                                r, g, b, a = colors[idx + 1]  # Use idx + 1 to match volume version
                                if 0 <= table_idx < color_lut.GetNumberOfTableValues():
                                    color_lut.SetTableValue(table_idx, r, g, b, a)

                        color_lut.Build()

                        if num_values > 1:
                            # Add color bar for ImagePlaneWidget
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)
                            # Create a separate lookup table for the color bar display
                            # Create lookup table
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(num_values)
                            lut.SetRange(physical_min, physical_max)

                            # Map colors to unique values
                            for idx, value in enumerate(unique_values):
                                if idx + 1 < len(colors):  # Ensure we don't exceed the colors array
                                    r, g, b, a = colors[idx + 1]
                                    lut.SetTableValue(idx, r, g, b, a)  # Map each value to a color

                            lut.Build()

                            color_bar.SetLookupTable(lut)
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(num_values)

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                        else:
                            color_bar = None

                    # Step 2: Create the ImagePlaneWidget
                    volume = vtk.vtkImagePlaneWidget()
                    volume.SetLeftButtonAction(3)
                    volume.SetMiddleButtonAction(3)
                    volume.SetPriority(0.0)
                    volume.SetHandleSize(0)
                    # Create a property for the margins
                    margin_property = vtk.vtkProperty()
                    margin_property.SetColor(1.0, 1.0, 1.0)
                    # Set the margin property
                    volume.SetMarginProperty(margin_property)
                    volume.SetMarginSizeX(0)
                    volume.SetMarginSizeY(0)
                    if (self.color_mapp in custom_color_mappings.keys()) or (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                        volume.SetResliceInterpolateToCubic()
                    else:
                        volume.SetResliceInterpolateToNearestNeighbour()

                    volume.SetInputData(image_data)  # Changed to SetInputData instead of SetInputConnection
                    volume.SetPlaneOrientation(axis)  # Set axis: 0=X, 1=Y, 2=Z
                    volume.SetSliceIndex(index)  # Set slice index
                    volume.SetInteractor(canvas)  # Attach to the interactor
                    volume.GetColorMap().SetLookupTable(color_lut)  # This is the key line that was missing
                    volume.On()  # Activate the widget
                    volume.template_2d = self.color_mapp
                    if color_bar is not None:
                        volume.color_bar = color_bar

                    if active_tab_index not in self.tab_volume_items:
                        self.tab_volume_items[active_tab_index] = []
                    self.tab_volume_items[active_tab_index].append(volume)

                else:

                    # Check if self.color_mapp matches the patterns
                    if (self.color_mapp in custom_color_mappings.keys()) or (self.color_mapp.startswith('Predicted') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones'])):
                        # Create volume mapper
                        volume_mapper = vtk.vtkSmartVolumeMapper()
                        volume_mapper.SetInputData(image_data)

                        # Create volume property
                        volume_property = vtk.vtkVolumeProperty()
                        volume_property.ShadeOff()

                        volume_property.SetInterpolationTypeToLinear()
                        # Create color transfer function (RGB only)
                        color_tf = vtk.vtkColorTransferFunction()

                        intensity_points = np.linspace(0, 255, len(colors))
                        for intensity, (r, g, b, _) in zip(intensity_points, colors):
                            color_tf.AddRGBPoint(intensity, r, g, b)

                        # Create opacity transfer function (for alpha values)
                        opacity_tf = vtk.vtkPiecewiseFunction()
                        for intensity, (_, _, _, a) in zip(intensity_points, colors):
                            opacity_tf.AddPoint(intensity, a)

                        volume_property.SetColor(color_tf)
                        volume_property.SetScalarOpacity(opacity_tf)

                        # Create volume
                        volume = vtk.vtkVolume()
                        volume.SetMapper(volume_mapper)
                        volume.SetProperty(volume_property)

                        # Center the volume
                        volume.SetPosition(-center[0], -center[1], -center[2])

                        # Create renderer and window
                        canvas.renderer.AddVolume(volume)

                        # Add color bar for Volume Rendering
                        color_bar = vtk.vtkScalarBarActor()
                        label_prop = color_bar.GetLabelTextProperty()
                        label_prop.SetBold(0)
                        color_bar.SetVisibility(self.three_d_color_bar_active)
                        # Create a lookup table based on the color transfer function
                        lut = vtk.vtkLookupTable()
                        lut.SetNumberOfTableValues(256)
                        lut.SetRange(physical_min, physical_max)

                        # Copy colors from volume property
                        ctf = volume_property.GetRGBTransferFunction()
                        for i in range(256):
                            normalized_value = i
                            color = ctf.GetColor(normalized_value)
                            lut.SetTableValue(i, color[0], color[1], color[2], 1.0)

                        lut.Build()

                        # Configure the color bar
                        color_bar.SetLookupTable(lut)  # Use the new lookup table
                        color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                        color_bar.SetNumberOfLabels(7)  # 10 ticks

                        # Adjust appearance
                        color_bar.SetWidth(0.06)  # Narrower
                        color_bar.SetHeight(0.4)  # Longer
                        color_bar.SetPosition(0.03, 0.55)  # Position in view
                        color_bar.SetLabelFormat("%.1f")
                        color_bar.SetVerticalTitleSeparation(10)
                        # Add the color bar to the renderer
                        canvas.renderer.AddActor2D(color_bar)
                        volume.color_bar = color_bar

                        if active_tab_index not in self.tab_volume_items:
                            self.tab_volume_items[active_tab_index] = []
                        self.tab_volume_items[active_tab_index].append(volume)

                    elif self.color_mapp.startswith('Probe'):
                        if self.cell_mesh:

                            tensor_3d = tensor_3d[tensor_3d != 0]

                            mini = np.min(tensor_3d)
                            maxi = np.max(tensor_3d)

                            # Threshold to get your layer
                            threshold = vtk.vtkThreshold()
                            threshold.SetInputData(image_data)
                            threshold.SetLowerThreshold(mini)
                            threshold.SetUpperThreshold(maxi)
                            threshold.SetInputArrayToProcess(0, 0, 0, vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS,
                                                             "scalars")
                            threshold.Update()

                            # Extract surface geometry
                            surface_filter = vtk.vtkGeometryFilter()
                            surface_filter.SetInputConnection(threshold.GetOutputPort())
                            surface_filter.Update()

                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(mini, maxi)
                            lut.Build()

                            # Get non-zero values for percentile calculation
                            data_values = tensor_3d[tensor_3d != 0]

                            # Calculate percentiles for the actual data distribution
                            num_colors = len(colors)
                            color_percentiles = np.linspace(0, 100, num_colors)
                            percentile_values = np.percentile(data_values, color_percentiles)

                            # Normalize percentile values to [0, 255] for lookup table indices
                            normalized_positions = np.interp(
                                percentile_values,
                                [mini, maxi],
                                [0, 255]
                            ).astype(int)

                            # Unzip colors
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Create interpolators for each color channel
                            r_interp = np.interp(range(256), normalized_positions, r_vals)
                            g_interp = np.interp(range(256), normalized_positions, g_vals)
                            b_interp = np.interp(range(256), normalized_positions, b_vals)
                            a_interp = np.interp(range(256), normalized_positions, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Create the mapper and assign the lookup table.
                            mapper = vtk.vtkPolyDataMapper()
                            mapper.SetInputConnection(surface_filter.GetOutputPort())
                            mapper.SetLookupTable(lut)
                            mapper.SetScalarRange(mini, maxi)
                            mapper.SetScalarModeToUsePointData()
                            mapper.SelectColorArray('scalars')
                            mapper.ScalarVisibilityOn()

                            # Create an actor.
                            actor = vtk.vtkActor()
                            actor.SetMapper(mapper)

                            # Center the volume
                            actor.SetPosition(-center[0], -center[1], -center[2])

                            # Create renderer and window
                            canvas.renderer.AddVolume(actor)

                            if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                                # Add color bar for Volume Rendering
                                color_bar = vtk.vtkScalarBarActor()
                                label_prop = color_bar.GetLabelTextProperty()
                                label_prop.SetBold(0)
                                color_bar.SetVisibility(self.three_d_color_bar_active)
                                # Create a lookup table based on the color transfer function

                                # Create a VTK lookup table with 256 entries.
                                dlut = vtk.vtkLookupTable()
                                dlut.SetNumberOfTableValues(256)
                                dlut.SetRange(mini, maxi)
                                dlut.Build()

                                num_colors = len(colors)
                                # Create an interpolation range
                                indices = np.linspace(0, 255, num_colors)
                                r_vals, g_vals, b_vals, a_vals = zip(*colors)

                                # Interpolate the color components
                                r_interp = np.interp(range(256), indices, r_vals)
                                g_interp = np.interp(range(256), indices, g_vals)
                                b_interp = np.interp(range(256), indices, b_vals)
                                a_interp = np.interp(range(256), indices, a_vals)

                                # Populate the lookup table
                                for idx in range(256):
                                    dlut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                                # Configure the color bar
                                color_bar.SetLookupTable(dlut)  # Use the new lookup table
                                color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                                color_bar.SetNumberOfLabels(7)  # 10 ticks

                                # Adjust appearance
                                color_bar.SetWidth(0.05)  # Narrower
                                color_bar.SetHeight(0.4)  # Longer
                                color_bar.SetPosition(0.03, 0.55)  # Position in view
                                color_bar.SetLabelFormat("%.1f")
                                color_bar.SetVerticalTitleSeparation(10)

                                # Add the color bar to the renderer
                                canvas.renderer.AddActor2D(color_bar)
                                actor.color_bar = color_bar
                                actor.transparent = True
                                actor.template = self.color_mapp

                            if active_tab_index not in self.tab_volume_items:
                                self.tab_volume_items[active_tab_index] = []
                            self.tab_volume_items[active_tab_index].append(actor)
                        elif self.point_mesh:

                            tensor_3d = tensor_3d[tensor_3d != 0]

                            mini = np.min(tensor_3d)
                            maxi = np.max(tensor_3d)

                            # Convert vtkImageData to PolyData
                            image_to_poly = vtk.vtkImageDataGeometryFilter()
                            image_to_poly.SetInputData(image_data)
                            image_to_poly.Update()

                            # Apply threshold
                            threshold = vtk.vtkThreshold()
                            threshold.SetInputData(image_to_poly.GetOutput())
                            threshold.SetLowerThreshold(mini)
                            threshold.SetUpperThreshold(maxi)
                            threshold.SetInputArrayToProcess(0, 0, 0, vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS,
                                                             "scalars")
                            threshold.Update()

                            # Convert UnstructuredGrid to PolyData
                            geometry_filter = vtk.vtkGeometryFilter()
                            geometry_filter.SetInputConnection(threshold.GetOutputPort())
                            geometry_filter.Update()

                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(mini, maxi)
                            lut.Build()

                            # Get non-zero values for percentile calculation
                            data_values = tensor_3d[tensor_3d != 0]

                            # Calculate percentiles for the actual data distribution
                            num_colors = len(colors)
                            color_percentiles = np.linspace(0, 100, num_colors)
                            percentile_values = np.percentile(data_values, color_percentiles)

                            # Normalize percentile values to [0, 255] for lookup table indices
                            normalized_positions = np.interp(
                                percentile_values,
                                [mini, maxi],
                                [0, 255]
                            ).astype(int)

                            # Unzip colors
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Create interpolators for each color channel
                            r_interp = np.interp(range(256), normalized_positions, r_vals)
                            g_interp = np.interp(range(256), normalized_positions, g_vals)
                            b_interp = np.interp(range(256), normalized_positions, b_vals)
                            a_interp = np.interp(range(256), normalized_positions, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Create the mapper and assign the lookup table.
                            mapper = vtk.vtkPolyDataMapper()
                            mapper.SetInputConnection(geometry_filter.GetOutputPort())
                            mapper.SetLookupTable(lut)
                            mapper.SetScalarRange(mini, maxi)
                            mapper.SetScalarModeToUsePointData()
                            mapper.SelectColorArray('scalars')
                            mapper.ScalarVisibilityOn()

                            # Create an actor.
                            actor = vtk.vtkActor()
                            actor.SetMapper(mapper)

                            # Center the volume
                            actor.SetPosition(-center[0], -center[1], -center[2])

                            # Create renderer and window
                            canvas.renderer.AddVolume(actor)

                            if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                                # Add color bar for Volume Rendering
                                color_bar = vtk.vtkScalarBarActor()
                                label_prop = color_bar.GetLabelTextProperty()
                                label_prop.SetBold(0)
                                color_bar.SetVisibility(self.three_d_color_bar_active)
                                # Create a lookup table based on the color transfer function

                                # Create a VTK lookup table with 256 entries.
                                dlut = vtk.vtkLookupTable()
                                dlut.SetNumberOfTableValues(256)
                                dlut.SetRange(mini, maxi)
                                dlut.Build()

                                num_colors = len(colors)
                                # Create an interpolation range
                                indices = np.linspace(0, 255, num_colors)
                                r_vals, g_vals, b_vals, a_vals = zip(*colors)

                                # Interpolate the color components
                                r_interp = np.interp(range(256), indices, r_vals)
                                g_interp = np.interp(range(256), indices, g_vals)
                                b_interp = np.interp(range(256), indices, b_vals)
                                a_interp = np.interp(range(256), indices, a_vals)

                                # Populate the lookup table
                                for idx in range(256):
                                    dlut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                                # Configure the color bar
                                color_bar.SetLookupTable(dlut)  # Use the new lookup table
                                color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                                color_bar.SetNumberOfLabels(7)  # 10 ticks

                                # Adjust appearance
                                color_bar.SetWidth(0.05)  # Narrower
                                color_bar.SetHeight(0.4)  # Longer
                                color_bar.SetPosition(0.03, 0.55)  # Position in view
                                color_bar.SetLabelFormat("%.1f")
                                color_bar.SetVerticalTitleSeparation(10)

                                # Add the color bar to the renderer
                                canvas.renderer.AddActor2D(color_bar)
                                actor.color_bar = color_bar
                                actor.transparent = True
                                actor.template = self.color_mapp

                            if active_tab_index not in self.tab_volume_items:
                                self.tab_volume_items[active_tab_index] = []
                            self.tab_volume_items[active_tab_index].append(actor)
                        else:
                            # Create volume mapper
                            volume_mapper = vtk.vtkSmartVolumeMapper()
                            volume_mapper.SetInputData(image_data)

                            # Create volume property
                            volume_property = vtk.vtkVolumeProperty()

                            volume_property.ShadeOff()

                            volume_property.SetInterpolationTypeToNearest()

                            # Create opacity transfer function (for alpha values)
                            opacity_tf = vtk.vtkPiecewiseFunction()
                            # Create color transfer function (RGB only)
                            color_tf = vtk.vtkColorTransferFunction()

                            if np.min(tensor_3d) < 0:
                                mini = np.min(tensor_3d)
                                maxi = np.max(tensor_3d[tensor_3d != 0])

                                # Handle background values (transparent white) for negative range
                                color_tf.AddRGBPoint(mini, 1.0, 1.0, 1.0)
                                opacity_tf.AddPoint(mini, 0.0)
                                color_tf.AddRGBPoint(0, 1.0, 1.0, 1.0)
                                opacity_tf.AddPoint(0, 0.0)
                            else:
                                # Only try to find minimum of non-zero values if they exist
                                non_zero_values = tensor_3d[tensor_3d != 0]
                                if len(non_zero_values) > 0:
                                    mini = np.min(non_zero_values)
                                else:
                                    # Fallback value when no non-zero elements exist
                                    mini = 0  # or another appropriate default value

                                maxi = np.max(tensor_3d)

                                # Handle background values (transparent white) for positive range
                                color_tf.AddRGBPoint(0, 1.0, 1.0, 1.0)
                                opacity_tf.AddPoint(0, 0.0)
                                color_tf.AddRGBPoint(mini, 1.0, 1.0, 1.0)
                                opacity_tf.AddPoint(mini, 0.0)

                            # Find the actual data range in the non-zero regions
                            data_values = tensor_3d[tensor_3d != 0]

                            if len(data_values) > 0:
                                # Create intensity points with better distribution
                                # Using percentile-based mapping to spread colors more evenly
                                percentiles = np.linspace(0, 100, len(colors))
                                intensity_points = np.percentile(data_values, percentiles)
                            else:
                                intensity_points = np.linspace(mini, maxi, len(colors))

                            # Map colors across the actual data range
                            for i in range(len(colors)):
                                point = intensity_points[i]
                                color = colors[i]

                                # Add color point
                                color_tf.AddRGBPoint(point, color[0], color[1], color[2])

                                # Set opacity to 1.0 for all non-background points
                                if point > mini:
                                    opacity_tf.AddPoint(point, 1.0)

                            # Ensure the last point has opacity 1.0
                            opacity_tf.AddPoint(maxi, 1.0)

                            volume_property.SetColor(color_tf)
                            volume_property.SetScalarOpacity(opacity_tf)
                            color_tf.SetScaleToLinear()

                            # Create volume
                            volume = vtk.vtkVolume()
                            volume.SetMapper(volume_mapper)
                            volume.SetProperty(volume_property)

                            # Center the volume
                            volume.SetPosition(-center[0], -center[1], -center[2])

                            # Create renderer and window
                            canvas.renderer.AddVolume(volume)

                            if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                                # Add color bar for Volume Rendering
                                color_bar = vtk.vtkScalarBarActor()
                                label_prop = color_bar.GetLabelTextProperty()
                                label_prop.SetBold(0)
                                color_bar.SetVisibility(self.three_d_color_bar_active)
                                # Create a lookup table based on the color transfer function
                                # Create a VTK lookup table with 256 entries.
                                lut = vtk.vtkLookupTable()
                                lut.SetNumberOfTableValues(256)
                                lut.SetRange(mini, maxi)
                                lut.Build()

                                num_colors = len(colors)
                                # Create an interpolation range
                                indices = np.linspace(0, 255, num_colors)
                                r_vals, g_vals, b_vals, a_vals = zip(*colors)

                                # Interpolate the color components
                                r_interp = np.interp(range(256), indices, r_vals)
                                g_interp = np.interp(range(256), indices, g_vals)
                                b_interp = np.interp(range(256), indices, b_vals)
                                a_interp = np.interp(range(256), indices, a_vals)

                                # Populate the lookup table
                                for idx in range(256):
                                    lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                                # Configure the color bar
                                color_bar.SetLookupTable(lut)  # Use the new lookup table
                                color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                                color_bar.SetNumberOfLabels(7)  # 10 ticks

                                # Adjust appearance
                                color_bar.SetWidth(0.05)  # Narrower
                                color_bar.SetHeight(0.4)  # Longer
                                color_bar.SetPosition(0.03, 0.55)  # Position in view
                                color_bar.SetLabelFormat("%.1f")
                                color_bar.SetVerticalTitleSeparation(10)

                                # Add the color bar to the renderer
                                canvas.renderer.AddActor2D(color_bar)
                                volume.color_bar = color_bar
                                volume.transparent = True
                                volume.template = self.color_mapp

                            if active_tab_index not in self.tab_volume_items:
                                self.tab_volume_items[active_tab_index] = []
                            self.tab_volume_items[active_tab_index].append(volume)
                    elif self.color_mapp.startswith('Anomalies'):
                        # Create volume mapper
                        volume_mapper = vtk.vtkSmartVolumeMapper()
                        volume_mapper.SetInputData(image_data)

                        # Create volume property
                        volume_property = vtk.vtkVolumeProperty()

                        volume_property.ShadeOff()

                        volume_property.SetInterpolationTypeToNearest()

                        # Create opacity transfer function (for alpha values)
                        opacity_tf = vtk.vtkPiecewiseFunction()
                        # Create color transfer function (RGB only)
                        color_tf = vtk.vtkColorTransferFunction()

                        # Find the smallest non-zero positive value
                        positive_values = intensity_z_score_normalized[intensity_z_score_normalized > 0]
                        if positive_values.size == 0:
                            smallest_non_zero_positive = 0
                        else:
                            smallest_non_zero_positive = np.min(positive_values)

                        # Find the largest negative value (closest to zero)
                        negative_values = intensity_z_score_normalized[intensity_z_score_normalized < 0]
                        if negative_values.size == 0:
                            largest_negative = 0
                        else:
                            largest_negative = np.max(negative_values)

                        # Find the value that corresponds to zero in normalized space
                        zero_normalized = (-data_min) / (data_max - data_min) * 255 if data_min != data_max else 0
                        one_normalized = (smallest_non_zero_positive - data_min) / (
                                    data_max - data_min) * 255 if data_min != data_max else 0
                        neg_one_normalized = (largest_negative - data_min) / (
                                    data_max - data_min) * 255 if data_min != data_max else 0

                        # Add zero points with transparency
                        color_tf.AddRGBPoint(zero_normalized, 1.0, 1.0, 1.0)  # White
                        opacity_tf.AddPoint(zero_normalized, 0.0)  # Transparent
                        color_tf.AddRGBPoint(one_normalized, 1.0, 1.0, 1.0)  # White
                        opacity_tf.AddPoint(one_normalized, 0.0)  # Transparent
                        color_tf.AddRGBPoint(neg_one_normalized, 1.0, 1.0, 1.0)  # White
                        opacity_tf.AddPoint(neg_one_normalized, 0.0)  # Transparent

                        # Add other color points
                        num_colors = len(colors)

                        intensity_points = np.linspace(0, 255, num_colors)

                        for intensity, (r, g, b, a) in zip(intensity_points, colors):
                            if one_normalized < intensity or intensity < neg_one_normalized:
                                color_tf.AddRGBPoint(intensity, r, g, b)
                                opacity_tf.AddPoint(intensity, a)

                        volume_property.SetColor(color_tf)
                        volume_property.SetScalarOpacity(opacity_tf)

                        # Create volume
                        volume = vtk.vtkVolume()
                        volume.SetMapper(volume_mapper)
                        volume.SetProperty(volume_property)

                        # Center the volume
                        volume.SetPosition(-center[0], -center[1], -center[2])

                        # Create renderer and window
                        canvas.renderer.AddVolume(volume)

                        if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                            # Add color bar for Volume Rendering
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)
                            # Create a lookup table based on the color transfer function
                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(physical_min, physical_max)
                            lut.Build()

                            num_colors = len(colors)
                            # Create an interpolation range
                            indices = np.linspace(0, 255, num_colors)
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Interpolate the color components
                            r_interp = np.interp(range(256), indices, r_vals)
                            g_interp = np.interp(range(256), indices, g_vals)
                            b_interp = np.interp(range(256), indices, b_vals)
                            a_interp = np.interp(range(256), indices, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Configure the color bar
                            color_bar.SetLookupTable(lut)  # Use the new lookup table
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(7)  # 10 ticks

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                            volume.color_bar = color_bar
                            volume.transparent = True
                            volume.template = self.color_mapp

                        if active_tab_index not in self.tab_volume_items:
                            self.tab_volume_items[active_tab_index] = []
                        self.tab_volume_items[active_tab_index].append(volume)

                    elif self.color_mapp.startswith('Upscaled') and all(word not in self.color_mapp for word in ['Clusters', 'Labels', 'Surface', 'Zones']):

                        # Create volume mapper
                        volume_mapper = vtk.vtkSmartVolumeMapper()
                        volume_mapper.SetInputData(image_data)

                        # Create volume property
                        volume_property = vtk.vtkVolumeProperty()

                        volume_property.ShadeOff()

                        volume_property.SetInterpolationTypeToNearest()

                        # Create opacity transfer function (for alpha values)
                        opacity_tf = vtk.vtkPiecewiseFunction()
                        # Create color transfer function (RGB only)
                        color_tf = vtk.vtkColorTransferFunction()

                        # Define negative range (white transparent)
                        background_value = -999.25  # Your background value
                        physical_min = np.min(
                            intensity_z_score_normalized[intensity_z_score_normalized > background_value]) if np.any(
                            intensity_z_score_normalized > background_value) else 0  # Find min of actual data # Find min of actual data
                        physical_max = np.max(intensity_z_score_normalized)  # Max of actual data

                        # Reserve just a few indices for background (e.g., first 10 indices)
                        background_indices = 10

                        # Add transparency for negative values
                        color_tf.AddRGBPoint(0, 1.0, 1.0, 1.0)  # White
                        opacity_tf.AddPoint(0, 0.0)  # Fully transparent

                        # Map positive values to six colors
                        intensity_points = np.linspace(background_indices, 255, len(colors))
                        for intensity, (r, g, b, a) in zip(intensity_points, colors):
                            color_tf.AddRGBPoint(intensity, r, g, b)
                            opacity_tf.AddPoint(intensity, a)  # Fully opaque

                        volume_property.SetColor(color_tf)
                        volume_property.SetScalarOpacity(opacity_tf)

                        # Create volume
                        volume = vtk.vtkVolume()
                        volume.SetMapper(volume_mapper)
                        volume.SetProperty(volume_property)

                        # Center the volume
                        volume.SetPosition(-center[0], -center[1], -center[2])

                        # Create renderer and window
                        canvas.renderer.AddVolume(volume)

                        if not np.all(intensity_z_score_normalized == intensity_z_score_normalized.flat[0]):
                            # Add color bar for Volume Rendering
                            color_bar = vtk.vtkScalarBarActor()
                            label_prop = color_bar.GetLabelTextProperty()
                            label_prop.SetBold(0)
                            color_bar.SetVisibility(self.three_d_color_bar_active)

                            # Create a lookup table based on the color transfer function
                            # Create a VTK lookup table with 256 entries.
                            lut = vtk.vtkLookupTable()
                            lut.SetNumberOfTableValues(256)
                            lut.SetRange(physical_min, physical_max)
                            lut.Build()

                            num_colors = len(colors)
                            # Create an interpolation range
                            indices = np.linspace(0, 255, num_colors)
                            r_vals, g_vals, b_vals, a_vals = zip(*colors)

                            # Interpolate the color components
                            r_interp = np.interp(range(256), indices, r_vals)
                            g_interp = np.interp(range(256), indices, g_vals)
                            b_interp = np.interp(range(256), indices, b_vals)
                            a_interp = np.interp(range(256), indices, a_vals)

                            # Populate the lookup table
                            for idx in range(256):
                                lut.SetTableValue(idx, r_interp[idx], g_interp[idx], b_interp[idx], a_interp[idx])

                            # Configure the color bar
                            color_bar.SetLookupTable(lut)  # Use the new lookup table
                            color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                            color_bar.SetNumberOfLabels(7)  # 10 ticks

                            # Adjust appearance
                            color_bar.SetWidth(0.05)  # Narrower
                            color_bar.SetHeight(0.4)  # Longer
                            color_bar.SetPosition(0.03, 0.55)  # Position in view
                            color_bar.SetLabelFormat("%.1f")
                            color_bar.SetVerticalTitleSeparation(10)

                            # Add the color bar to the renderer
                            canvas.renderer.AddActor2D(color_bar)
                            volume.transparent = True
                            volume.color_bar = color_bar
                            volume.template = self.color_mapp

                        if active_tab_index not in self.tab_volume_items:
                            self.tab_volume_items[active_tab_index] = []
                        self.tab_volume_items[active_tab_index].append(volume)

                    else:
                        # Get unique values and their counts (assuming tensor_3d is a NumPy array)
                        unique_values = np.unique(tensor_3d)

                        # Exclude zero
                        non_zero_values = unique_values[unique_values != 0]
                        physical_min = (np.min(non_zero_values) if not self.color_mapp.startswith('Upscaled') else (np.min(data[data >= 0]) if np.any(data >= 0) else 0)) if not non_zero_values.size == 0 else 0

                        # Get the number of unique non-zero categories
                        num_categories = len(non_zero_values)

                        # Check the condition: only one category
                        if num_categories == 1:
                            surface_extractor = vtk.vtkDiscreteMarchingCubes()
                            surface_extractor.SetInputData(image_data)  # Use your existing image_data
                            surface_extractor.SetValue(0, non_zero_values)  # Threshold between 0 and 1
                            surface_extractor.ComputeNormalsOn()

                            # Create mapper
                            surface_mapper = vtk.vtkPolyDataMapper()
                            surface_mapper.SetInputConnection(surface_extractor.GetOutputPort())
                            # Approach 2: Disable scalar coloring in the mapper
                            surface_mapper.ScalarVisibilityOff()

                            # Create actor
                            surface_actor = vtk.vtkActor()
                            surface_actor.SetMapper(surface_mapper)

                            # Set appearance (using your existing color if you want)
                            property = surface_actor.GetProperty()
                            r, g, b, _ = self.geo_objects_color  # Use the first non-background color
                            property.SetColor(r, g, b)

                            surface_actor.SetPosition(-center[0], -center[1], -center[2])

                            # Add to renderer (replace the volume adding line)
                            canvas.renderer.AddActor(surface_actor)  # Instead of AddVolume()

                            surface_actor.transparent = True
                            surface_actor.template = self.color_mapp

                            if active_tab_index not in self.tab_volume_items:
                                self.tab_volume_items[active_tab_index] = []
                            self.tab_volume_items[active_tab_index].append(surface_actor)
                        else:
                            # Create volume mapper
                            volume_mapper = vtk.vtkSmartVolumeMapper()
                            volume_mapper.SetInputData(image_data)

                            # Create volume property
                            volume_property = vtk.vtkVolumeProperty()

                            # Determine special value based on color mapping type
                            special_value = -999.25 if self.color_mapp.startswith('Upscaled') else 0

                            if self.color_mapp.startswith('Upscaled'):
                                volume_property.ShadeOff()
                            elif (tensor_3d == special_value).mean() >= self.shade_threshold:
                                # Enable shading for better 3D appearance
                                volume_property.ShadeOn()  # This is crucial for angular viewing
                                # Set lighting parameters for uniform illumination
                                volume_property.SetAmbient(0.8)  # Increase ambient light for better visibility
                                volume_property.SetDiffuse(0.4)  # Reduce diffuse to prevent spotlighting effect
                                volume_property.SetSpecular(0.1)  # Minimal specular to reduce bright spots
                                volume_property.SetSpecularPower(1.0)  # Wide specular highlights
                            else:
                                volume_property.ShadeOff()

                            volume_property.SetInterpolationTypeToNearest()

                            # Create transfer functions
                            color_tf = vtk.vtkColorTransferFunction()
                            opacity_tf = vtk.vtkPiecewiseFunction()

                            # Map special value (usually background)
                            color_tf.AddRGBPoint(special_value, 1.0, 1.0, 1.0)  # White
                            opacity_tf.AddPoint(special_value, 0.0)  # Transparent

                            # Get unique values and sort them for consistent mapping
                            unique_values = np.unique(intensity_z_score_normalized)
                            unique_values = np.sort(unique_values[unique_values != special_value])

                            # Map colors to unique values
                            for idx, value in enumerate(unique_values):
                                if idx + 1 < len(colors):  # Ensure we don't exceed color array
                                    r, g, b, a = colors[idx + 1]
                                    color_tf.AddRGBPoint(value, r, g, b)
                                    opacity_tf.AddPoint(value, a)

                            volume_property.SetColor(color_tf)
                            volume_property.SetScalarOpacity(opacity_tf)

                            # Create volume
                            volume = vtk.vtkVolume()
                            volume.SetMapper(volume_mapper)
                            volume.SetProperty(volume_property)

                            # Center the volume
                            volume.SetPosition(-center[0], -center[1], -center[2])

                            # Create renderer and window
                            canvas.renderer.AddVolume(volume)

                            if num_categories > 1:
                                # Add color bar for Volume Rendering
                                color_bar = vtk.vtkScalarBarActor()
                                label_prop = color_bar.GetLabelTextProperty()
                                label_prop.SetBold(0)
                                color_bar.SetVisibility(self.three_d_color_bar_active)
                                # Create lookup table
                                lut = vtk.vtkLookupTable()
                                lut.SetNumberOfTableValues(len(unique_values))
                                lut.SetRange(physical_min, physical_max)

                                # Map colors to unique values
                                for idx, value in enumerate(unique_values):
                                    if idx + 1 < len(colors):  # Ensure we don't exceed the colors array
                                        r, g, b, a = colors[idx + 1]
                                        lut.SetTableValue(idx, r, g, b, a)  # Map each value to a color

                                lut.Build()

                                # Configure the color bar
                                color_bar.SetLookupTable(lut)  # Use the new lookup table
                                color_bar.SetTitle(f"{self.metadata[self.file_name]['template']}")  # Set title
                                color_bar.SetNumberOfLabels(len(unique_values))

                                # Adjust appearance
                                color_bar.SetWidth(0.05)  # Narrower
                                color_bar.SetHeight(0.4)  # Longer
                                color_bar.SetPosition(0.03, 0.55)  # Position in view
                                color_bar.SetLabelFormat("%.1f")
                                color_bar.SetVerticalTitleSeparation(10)

                                # Add the color bar to the renderer
                                canvas.renderer.AddActor2D(color_bar)
                                volume.color_bar = color_bar

                            volume.transparent = True
                            volume.template = self.color_mapp
                            if active_tab_index not in self.tab_volume_items:
                                self.tab_volume_items[active_tab_index] = []
                            self.tab_volume_items[active_tab_index].append(volume)

                # Check if the grid for the current tab exists in the dictionary
                if active_tab_index in self.grids:
                    # The grid already exists for the current tab, so retrieve it
                    grid = self.grids[active_tab_index]
                    canvas.renderer.RemoveActor(grid)

                # Create and configure the grid
                cube_axes = vtk.vtkCubeAxesActor()
                cube_axes.SetVisibility(self.grid_active)
                cube_axes.SetBounds(-center[0], center[0],
                                    -center[1], center[1],
                                    -center[2], center[2])
                cube_axes.SetGridLineLocation(vtk.vtkCubeAxesActor.VTK_GRID_LINES_ALL)
                cube_axes.SetFlyMode(vtk.vtkCubeAxesActor.VTK_FLY_OUTER_EDGES)
                cube_axes.GetXAxesGridlinesProperty().SetLineWidth(0.5)
                cube_axes.GetYAxesGridlinesProperty().SetLineWidth(0.5)
                cube_axes.GetZAxesGridlinesProperty().SetLineWidth(0.5)
                cube_axes.SetCamera(canvas.renderer.GetActiveCamera())
                # Set custom axis titles to fix the label issue
                cube_axes.SetXTitle("Inline")  # Or an appropriate label for X
                cube_axes.SetYTitle("Time (ms)")  # Swapped with Z
                cube_axes.SetZTitle("Crossline")  # Swapped with Y

                # Extract the tensor dimensions
                nz, ny, nx = tensor_data.shape

                # Extract the min and max values from the filename
                sampling_interval = metadata.get('sampling_interval_ms')
                inline_range = metadata.get('inline_range',
                                            (0, (nx - 1) / self.three_d_interpolation))
                xline_range = metadata.get('xline_range',
                                           (0, (nz - 1) / self.three_d_interpolation))

                # Set X-axis range
                cube_axes.SetXAxisRange(inline_range[0], inline_range[1])

                # Set y-axis range
                cube_axes.SetYAxisRange(max(0, -(metadata.get('time_range', [0, 0, 0])[1])),
                                        int((ny - 1) * sampling_interval / self.three_d_interpolation)
                                        + max(0, -(metadata.get('time_range', [0, 0, 0])[1])))

                # Set Z-axis range
                cube_axes.SetZAxisRange(xline_range[0], xline_range[1])

                # Enable dynamic scaling for labels
                cube_axes.SetLabelScaling(True, 5, 5, 5)

                # Set the camera and add to the renderer
                cube_axes.SetCamera(canvas.renderer.GetActiveCamera())
                canvas.renderer.AddActor(cube_axes)

                origin = metadata.get('origin')
                xline_end = metadata.get('xline_end')

                if origin is not None and xline_end is not None:
                    # Extract coordinates
                    ox, oy = origin
                    xx, xy = xline_end

                    # Calculate vectors for inline and crossline
                    xline_vector = np.array([xx - ox, xy - oy])

                    # Calculate rotation (azimuth)
                    # Reference direction is North (positive y-axis)
                    north = np.array([0, 1])

                    # Normalize vectors
                    xline_unit = xline_vector / np.linalg.norm(xline_vector) if np.linalg.norm(
                        xline_vector) > 0 else np.array([0, 0])

                    # Calculate angles using dot product (in radians)
                    xline_angle_rad = np.arccos(np.clip(np.dot(north, xline_unit), -1.0, 1.0))

                    if xline_unit[0] < 0:
                        xline_angle_rad = 2 * np.pi - xline_angle_rad

                    # Convert to degrees
                    azimuth = np.degrees(xline_angle_rad)

                    # Create the base 2D arrow shape (unchanged)
                    arrow_points = vtk.vtkPoints()
                    arrow_points.InsertNextPoint(0, 0, 0)  # Base center
                    arrow_points.InsertNextPoint(-0.2, 0, 0)  # Base left
                    arrow_points.InsertNextPoint(-0.2, 0, 0.8)  # Shaft left
                    arrow_points.InsertNextPoint(-0.4, 0, 0.8)  # Arrow wing left
                    arrow_points.InsertNextPoint(0, 0, 1.3)  # Arrow tip
                    arrow_points.InsertNextPoint(0.4, 0, 0.8)  # Arrow wing right
                    arrow_points.InsertNextPoint(0.2, 0, 0.8)  # Shaft right
                    arrow_points.InsertNextPoint(0.2, 0, 0)  # Base right

                    # Create polygon cell (unchanged)
                    polygons = vtk.vtkCellArray()
                    polygon = vtk.vtkPolygon()
                    polygon.GetPointIds().SetNumberOfIds(8)
                    for i in range(8):
                        polygon.GetPointIds().SetId(i, i)
                    polygons.InsertNextCell(polygon)

                    # Create base polydata (unchanged)
                    arrow_polydata = vtk.vtkPolyData()
                    arrow_polydata.SetPoints(arrow_points)
                    arrow_polydata.SetPolys(polygons)

                    # Create two extrusion filters for top and bottom halves
                    extrude_top = vtk.vtkLinearExtrusionFilter()
                    extrude_top.SetInputData(arrow_polydata)
                    extrude_top.SetExtrusionTypeToNormalExtrusion()
                    extrude_top.SetVector(0, 0.1, 0)
                    extrude_top.Update()

                    extrude_bottom = vtk.vtkLinearExtrusionFilter()
                    extrude_bottom.SetInputData(arrow_polydata)
                    extrude_bottom.SetExtrusionTypeToNormalExtrusion()
                    extrude_bottom.SetVector(0, -0.1, 0)
                    extrude_bottom.Update()

                    # Triangulate both halves
                    triangulate_top = vtk.vtkTriangleFilter()
                    triangulate_top.SetInputConnection(extrude_top.GetOutputPort())
                    triangulate_top.Update()

                    triangulate_bottom = vtk.vtkTriangleFilter()
                    triangulate_bottom.SetInputConnection(extrude_bottom.GetOutputPort())
                    triangulate_bottom.Update()

                    # Add smooth normals to both
                    normals_top = vtk.vtkPolyDataNormals()
                    normals_top.SetInputConnection(triangulate_top.GetOutputPort())
                    normals_top.SetFeatureAngle(60)
                    normals_top.Update()

                    normals_bottom = vtk.vtkPolyDataNormals()
                    normals_bottom.SetInputConnection(triangulate_bottom.GetOutputPort())
                    normals_bottom.SetFeatureAngle(60)
                    normals_bottom.Update()

                    # Apply rotation transform to both halves
                    transform = vtk.vtkTransform()
                    transform.RotateY(-azimuth)

                    transform_filter_top = vtk.vtkTransformPolyDataFilter()
                    transform_filter_top.SetInputConnection(normals_top.GetOutputPort())
                    transform_filter_top.SetTransform(transform)
                    transform_filter_top.Update()

                    transform_filter_bottom = vtk.vtkTransformPolyDataFilter()
                    transform_filter_bottom.SetInputConnection(normals_bottom.GetOutputPort())
                    transform_filter_bottom.SetTransform(transform)
                    transform_filter_bottom.Update()

                    # Create separate actors for each half
                    arrow_mapper_top = vtk.vtkPolyDataMapper()
                    arrow_mapper_top.SetInputConnection(transform_filter_top.GetOutputPort())
                    arrow_actor_top = vtk.vtkActor()
                    arrow_actor_top.SetMapper(arrow_mapper_top)
                    arrow_actor_top.GetProperty().SetColor(1.0, 0.0, 0.0)
                    arrow_actor_top.GetProperty().SetSpecular(0.6)
                    arrow_actor_top.GetProperty().SetSpecularPower(30)

                    arrow_mapper_bottom = vtk.vtkPolyDataMapper()
                    arrow_mapper_bottom.SetInputConnection(transform_filter_bottom.GetOutputPort())
                    arrow_actor_bottom = vtk.vtkActor()
                    arrow_actor_bottom.SetMapper(arrow_mapper_bottom)
                    arrow_actor_bottom.GetProperty().SetColor(0.0, 1.0, 0.0)
                    arrow_actor_bottom.GetProperty().SetSpecular(0.6)
                    arrow_actor_bottom.GetProperty().SetSpecularPower(30)

                    # Combine both halves into a single assembly
                    arrow_assembly = vtk.vtkAssembly()
                    arrow_assembly.AddPart(arrow_actor_top)
                    arrow_assembly.AddPart(arrow_actor_bottom)

                    # Create orientation marker with the assembly
                    orientation_marker = vtk.vtkOrientationMarkerWidget()
                    orientation_marker.SetOrientationMarker(arrow_assembly)
                    orientation_marker.SetInteractor(canvas.interactor)

                    window_size = canvas.renderer.GetSize()

                    window_width, window_height = window_size
                    marker_width_px = 70
                    marker_height_px = 70
                    gap_px = 5

                    # Calculate normalized coordinates
                    left = (window_width - gap_px - marker_width_px) / window_width
                    right = (window_width - gap_px) / window_width
                    bottom = gap_px / window_height
                    top = (gap_px + marker_height_px) / window_height

                    # Clamp values to [0, 1] to avoid invalid viewport
                    left = max(0.0, left)
                    right = min(1.0, right)
                    bottom = max(0.0, bottom)
                    top = min(1.0, top)

                    orientation_marker.SetViewport(left, bottom, right, top)

                    orientation_marker.SetEnabled(1)
                    orientation_marker.InteractiveOff()

                    # Set assembly visibility
                    arrow_assembly.SetVisibility(self.compass_active)
                    # Toggle orientation marker widget
                    orientation_marker.SetEnabled(self.compass_active)

                    # Store reference and render
                    canvas.orientation_marker = orientation_marker
                    canvas.arrow_assembly = arrow_assembly

                # Adjust clipping range
                canvas.renderer.ResetCamera()
                camera = canvas.renderer.GetActiveCamera()
                camera.Dolly(0.8)  # Simple zoom out - adjust this value to taste
                canvas.renderer.ResetCameraClippingRange()
                canvas.renderer.Render()
                canvas.interactor.Initialize()
                canvas.interactor.Start()

                self.grids[active_tab_index] = cube_axes
                # Convert the color to RGB if it's in hexadecimal format
                color = self.grid_color[active_tab_index]
                self.set_foreground_color(color, active_tab_index)

                QApplication.restoreOverrideCursor()

        except Exception as e:
            QApplication.restoreOverrideCursor()
            QMessageBox.critical(self, "Error", f"An error occurred: {e}")

    def plot_tensor(self):
        if self.tensor_data is not None and self.index_dim_entry.text():
            try:
                QApplication.setOverrideCursor(self.custom_cursor)

                tensor_key = self.file_name
                metadata = self.metadata[self.file_name]

                # Get the index of the currently active tab
                active_tab_index = self.tab_widget.currentIndex()

                # Retrieve the widget of the currently active tab
                active_tab_widget = self.tab_widget.widget(active_tab_index)

                # Assuming the canvas is the first widget in the layout of the active tab
                canvas_layout = active_tab_widget.layout()
                canvas = canvas_layout.itemAt(0).widget()

                if self.tensor_data.shape[-1] > 1:
                    channel_index = int(self.channel_index_entry.value())
                else:
                    channel_index = 0

                # Retrieve selected index
                # Get the value of the index dimension from the index_dim_entry
                index_dim_value = int(self.index_dim_entry.text())

                # Get the shape of the tensor data
                tensor_shape = self.tensor_data.shape

                # Get the maximum value for the slider from the shape of the tensor data
                max_value = tensor_shape[index_dim_value] - 1

                if index_dim_value == 0:
                    sampling_interval = metadata.get('sampling_interval_ms')
                    min_slider_value = max(0, -(metadata.get('time_range', [0, 0, 0])[1]))
                    selected_index = int((int(self.index_entry.value()) - min_slider_value) / sampling_interval)
                else:
                    if index_dim_value == 1:
                        axis_range = metadata.get('inline_range', (0, max_value))
                    else:
                        axis_range = metadata.get('xline_range', (0, max_value))

                    min_slider_value = axis_range[0]
                    step = (axis_range[1] - axis_range[0]) / max_value if tensor_shape[index_dim_value] > 1 else 1

                    selected_index = int((int(self.index_entry.value()) - min_slider_value) / step)

                # Extract the 2D slice from the tensor
                slice_2d = np.squeeze(
                    np.take(self.tensor_data, selected_index, axis=index_dim_value)[..., channel_index])

                if np.issubdtype(slice_2d.dtype, np.floating):
                    if slice_2d.dtype not in (np.float16, np.float32):
                        slice_2d = slice_2d.astype(np.float32)  # Convert only if not already float16/float32
                    else:
                        slice_2d = np.copy(slice_2d)  # Copy if dtype remains unchanged
                elif np.issubdtype(slice_2d.dtype, np.bool_):
                    slice_2d = slice_2d.astype(np.int16)  # Convert bool → int32 (no need to check if already int32)
                elif np.issubdtype(slice_2d.dtype, np.integer):
                    if slice_2d.dtype in (np.int16, np.int32):
                        slice_2d = np.copy(slice_2d)  # Copy if dtype remains unchanged (int16 or int32)
                    else:
                        slice_2d = slice_2d.astype(np.int32)  # Convert other integer types to int32

                # Determine title and file name based on conditions
                if int(self.index_dim_entry.text()) < int(self.dim1_entry.text()) and \
                        int(self.index_dim_entry.text()) < int(self.dim2_entry.text()):
                    title_word = "Time Slice"
                else:
                    if int(self.index_dim_entry.text()) > int(self.dim1_entry.text()):
                        title_word = "Crossline"
                    else:
                        title_word = "Inline"

                # Check if dimension 2 is smaller than other dimensions
                if int(self.dim2_entry.text()) < int(self.dim1_entry.text()) and \
                        int(self.dim2_entry.text()) < int(self.index_dim_entry.text()):
                    flip_vertical_axis = True
                else:
                    flip_vertical_axis = False

                tensor_max_abs = np.max(np.abs(slice_2d))

                if gpu_available:
                    if np.all(np.modf(slice_2d)[0] == 0):
                        # Use CuPy for GPU-based interpolation
                        slice_2d_data_gpu = cp.array(slice_2d)  # Move data to GPU
                        interpolated_volume_gpu = zoom(slice_2d_data_gpu, zoom=self.two_d_interpolation, order=1)
                        interpolated_slice_2d = cp.asnumpy(interpolated_volume_gpu)  # Move back to CPU if needed
                        del slice_2d_data_gpu
                        del interpolated_volume_gpu
                        cp.get_default_memory_pool().free_all_blocks()
                    else:
                        # Use CuPy for GPU-based interpolation
                        slice_2d_data_gpu = cp.array(slice_2d)  # Move data to GPU
                        interpolated_volume_gpu = zoom(slice_2d_data_gpu, zoom=self.two_d_interpolation, order=3)
                        interpolated_slice_2d = cp.asnumpy(interpolated_volume_gpu)  # Move back to CPU if needed
                        del slice_2d_data_gpu
                        del interpolated_volume_gpu
                        cp.get_default_memory_pool().free_all_blocks()
                else:
                    # Use NumPy/SciPy for CPU-based interpolation
                    interpolated_slice_2d = ndimage.zoom(slice_2d, zoom=self.two_d_interpolation, order=3)

                tensor_min = np.min(slice_2d)
                tensor_max = np.max(slice_2d)

                # Define the custom color schemes
                custom_color_mappings = {
                    'Seismic': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'App. Polarity': [(0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 1.0, 1.0, 1.0),
                                      (0.7490196078431373, 0.0, 0.0, 1.0)],
                    'Polarity': [(0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 1.0, 1.0, 1.0),
                                 (0.7490196078431373, 0.0, 0.0, 1.0)],
                    'Inst. Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                        (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                        (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                        (1.0, 0.0, 1.0, 1.0)],
                    'Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                        (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                        (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                        (1.0, 0.0, 1.0, 1.0)],
                    'Cos Phase': [(0.0, 0.0, 0.0, 1.0), (1.0, 1.0, 1.0, 1.0)],
                    'Inst. Phase': [(1.0, 0.4117647058823529, 0.7058823529411765, 1.0),
                                    (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0),
                                    (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                                    (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)],
                    'Phase': [(1.0, 0.4117647058823529, 0.7058823529411765, 1.0),
                              (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0),
                              (0.0, 0.5019607843137255, 0.0, 1.0), (0.0, 1.0, 1.0, 1.0),
                              (0.0, 0.0, 0.5019607843137255, 1.0), (1.0, 0.0, 1.0, 1.0)],
                    'Envelope': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                 (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                 (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                 (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'Inst. Bandwidth': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                        (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                        (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                        (1.0, 0.0, 1.0, 1.0)],
                    'Dom. Frequency': [(0.0, 0.0, 0.0, 1.0), (0.7490196078431373, 0.0, 0.0, 1.0),
                                           (1.0, 1.0, 0.0, 1.0), (0.0, 0.5019607843137255, 0.0, 1.0),
                                           (0.0, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                           (1.0, 0.0, 1.0, 1.0)],
                    'Sweetness': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                  (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                  (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                  (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'RMS Amplitude': [(0.6313725490196078, 1.0, 1.0, 1.0), (0.0, 0.0, 0.5019607843137255, 1.0),
                                      (0.30196078431372547, 0.30196078431372547, 0.30196078431372547, 1.0),
                                      (1.0, 1.0, 1.0, 1.0), (0.3803921568627451, 0.27058823529411763, 0.0, 1.0),
                                      (0.7490196078431373, 0.0, 0.0, 1.0), (1.0, 1.0, 0.0, 1.0)],
                    'Coherence': [
                        (1.0, 1.0, 1.0, 1.0),  # White
                        (0.5, 0.5, 0.5, 1.0),  # Gray
                        (0.0, 0.0, 0.0, 1.0),  # Black (middle)
                        (1.0, 0.0, 0.0, 1.0),  # Red
                        (1.0, 0.647, 0.0, 1.0),  # Orange
                        (1.0, 1.0, 0.0, 1.0)  # Yellow
                    ],
                    'GST': [
                        (1.0, 1.0, 1.0, 1.0),  # White
                        (0.5, 0.5, 0.5, 1.0),  # Gray
                        (0.0, 0.0, 0.0, 1.0),  # Black (middle)
                        (1.0, 0.0, 0.0, 1.0),  # Red
                        (1.0, 0.647, 0.0, 1.0),  # Orange
                        (1.0, 1.0, 0.0, 1.0)  # Yellow
                    ],
                }

                def get_amplitude_anomaly_colors_from_colormap(colormap_name):
                    """
                    Generate seven colors with a transparent white color in the middle
                    from the given colormap.

                    Parameters:
                        colormap_name (str): The name of the colormap.

                    Returns:
                        list: A list of seven RGBA tuples.
                    """
                    colormap = plt.get_cmap(colormap_name)  # Get the colormap

                    # Generate six evenly spaced colors
                    colors = [colormap(i) for i in np.linspace(0, 1, 6)]

                    # Add a white transparent color as the middle (fourth) color
                    white_transparent = (1.0, 1.0, 1.0, 0.0)  # RGBA for transparent white
                    colors_with_middle = colors[:3] + [white_transparent] + colors[3:]

                    return colors_with_middle

                def generate_rgb_color_list(number_colors):
                    # Start with white and a second predefined color (e.g., light gray)
                    colours = [(1.0, 1.0, 1.0, 0.0), self.geo_objects_color]  # White, then light gray

                    if number_colors > 2:
                        # Generate a color map using a matplotlib colormap
                        cmap = plt.get_cmap(self.geophysical_Object_CMap, number_colors)

                        # Generate all the colors at once, starting from index 2
                        rgb_array = cmap(np.arange(2, number_colors))

                        # Append the generated RGB colors as tuples
                        colours.extend([tuple(rgb) for rgb in rgb_array])

                    return colours

                # Check the name of the reference and choose the color mapping
                matching_key = next((key for key in custom_color_mappings if key in self.color_mapp), None)
                if matching_key:
                    base_colors = custom_color_mappings[matching_key]
                elif self.color_mapp in ['Geobodies', 'Annotations']:
                    pass
                elif self.color_mapp.startswith('Anomalies'):
                    base_colors = get_amplitude_anomaly_colors_from_colormap(self.geophysical_Object_CMap)
                else:
                    # Get the unique values in the array, excluding zero
                    num_clusters = len(np.unique(slice_2d))
                    base_colors = generate_rgb_color_list(num_clusters)

                if self.color_mapp.startswith('Upscaled'):
                    color_bar_limits = (0, tensor_max)
                elif np.any(slice_2d < 0):
                    color_bar_limits = (-tensor_max_abs, tensor_max_abs)
                else:
                    color_bar_limits = (tensor_min, tensor_max)

                # Define the function to create a fully populated LUT
                def create_custom_lut():
                    # Define base colors
                    custom_base_colors = [
                        (255, 255, 255, 0),  # White to Transparent (RGBA)
                        self.geo_objects_color  # Yellow to Opaque (RGBA)
                    ]

                    # Create an empty LUT
                    custom_lut = np.zeros((256, 4), dtype=np.uint8)

                    # Populate LUT with a gradient between the defined base colors
                    for i in range(256):
                        # Linear interpolation between the base colors
                        r = int((1 - i / 255) * custom_base_colors[0][0] + (i / 255) * custom_base_colors[1][0])
                        g = int((1 - i / 255) * custom_base_colors[0][1] + (i / 255) * custom_base_colors[1][1])
                        b = int((1 - i / 255) * custom_base_colors[0][2] + (i / 255) * custom_base_colors[1][2])
                        a = int((1 - i / 255) * custom_base_colors[0][3] + (i / 255) * custom_base_colors[1][3])
                        custom_lut[i] = [r, g, b, a]

                    return custom_lut

                # Define the function to create a fully populated LUT
                def create_upscaled_wells_lut():

                    # Use a colormap with sufficient distinct colors
                    cmap = pg.colormap.get(self.geophysical_Object_CMap, source='matplotlib')

                    # Create a LUT array
                    upscaled_lut = np.zeros((256, 4), dtype=np.ubyte)

                    # Populate the LUT with colors from the colormap for values from 1 to 255
                    for i in range(1, 256):
                        qcolor = cmap.map(i / 255.0, mode='qcolor')
                        r, g, b, a = qcolor.red(), qcolor.green(), qcolor.blue(), qcolor.alpha()
                        upscaled_lut[i] = [r, g, b, a]

                    # Set the first entry to be fully transparent for the special value
                    upscaled_lut[0] = [0, 0, 0, 0]

                    return upscaled_lut

                # Define the function to create a fully populated LUT
                def create_else_lut():

                    # Create an empty LUT
                    else_lut = np.zeros((1, 4), dtype=np.uint8)

                    # Populate LUT with a gradient between the defined base colors
                    for i in range(1):
                        # Linear interpolation between the base colors
                        r = int((1 - i) * base_colors[0][0] + i * base_colors[1][0])
                        g = int((1 - i) * base_colors[0][1] + i * base_colors[1][1])
                        b = int((1 - i) * base_colors[0][2] + i * base_colors[1][2])
                        a = int((1 - i) * base_colors[0][3] + i * base_colors[1][3])
                        else_lut[i] = [r, g, b, a]

                    return else_lut

                if self.color_mapp in ['Geobodies', 'Annotations']:
                    lut = create_custom_lut()
                elif self.color_mapp.startswith('Upscaled'):
                    lut = create_upscaled_wells_lut()
                else:
                    if not np.all(slice_2d == slice_2d.flat[0]):
                        lut = create_else_lut()
                    else:
                        lut = create_custom_lut()

                matching_key = next((key for key in custom_color_mappings if key in self.color_mapp), None)
                if matching_key or self.color_mapp.startswith("Predicted"):
                    # Clear the canvas before plotting a new image
                    canvas.plotItem.clear()
                    # Clear the image items for the active tab
                    self.tensor_image_items[active_tab_index] = {}
                else:
                    # Ensure that the dictionary for the active tab is initialized
                    if active_tab_index not in self.tensor_image_items:
                        self.tensor_image_items[active_tab_index] = {}

                    # For secondary data, check and remove existing items with the same colorMap
                    existing_keys = [
                        key for key, item in self.tensor_image_items[active_tab_index].items()
                        if self.metadata[key].get('template') == self.color_mapp
                    ]

                    for key in existing_keys:
                        old_item = self.tensor_image_items[active_tab_index].pop(key)
                        canvas.plotItem.removeItem(old_item)

                if active_tab_index not in self.tensor_image_items:
                    self.tensor_image_items[active_tab_index] = {}

                # Check if the vertical axis needs to be flipped
                if flip_vertical_axis:
                    # Create the ImageItem with the slice_2d data
                    img_item = pg.ImageItem(interpolated_slice_2d)
                    img_item.setLookupTable(lut)  # Apply the LUT to the ImageItem
                    transform = pg.QtGui.QTransform()
                    transform.rotate(-90)
                    img_item.setTransform(transform)
                    img_item.setLevels(color_bar_limits)
                    canvas.plotItem.addItem(img_item)  # Add the ImageItem to the canvas

                    x = interpolated_slice_2d.shape[1] / self.two_d_interpolation

                    # Add x-axis title with a specific font size
                    if title_word == "Crossline":
                        xlabel = "Inline"
                        axis_range = metadata.get('inline_range', (0, x - 1))
                    else:
                        xlabel = "Crossline"
                        axis_range = metadata.get('xline_range', (0, x - 1))

                    step = (axis_range[1] - axis_range[0]) / (x - 1) if x > 1 else 1

                    sampling_interval_ms = metadata.get('sampling_interval_ms', 1)
                    time_range = metadata.get('time_range', (0, 0, 0))

                    lower_display = max(0, -time_range[1])
                    # Configure image item position
                    img_item.setPos(axis_range[0] * self.two_d_interpolation, -lower_display / sampling_interval_ms * self.two_d_interpolation)

                    # Access the PlotItem for further customization
                    plot_item = canvas.getPlotItem()
                    plot_item.setAspectLocked(True)
                    # Reset the zoom to fit the new image
                    plot_item.getViewBox().autoRange()

                    # Create a color bar
                    if active_tab_index not in self.cbar or self.cbar[active_tab_index] is None:
                        self.cbar[active_tab_index] = {}

                    # Create a color bar if it doesn't exist
                    if tensor_key not in self.cbar[active_tab_index] or self.cbar[active_tab_index][tensor_key] \
                            is None:
                        # Create Custom Color Bar for the specific tensor_key
                        self.cbar[active_tab_index][tensor_key] = CustomColorBarItem(orientation='left')

                    # Remove all existing color bars from the current plot item
                    if self.cbar[active_tab_index] is not None:
                        for key in list(self.cbar[active_tab_index].keys()):
                            old_cbar = self.cbar[active_tab_index][key]
                            old_cbar.hide()

                    # Show the color bar for the specific tensor_key
                    if self.cbar[active_tab_index][tensor_key]:
                        self.cbar[active_tab_index][tensor_key].show()

                    # Link the Custom Color Bar to the ImageItem
                    self.cbar[active_tab_index][tensor_key].setImageItem(img_item)

                    # Set the levels for the histogram
                    self.cbar[active_tab_index][tensor_key].setLevels(color_bar_limits)

                    if self.color_mapp in ['Geobodies', 'Annotations'] or np.all(slice_2d == slice_2d.flat[0]):
                        # Define colors with alpha channels for the colormap
                        colors = [
                            (1.0, 1.0, 1.0, 0.0),  # White with full transparency
                            self.geo_objects_color  # Yellow with full opacity
                        ]

                        # Create a custom colormap for the color bar visualization
                        custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', colors, N=256)

                        # Extract the color values including alpha
                        color_values = custom_cmap(np.linspace(0, 1, 256))

                        # Convert colormap values to PyQtGraph's ColorMap format
                        pg_colormap = pg.ColorMap(np.linspace(0, 1, 256),
                                                  [pg.mkColor((c[0] * 255, c[1] * 255, c[2] * 255, c[3] * 255)) for
                                                   c in color_values])
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)
                    elif self.color_mapp.startswith('Upscaled'):
                        # Apply the same LUT colors to the color bar
                        num_colors = len(lut)

                        # Extract color values from the LUT
                        colors = [tuple(lut[i]) for i in range(num_colors)]

                        # Create a PyQtGraph ColorMap with the same colors
                        pg_colormap = pg.ColorMap(pos=np.linspace(0, 1, num_colors), color=colors)

                        # Apply the color map to the color bar
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)
                    else:

                        # Create a custom colormap for the color bar visualization
                        custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', base_colors, N=256)

                        # Extract the color values including alpha
                        color_values = custom_cmap(np.linspace(0, 1, 256))

                        # Convert colormap values to PyQtGraph's ColorMap format
                        pg_colormap = pg.ColorMap(np.linspace(0, 1, 256),
                                                  [pg.mkColor((c[0] * 255, c[1] * 255, c[2] * 255, c[3] * 255)) for
                                                   c in color_values])
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)

                    # Add the color bar to the layout to the right of the image
                    plot_item.layout.addItem(self.cbar[active_tab_index][tensor_key], 2, 4)

                    # Move the x-axis to the top
                    plot_item.getAxis('bottom').setZValue(-10000)  # This moves the x-axis to the top
                    plot_item.getAxis('top').setZValue(10000)
                    plot_item.getAxis('top').setScale((1 / self.two_d_interpolation) * step)
                    plot_item.showAxis('left')
                    plot_item.getAxis('left').setScale(
                        (1 / self.two_d_interpolation) * sampling_interval_ms)
                    plot_item.showAxis('top')
                    plot_item.hideAxis('bottom')
                    plot_item.setLabel('top', xlabel)  # This sets the x-axis label to the top
                    plot_item.setLabel('left', 'TWT (ms)')  # This sets the x-axis label to the top
                    plot_item.setTitle(f"{title_word} {int(self.index_entry.value())}", axis='bottom')
                else:
                    # Create the ImageItem with the slice_2d data
                    img_item = pg.ImageItem(interpolated_slice_2d)
                    img_item.setLookupTable(lut)  # Apply the LUT to the ImageItem
                    img_item.setLevels(color_bar_limits)
                    canvas.plotItem.addItem(img_item)  # Add the ImageItem to the canvas

                    x = interpolated_slice_2d.shape[0] / self.two_d_interpolation
                    y = interpolated_slice_2d.shape[1] / self.two_d_interpolation

                    x_axis_range = metadata.get('inline_range', (0, x - 1))

                    y_axis_range = metadata.get('xline_range', (0, y - 1))

                    step_x = (x_axis_range[1] - x_axis_range[0]) / (x - 1) if x > 1 else 1
                    step_y = (y_axis_range[1] - y_axis_range[0]) / (y - 1) if y > 1 else 1

                    # Configure image item scaling/position
                    img_item.setPos(x_axis_range[0] * self.two_d_interpolation,
                                    y_axis_range[0] * self.two_d_interpolation)

                    plot_item = canvas.getPlotItem()
                    plot_item.setAspectLocked(True)
                    plot_item.getViewBox().autoRange()

                    # Create a color bar
                    if active_tab_index not in self.cbar or self.cbar[active_tab_index] is None:
                        self.cbar[active_tab_index] = {}

                    # Create a color bar if it doesn't exist
                    if tensor_key not in self.cbar[active_tab_index] or self.cbar[active_tab_index][
                        tensor_key] is None:
                        # Create Custom Color Bar for the specific tensor_key
                        self.cbar[active_tab_index][tensor_key] = CustomColorBarItem(orientation='left')

                    # Remove all existing color bars from the current plot item
                    if self.cbar[active_tab_index] is not None:
                        for key in list(self.cbar[active_tab_index].keys()):
                            old_cbar = self.cbar[active_tab_index][key]
                            old_cbar.hide()

                    # Show the color bar for the specific tensor_key
                    if self.cbar[active_tab_index][tensor_key]:
                        self.cbar[active_tab_index][tensor_key].show()

                    # Link the color bar to the ImageItem
                    self.cbar[active_tab_index][tensor_key].setImageItem(img_item)

                    # Set the levels for the color bar
                    self.cbar[active_tab_index][tensor_key].setLevels(color_bar_limits)

                    if self.color_mapp in ['Geobodies', 'Annotations'] or np.all(slice_2d == slice_2d.flat[0]):
                        # Define colors with alpha channels for the colormap
                        colors = [
                            (1.0, 1.0, 1.0, 0.0),  # White with full transparency
                            self.geo_objects_color  # Yellow with full opacity
                        ]

                        # Create a custom colormap for the color bar visualization
                        custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', colors, N=256)

                        # Extract the color values including alpha
                        color_values = custom_cmap(np.linspace(0, 1, 256))

                        # Convert colormap values to PyQtGraph's ColorMap format
                        pg_colormap = pg.ColorMap(np.linspace(0, 1, 256),
                                                  [pg.mkColor((c[0] * 255, c[1] * 255, c[2] * 255, c[3] * 255)) for
                                                   c in color_values])
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)
                    elif self.color_mapp.startswith('Upscaled'):
                        # Apply the same LUT colors to the color bar
                        num_colors = len(lut)

                        # Extract color values from the LUT
                        colors = [tuple(lut[i]) for i in range(num_colors)]

                        # Create a PyQtGraph ColorMap with the same colors
                        pg_colormap = pg.ColorMap(pos=np.linspace(0, 1, num_colors), color=colors)

                        # Apply the color map to the color bar
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)
                    else:

                        # Create a custom colormap for the color bar visualization
                        custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', base_colors, N=256)

                        # Extract the color values including alpha
                        color_values = custom_cmap(np.linspace(0, 1, 256))

                        # Convert colormap values to PyQtGraph's ColorMap format
                        pg_colormap = pg.ColorMap(np.linspace(0, 1, 256),
                                                  [pg.mkColor((c[0] * 255, c[1] * 255, c[2] * 255, c[3] * 255)) for
                                                   c in color_values])
                        self.cbar[active_tab_index][tensor_key].setColorMap(pg_colormap)

                    # Add the color bar to the layout to the right of the image
                    plot_item.layout.addItem(self.cbar[active_tab_index][tensor_key], 2, 4)

                    plot_item.showAxis('left')
                    plot_item.showAxis('bottom')
                    plot_item.getAxis('bottom').setScale((1 / self.two_d_interpolation) * step_x)
                    plot_item.getAxis('left').setScale((1 / self.two_d_interpolation) * step_y)
                    plot_item.hideAxis('top')
                    plot_item.setLabel('bottom', 'Crossline')  # This sets the x-axis label to the top
                    plot_item.setLabel('left', 'Inline')
                    plot_item.setTitle(f"{title_word}  {-int(self.index_entry.value())} ms", axis='bottom')

                # Display the plot in the initial window
                layout = self.centralWidget().layout()

                # Check if there is a widget at index 1
                if layout.count() > 1:
                    # Display the plot in the initial window
                    canvas = canvas_layout.itemAt(0).widget()
                    canvas.setFocusPolicy(Qt.FocusPolicy.NoFocus)

                    # Check if the widget is a RoundedCanvas
                    if isinstance(canvas, RoundedCanvas):
                        # Use the existing canvas to show the plot
                        canvas.show()
                        canvas.plotItem.getViewBox().autoRange()

                # Ensure that the metadata dictionary for the active tab is a list
                if active_tab_index not in self.tensor_metadata:
                    self.tensor_metadata[active_tab_index] = []

                # Check if the tensor_key already exists in the metadata list for the active tab
                metadata_exists = False
                for item in self.tensor_metadata[active_tab_index]:
                    if item['tensor_key'] == tensor_key:
                        # Update the existing metadata
                        item['slice_params'] = {
                            'dim1': int(self.dim1_entry.text()),
                            'dim2': int(self.dim2_entry.text()),
                            'index_dim': int(self.index_dim_entry.text()),
                            'selected_index': selected_index,
                            'index_entry': self.index_entry.value(),
                            'channel_index': channel_index,
                        }
                        metadata_exists = True
                        break

                # If the tensor_key does not exist, append a new metadata entry
                if not metadata_exists:
                    self.tensor_metadata[active_tab_index].append({
                        'tensor_key': tensor_key,
                        'slice_params': {
                            'dim1': int(self.dim1_entry.text()),
                            'dim2': int(self.dim2_entry.text()),
                            'index_dim': int(self.index_dim_entry.text()),
                            'selected_index': selected_index,
                            'index_entry': self.index_entry.value(),
                            'channel_index': channel_index,
                        }
                    })

                # Ensure that the dictionary for the active tab is initialized
                if active_tab_index not in self.tensor_image_items:
                    self.tensor_image_items[active_tab_index] = {}

                # Update or add the image item for the tensor key
                self.tensor_image_items[active_tab_index][tensor_key] = img_item

                QApplication.restoreOverrideCursor()

            except Exception as e:
                QApplication.restoreOverrideCursor()
                traceback.print_exc()
                QMessageBox.critical(self, "Error", f"An error occurred: {e}")


def run_app():
    multiprocessing.freeze_support()
    app = QApplication(sys.argv)

    # Create and show the splash screen
    splash_pix = QPixmap("icon.png")
    splash = QSplashScreen(splash_pix, Qt.WindowType.WindowStaysOnTopHint)
    splash.show()

    app.setStyle(QtWidgets.QStyleFactory.create("Fusion"))

    # Set the global application icon
    app_icon = QIcon("icon.png")
    app.setWindowIcon(app_icon)

    main_window = TensorVisualizer()

    try:
        splash.finish(main_window)
        main_window.setGeometry(100, 100, 1220, 820)
        main_window.show()
        sys.exit(app.exec())

    except Exception as e:
        splash.close()
        QMessageBox.critical(main_window, "Launch Error", str(e))
        sys.exit(0)


if __name__ == "__main__":
    run_app()
